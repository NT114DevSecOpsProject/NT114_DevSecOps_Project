# Alternative Provider Configuration (Recommended)
#
# This approach separates infrastructure deployment into two stages:
# Stage 1: Deploy VPC and EKS cluster
# Stage 2: Deploy node groups and Helm releases
#
# To use this approach:
# 1. Rename this file to providers.tf
# 2. Comment out alb-controller and eks-nodegroup modules in main.tf
# 3. Run: terraform apply (creates VPC and EKS)
# 4. Uncomment alb-controller and eks-nodegroup modules
# 5. Run: terraform apply (deploys everything else)

terraform {
  required_version = ">= 1.5.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.29"
    }
    helm = {
      source  = "hashicorp/helm"
      version = ">= 2.11.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# Use environment variables or AWS CLI for kubectl configuration
# Run this after EKS cluster is created:
# aws eks update-kubeconfig --region us-east-1 --name eks-1

provider "kubernetes" {
  host                   = module.eks_cluster.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks_cluster.cluster_certificate_authority_data)

  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    command     = "aws"
    # This requires the awscli to be installed locally
    args = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
  }
}

provider "helm" {
  kubernetes {
    host                   = module.eks_cluster.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks_cluster.cluster_certificate_authority_data)

    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      command     = "aws"
      args        = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
    }
  }
}
