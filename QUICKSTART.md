# Quick Start Guide - NT114 DevSecOps Project

H∆∞·ªõng d·∫´n ƒë·∫ßy ƒë·ªß t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi ƒë·ªÉ deploy application l√™n AWS EKS.

---

## üìã Prerequisites

ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t:

- ‚úÖ **AWS Account** v·ªõi admin access
- ‚úÖ **AWS CLI** configured (`aws configure`)
- ‚úÖ **Terraform** >= 1.5.0
- ‚úÖ **kubectl**
- ‚úÖ **Helm** >= 3.x
- ‚úÖ **Git**
- ‚úÖ **GitHub Account** (ƒë√£ fork repo n√†y)

**Ki·ªÉm tra:**
```bash
aws --version
terraform --version
kubectl version --client
helm version
git --version
```

---

## üöÄ B∆∞·ªõc 1: T·∫°o Infrastructure v·ªõi Terraform

### 1.1 - Navigate to Terraform directory

```bash
cd terraform/environments/dev
```

### 1.2 - Initialize Terraform

```bash
terraform init
```

**Output mong ƒë·ª£i:**
```
Initializing modules...
Initializing the backend...
Terraform has been successfully initialized!
```

### 1.3 - Review Plan

```bash
terraform plan
```

**Output:** S·∫Ω t·∫°o ~50-60 resources bao g·ªìm:
- VPC v·ªõi public/private subnets
- NAT Gateway, Internet Gateway
- EKS Cluster (eks-1)
- EKS Node Group (2 nodes t3.large)
- RDS PostgreSQL instance
- Security Groups
- IAM Roles

### 1.4 - Apply Infrastructure

```bash
terraform apply
```

**Nh·∫≠p:** `yes` khi ƒë∆∞·ª£c h·ªèi

‚è±Ô∏è **Th·ªùi gian:** ~15-20 ph√∫t

**Output cu·ªëi c√πng:**
```
Apply complete! Resources: 56 added, 0 changed, 0 destroyed.

Outputs:
cluster_name = "eks-1"
cluster_endpoint = "https://xxxxx.eks.us-east-1.amazonaws.com"
vpc_id = "vpc-xxxxx"
database_endpoint = "nt114-auth-db.xxxxx.us-east-1.rds.amazonaws.com"
```

‚úÖ **Checkpoint:** Infrastructure ƒë√£ ƒë∆∞·ª£c t·∫°o

---

## üîß B∆∞·ªõc 2: Configure kubectl

### 2.1 - Update kubeconfig

```bash
aws eks update-kubeconfig --region us-east-1 --name eks-1
```

### 2.2 - Verify cluster access

```bash
kubectl get nodes
```

**Output mong ƒë·ª£i:**
```
NAME                           STATUS   ROLES    AGE   VERSION
ip-11-0-1-xxx.ec2.internal     Ready    <none>   5m    v1.31.x
ip-11-0-2-xxx.ec2.internal     Ready    <none>   5m    v1.31.x
```

### 2.3 - Check namespaces

```bash
kubectl get namespaces
```

**Output:** S·∫Ω th·∫•y `dev` namespace ƒë√£ ƒë∆∞·ª£c t·∫°o b·ªüi Terraform

‚úÖ **Checkpoint:** kubectl ƒë√£ connect ƒë·∫øn EKS cluster

---

## üì¶ B∆∞·ªõc 3: Setup GitHub Secrets

### 3.1 - Get AWS credentials

L·∫•y AWS Access Key v√† Secret Key t·ª´ AWS Console ho·∫∑c:

```bash
aws configure list
```

### 3.2 - Add GitHub Secrets

V√†o GitHub repo ‚Üí **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions** ‚Üí **New repository secret**

Th√™m 2 secrets:
- `AWS_ACCESS_KEY_ID`: Your AWS access key
- `AWS_SECRET_ACCESS_KEY`: Your AWS secret key

‚úÖ **Checkpoint:** GitHub secrets ƒë√£ ƒë∆∞·ª£c th√™m

---

## üèóÔ∏è B∆∞·ªõc 4: Build v√† Push Docker Images

### 4.1 - Trigger Frontend Build

**C√°ch 1:** Push code changes trong folder `frontend/`

**C√°ch 2:** Manual trigger qua GitHub Actions
- V√†o tab **Actions** ‚Üí **Frontend Build** ‚Üí **Run workflow**

‚è±Ô∏è **Th·ªùi gian:** ~3-5 ph√∫t

**K·∫øt qu·∫£:** Image ƒë∆∞·ª£c push l√™n ECR:
```
039612870452.dkr.ecr.us-east-1.amazonaws.com/nt114-devsecops/frontend:latest
```

### 4.2 - Trigger Backend Build

**C√°ch 1:** Push code changes trong folder `microservices/`

**C√°ch 2:** Manual trigger qua GitHub Actions
- V√†o tab **Actions** ‚Üí **Backend Microservices Build** ‚Üí **Run workflow**

‚è±Ô∏è **Th·ªùi gian:** ~5-8 ph√∫t (build 4 services song song)

**K·∫øt qu·∫£:** 4 images ƒë∆∞·ª£c push l√™n ECR:
- `api-gateway:latest`
- `user-management-service:latest`
- `exercises-service:latest`
- `scores-service:latest`

### 4.3 - Verify images in ECR

```bash
aws ecr list-images --repository-name nt114-devsecops/frontend --region us-east-1
aws ecr list-images --repository-name nt114-devsecops/api-gateway --region us-east-1
```

‚úÖ **Checkpoint:** T·∫•t c·∫£ images ƒë√£ c√≥ tr√™n ECR

---

## üóÑÔ∏è B∆∞·ªõc 5: Setup Database

### 5.1 - Get RDS endpoint

```bash
cd terraform/environments/dev
terraform output database_endpoint
```

**Output:** `nt114-auth-db.xxxxxx.us-east-1.rds.amazonaws.com`

### 5.2 - Create database schema

T·ª´ root folder c·ªßa project:

```bash
# Set environment variables
export DB_HOST="<RDS_ENDPOINT_FROM_ABOVE>"
export DB_PORT="5432"
export DB_NAME="auth_db"
export DB_USER="postgres"
export DB_PASSWORD="postgres123"  # Ho·∫∑c password b·∫°n ƒë√£ set trong Terraform

# Run schema creation script
python3 create_db_schema.py
```

**Output mong ƒë·ª£i:**
```
Connecting to database...
Creating users table...
Creating exercises table...
Creating scores table...
‚úì Database schema created successfully!
```

### 5.3 - Verify tables created

```bash
# Connect to RDS
psql -h $DB_HOST -U $DB_USER -d $DB_NAME

# List tables
\dt

# Exit
\q
```

**Ho·∫∑c d√πng kubectl exec v√†o m·ªôt pod v√† connect:**

```bash
kubectl exec -it -n dev deployment/user-management-service -- bash
psql -h nt114-auth-db.xxxxx.us-east-1.rds.amazonaws.com -U postgres -d auth_db
```

‚úÖ **Checkpoint:** Database ƒë√£ s·∫µn s√†ng

---

## üîê B∆∞·ªõc 6: Create Kubernetes Secrets

### 6.1 - Create database secret

```bash
kubectl create secret generic user-management-db-secret \
  --from-literal=DB_HOST='<RDS_ENDPOINT>' \
  --from-literal=DB_PORT='5432' \
  --from-literal=DB_NAME='auth_db' \
  --from-literal=DB_USER='postgres' \
  --from-literal=DB_PASSWORD='postgres123' \
  -n dev
```

### 6.2 - Create ECR pull secret

```bash
# Get ECR login password
ECR_PASSWORD=$(aws ecr get-login-password --region us-east-1)

# Create secret
kubectl create secret docker-registry ecr-secret \
  --docker-server=039612870452.dkr.ecr.us-east-1.amazonaws.com \
  --docker-username=AWS \
  --docker-password=$ECR_PASSWORD \
  -n dev
```

### 6.3 - Verify secrets

```bash
kubectl get secrets -n dev
```

**Output:**
```
NAME                           TYPE                             DATA   AGE
user-management-db-secret      Opaque                           5      10s
ecr-secret                     kubernetes.io/dockerconfigjson   1      5s
```

‚úÖ **Checkpoint:** Secrets ƒë√£ ƒë∆∞·ª£c t·∫°o

---

## üì± B∆∞·ªõc 7: Deploy Services v·ªõi Helm

### 7.1 - Deploy API Gateway

```bash
cd helm
helm install api-gateway ./api-gateway -f ./api-gateway/values-eks.yaml -n dev
```

### 7.2 - Deploy User Management Service

```bash
helm install user-management-service ./user-management-service -f ./user-management-service/values-eks.yaml -n dev
```

### 7.3 - Deploy Exercises Service

```bash
helm install exercises-service ./exercises-service -f ./exercises-service/values-eks.yaml -n dev
```

### 7.4 - Deploy Scores Service

```bash
helm install scores-service ./scores-service -f ./scores-service/values-eks.yaml -n dev
```

### 7.5 - Deploy Frontend

```bash
helm install frontend ./frontend -f ./frontend/values-eks.yaml -n dev
```

### 7.6 - Verify deployments

```bash
kubectl get pods -n dev
```

**Output mong ƒë·ª£i (sau 2-3 ph√∫t):**
```
NAME                                      READY   STATUS    RESTARTS   AGE
api-gateway-xxxxx-xxxxx                   1/1     Running   0          2m
api-gateway-xxxxx-xxxxx                   1/1     Running   0          2m
user-management-service-xxxxx-xxxxx       1/1     Running   0          2m
user-management-service-xxxxx-xxxxx       1/1     Running   0          2m
exercises-service-xxxxx-xxxxx             1/1     Running   0          2m
exercises-service-xxxxx-xxxxx             1/1     Running   0          2m
scores-service-xxxxx-xxxxx                1/1     Running   0          2m
scores-service-xxxxx-xxxxx                1/1     Running   0          2m
frontend-xxxxx-xxxxx                      1/1     Running   0          2m
frontend-xxxxx-xxxxx                      1/1     Running   0          2m
```

‚úÖ **Checkpoint:** T·∫•t c·∫£ services ƒëang ch·∫°y

---

## üåê B∆∞·ªõc 8: Expose Services

### 8.1 - Check services

```bash
kubectl get svc -n dev
```

**Output:**
```
NAME                        TYPE           CLUSTER-IP      EXTERNAL-IP                          PORT(S)
api-gateway                 LoadBalancer   10.100.x.x      axxxxx.us-east-1.elb.amazonaws.com   8080:30336/TCP
frontend                    LoadBalancer   10.100.x.x      axxxxx.us-east-1.elb.amazonaws.com   80:31184/TCP
user-management-service     ClusterIP      10.100.x.x      <none>                               8081/TCP
exercises-service           ClusterIP      10.100.x.x      <none>                               8082/TCP
scores-service              ClusterIP      10.100.x.x      <none>                               8083/TCP
```

### 8.2 - Get application URLs

```bash
# Frontend URL
FRONTEND_URL=$(kubectl get svc frontend -n dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "Frontend: http://$FRONTEND_URL"

# API Gateway URL
API_URL=$(kubectl get svc api-gateway -n dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "API Gateway: http://$API_URL:8080"
```

**L∆∞u l·∫°i 2 URLs n√†y!**

‚úÖ **Checkpoint:** Services ƒë√£ ƒë∆∞·ª£c expose qua LoadBalancer

---

## ‚úÖ B∆∞·ªõc 9: Verify Application

### 9.1 - Test API Gateway

```bash
# Health check
curl http://$API_URL:8080/health

# Test registration
curl -X POST http://$API_URL:8080/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "username": "testuser",
    "password": "password123"
  }'
```

**Output mong ƒë·ª£i:**
```json
{
  "message": "User registered successfully.",
  "status": "success"
}
```

### 9.2 - Test Login

```bash
curl -X POST http://$API_URL:8080/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "test@example.com",
    "password": "password123"
  }'
```

**Output:**
```json
{
  "auth_token": "eyJhbGci...",
  "data": {
    "email": "test@example.com",
    "username": "testuser"
  },
  "status": "success"
}
```

### 9.3 - Test Frontend

M·ªü browser v√† truy c·∫≠p: `http://<FRONTEND_URL>`

**B·∫°n s·∫Ω th·∫•y:**
- ‚úÖ Trang web hi·ªÉn th·ªã
- ‚úÖ C√≥ th·ªÉ ƒëƒÉng k√Ω t√†i kho·∫£n
- ‚úÖ C√≥ th·ªÉ ƒëƒÉng nh·∫≠p
- ‚úÖ C√≥ th·ªÉ v√†o Dashboard sau khi login
- ‚úÖ C√≥ th·ªÉ xem Scores v√† Exercises

‚úÖ **Checkpoint:** Application ho·∫°t ƒë·ªông ho√†n to√†n!

---

## üéâ Ho√†n Th√†nh!

B·∫°n ƒë√£ deploy th√†nh c√¥ng ·ª©ng d·ª•ng v·ªõi:

- ‚úÖ **EKS Cluster** v·ªõi 2 worker nodes
- ‚úÖ **RDS PostgreSQL** database
- ‚úÖ **5 services** running (1 frontend + 4 backend microservices)
- ‚úÖ **Load Balancers** cho external access
- ‚úÖ **Auto-scaling** enabled (HPA)
- ‚úÖ **Monitoring** v·ªõi health checks

---

## üîß Useful Commands

### Check Pods
```bash
kubectl get pods -n dev
kubectl logs -f <pod-name> -n dev
kubectl describe pod <pod-name> -n dev
```

### Check Services
```bash
kubectl get svc -n dev
kubectl describe svc <service-name> -n dev
```

### Check HPA (Auto-scaling)
```bash
kubectl get hpa -n dev
```

### Restart a service
```bash
kubectl rollout restart deployment/<service-name> -n dev
```

### Update a service
```bash
# After changing Helm values
helm upgrade <service-name> ./helm/<service-name> -f ./helm/<service-name>/values-eks.yaml -n dev
```

### Delete all services
```bash
helm uninstall api-gateway -n dev
helm uninstall user-management-service -n dev
helm uninstall exercises-service -n dev
helm uninstall scores-service -n dev
helm uninstall frontend -n dev
```

### Destroy infrastructure
```bash
cd terraform/environments/dev
terraform destroy
```

---

## üêõ Troubleshooting

### Pod kh√¥ng start

```bash
# Check pod status
kubectl get pods -n dev

# Check events
kubectl describe pod <pod-name> -n dev

# Check logs
kubectl logs <pod-name> -n dev
```

**Common issues:**
- **ImagePullBackOff**: ECR secret ch∆∞a ƒë√∫ng ho·∫∑c image kh√¥ng t·ªìn t·∫°i
  - Fix: Recreate ECR secret v·ªõi credentials m·ªõi
- **CrashLoopBackOff**: Container b·ªã crash
  - Fix: Check logs ƒë·ªÉ xem l·ªói g√¨
- **Pending**: Node kh√¥ng ƒë·ªß resources
  - Fix: Scale up node group ho·∫∑c gi·∫£m resource requests

### Service kh√¥ng accessible

```bash
# Check service
kubectl get svc <service-name> -n dev

# Check endpoints
kubectl get endpoints <service-name> -n dev
```

### Database connection issues

```bash
# Verify secret exists
kubectl get secret user-management-db-secret -n dev

# Check pod can connect to RDS
kubectl exec -it <pod-name> -n dev -- bash
nc -zv <RDS_ENDPOINT> 5432
```

**Common fix:** Check Security Groups - RDS ph·∫£i allow inbound t·ª´ EKS nodes

### Frontend can't connect to API

1. Check API Gateway LoadBalancer URL
2. Verify nginx config forwards requests correctly
3. Check CORS settings
4. Verify frontend env var `VITE_API_URL` is empty (uses nginx proxy)

---

## üîÑ B∆∞·ªõc 10: Setup GitOps v·ªõi ArgoCD (Recommended)

ArgoCD gi√∫p t·ª± ƒë·ªông deploy applications t·ª´ Git repository, gi√∫p qu·∫£n l√Ω deployments d·ªÖ d√†ng h∆°n v√† ƒë·∫£m b·∫£o Git l√† single source of truth.

### 10.1 - Install ArgoCD

```bash
# Create argocd namespace
kubectl create namespace argocd

# Install ArgoCD
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

‚è±Ô∏è **Th·ªùi gian:** ~2-3 ph√∫t

### 10.2 - Expose ArgoCD Server

```bash
# Patch argocd-server service to LoadBalancer
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# Wait for LoadBalancer to be ready
kubectl get svc argocd-server -n argocd -w
```

ƒê·ª£i cho ƒë·∫øn khi th·∫•y EXTERNAL-IP (Ctrl+C ƒë·ªÉ tho√°t watch):

```
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP                                            PORT(S)
argocd-server   LoadBalancer   10.100.x.x      axxxxx.us-east-1.elb.amazonaws.com                    80:xxxxx/TCP,443:xxxxx/TCP
```

### 10.3 - Get ArgoCD Admin Password

```bash
# Get initial admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
```

**L∆∞u l·∫°i password n√†y!**

### 10.4 - Access ArgoCD UI

```bash
# Get ArgoCD URL
ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "ArgoCD URL: http://$ARGOCD_URL"
```

M·ªü browser v√† ƒëƒÉng nh·∫≠p:
- **URL**: `http://<ARGOCD_URL>`
- **Username**: `admin`
- **Password**: `<password t·ª´ b∆∞·ªõc 10.3>`

### 10.5 - Deploy Applications v·ªõi ArgoCD

#### Option A: Deploy t·∫•t c·∫£ applications c√πng l√∫c

```bash
# Apply all ArgoCD application manifests
kubectl apply -f argocd/applications/
```

#### Option B: Deploy t·ª´ng application ri√™ng

```bash
# Deploy API Gateway
kubectl apply -f argocd/applications/api-gateway.yaml

# Deploy User Management Service
kubectl apply -f argocd/applications/user-management-service.yaml

# Deploy Exercises Service
kubectl apply -f argocd/applications/exercises-service.yaml

# Deploy Scores Service
kubectl apply -f argocd/applications/scores-service.yaml

# Deploy Frontend
kubectl apply -f argocd/applications/frontend.yaml
```

### 10.6 - Verify ArgoCD Applications

```bash
# Check applications status
kubectl get applications -n argocd
```

**Output mong ƒë·ª£i:**
```
NAME                      SYNC STATUS   HEALTH STATUS
api-gateway               Synced        Healthy
exercises-service         Synced        Healthy
frontend                  Synced        Healthy
scores-service            Synced        Healthy
user-management-service   Synced        Healthy
```

### 10.7 - Monitor Sync Progress

**Via CLI:**
```bash
# Watch all applications
kubectl get applications -n argocd -w

# Get detailed status of specific app
kubectl describe application api-gateway -n argocd
```

**Via ArgoCD UI:**
1. M·ªü ArgoCD UI trong browser
2. B·∫°n s·∫Ω th·∫•y t·∫•t c·∫£ 5 applications
3. Click v√†o b·∫•t k·ª≥ application ƒë·ªÉ xem resource tree
4. M√†u xanh = Healthy & Synced

### 10.8 - Verify Auto-Sync

ArgoCD ƒë√£ ƒë∆∞·ª£c configure v·ªõi auto-sync enabled. Test b·∫±ng c√°ch:

```bash
# Edit m·ªôt Helm value (v√≠ d·ª•: change replica count)
cd helm/api-gateway
# Edit values-eks.yaml, change replicaCount t·ª´ 2 th√†nh 3

# Commit v√† push
git add values-eks.yaml
git commit -m "test: increase api-gateway replicas to 3"
git push origin main

# ArgoCD s·∫Ω t·ª± ƒë·ªông detect v√† sync trong v√≤ng ~3 ph√∫t
# Watch trong ArgoCD UI ho·∫∑c CLI
kubectl get applications -n argocd -w
```

‚úÖ **Checkpoint:** ArgoCD ƒëang qu·∫£n l√Ω t·∫•t c·∫£ applications

---

## üéØ GitOps Workflow v·ªõi ArgoCD

Sau khi setup ArgoCD, workflow c·ªßa b·∫°n s·∫Ω ƒë∆°n gi·∫£n h∆°n:

### Update Application

**Before (Manual Helm):**
```bash
# Edit Helm values
vim helm/api-gateway/values-eks.yaml

# Apply manually
helm upgrade api-gateway ./helm/api-gateway -f ./helm/api-gateway/values-eks.yaml -n dev
```

**After (GitOps with ArgoCD):**
```bash
# Edit Helm values
vim helm/api-gateway/values-eks.yaml

# Commit and push
git add helm/api-gateway/values-eks.yaml
git commit -m "feat: update api-gateway configuration"
git push origin main

# ArgoCD t·ª± ƒë·ªông deploy! Kh√¥ng c·∫ßn ch·∫°y helm upgrade
```

### Rollback Application

**Via ArgoCD UI:**
1. V√†o application page
2. Click **History and Rollback**
3. Ch·ªçn revision c≈© h∆°n
4. Click **Rollback**

**Via CLI:**
```bash
# View history
kubectl get applications api-gateway -n argocd -o yaml | grep -A 10 status:

# ArgoCD t·ª± ƒë·ªông rollback n·∫øu Git ƒë∆∞·ª£c revert
git revert <commit-hash>
git push origin main
```

### Add New Service

1. T·∫°o Helm chart m·ªõi trong `helm/<new-service>/`
2. T·∫°o ArgoCD Application manifest:

```yaml
# argocd/applications/new-service.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: new-service
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/NT114DevSecOpsProject/NT114_DevSecOps_Project.git
    targetRevision: main
    path: helm/new-service
    helm:
      valueFiles:
        - values-eks.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: dev
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

3. Apply manifest:
```bash
kubectl apply -f argocd/applications/new-service.yaml
```

‚úÖ ArgoCD s·∫Ω t·ª± ƒë·ªông deploy service m·ªõi!

---

## üìä ArgoCD Best Practices

### 1. Git l√† Source of Truth
- ‚ùå KH√îNG bao gi·ªù edit resources tr·ª±c ti·∫øp tr√™n cluster (`kubectl edit`)
- ‚úÖ LU√îN edit trong Git repository v√† push

### 2. Use Separate Branches
```bash
# Create feature branch
git checkout -b feature/update-api

# Make changes
vim helm/api-gateway/values-eks.yaml

# Test on feature branch first
# Update ArgoCD app to point to feature branch temporarily
kubectl patch app api-gateway -n argocd --type merge -p '{"spec":{"source":{"targetRevision":"feature/update-api"}}}'

# If OK, merge to main
git checkout main
git merge feature/update-api
git push origin main

# ArgoCD auto-syncs from main branch
```

### 3. Monitor Sync Status
```bash
# Setup alerts for sync failures (example)
kubectl get applications -n argocd -o json | jq '.items[] | select(.status.health.status != "Healthy")'
```

### 4. Documentation
ƒê·ªçc th√™m ArgoCD documentation t·∫°i: `argocd/README.md`

---

## üìö Next Steps

1. **Custom Domain**: Setup Route53 for custom domain
2. **HTTPS**: Add SSL certificate via ACM
3. **Monitoring**: Install Prometheus & Grafana
4. **Logging**: Setup CloudWatch Logs or ELK stack
5. **CI/CD**: Fully automate with GitHub Actions + ArgoCD
6. **Backup**: Setup database backups
7. **Security**: Implement WAF, security groups hardening
8. **Multi-Environment**: Create staging/production with ArgoCD ApplicationSets

---

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ:
1. Check [DEPLOYMENT.md](DEPLOYMENT.md) cho chi ti·∫øt h∆°n
2. Check [argocd/README.md](argocd/README.md) cho ArgoCD troubleshooting
3. Check logs: `kubectl logs <pod-name> -n dev`
4. Check events: `kubectl get events -n dev --sort-by='.lastTimestamp'`
5. Check ArgoCD app status: `kubectl get applications -n argocd`
6. Verify all prerequisites ƒë∆∞·ª£c c√†i ƒë√∫ng version
