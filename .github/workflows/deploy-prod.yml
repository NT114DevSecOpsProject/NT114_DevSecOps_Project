name: Deploy to Production (GitOps)

on:
  workflow_dispatch:  # Manual trigger only (production safety)

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY_ALIAS: nt114-devsecops
  ENVIRONMENT: prod
  EKS_CLUSTER_NAME: eks-prod

permissions:
  contents: write  # Required to push image tag updates to Git

jobs:
  # Detect which services changed
  detect-changes:
    name: Detect Changed Services
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.set-outputs.outputs.backend }}
      frontend: ${{ steps.set-outputs.outputs.frontend }}
      api-gateway: ${{ steps.set-outputs.outputs.api-gateway }}
      exercises: ${{ steps.set-outputs.outputs.exercises }}
      scores: ${{ steps.set-outputs.outputs.scores }}
      user-management: ${{ steps.set-outputs.outputs.user-management }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - uses: dorny/paths-filter@v3
        id: filter
        if: github.event_name == 'push'
        with:
          filters: |
            backend:
              - 'microservices/**'
            frontend:
              - 'frontend/**'
            api-gateway:
              - 'microservices/api-gateway/**'
            exercises:
              - 'microservices/exercises-service/**'
            scores:
              - 'microservices/scores-service/**'
            user-management:
              - 'microservices/user-management-service/**'

      - name: Set outputs (force all services on manual dispatch)
        id: set-outputs
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Manual dispatch detected - building all services"
            echo "backend=true" >> $GITHUB_OUTPUT
            echo "frontend=true" >> $GITHUB_OUTPUT
            echo "api-gateway=true" >> $GITHUB_OUTPUT
            echo "exercises=true" >> $GITHUB_OUTPUT
            echo "scores=true" >> $GITHUB_OUTPUT
            echo "user-management=true" >> $GITHUB_OUTPUT
          else
            echo "Push event detected - using path filter results"
            echo "backend=${{ steps.filter.outputs.backend }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ steps.filter.outputs.frontend }}" >> $GITHUB_OUTPUT
            echo "api-gateway=${{ steps.filter.outputs.api-gateway }}" >> $GITHUB_OUTPUT
            echo "exercises=${{ steps.filter.outputs.exercises }}" >> $GITHUB_OUTPUT
            echo "scores=${{ steps.filter.outputs.scores }}" >> $GITHUB_OUTPUT
            echo "user-management=${{ steps.filter.outputs.user-management }}" >> $GITHUB_OUTPUT
          fi

  # Setup Kubernetes secrets for database connections
  setup-secrets:
    name: Setup Database Secrets
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      rds_endpoint: ${{ steps.rds-info.outputs.endpoint }}
      rds_password: ${{ steps.rds-info.outputs.password }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Validate EKS cluster connectivity
        run: |
          echo "üîç Validating EKS cluster connectivity..."
          echo "Target cluster: ${{ env.EKS_CLUSTER_NAME }}"
          echo "Region: ${{ env.AWS_REGION }}"

          # Clear any existing kubeconfig to avoid cache issues
          rm -f ~/.kube/config
          mkdir -p ~/.kube

          # Check cluster status
          echo "Checking cluster status..."
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text)
          echo "Cluster status: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "‚ùå ERROR: EKS cluster is not ACTIVE (status: $CLUSTER_STATUS)"
            exit 1
          fi

          # Configure kubectl
          echo "Configuring kubectl for cluster: ${{ env.EKS_CLUSTER_NAME }}..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

          # Verify kubeconfig
          echo "Current kubeconfig context:"
          kubectl config current-context

          # Test connection
          echo "Testing cluster connection..."
          kubectl cluster-info || {
            echo "‚ùå ERROR: Cannot connect to EKS cluster"
            echo "Possible causes:"
            echo "  1. IAM user lacks EKS access entry"
            echo "  2. Security group blocking access"
            echo "  3. Network connectivity issue"
            exit 1
          }

          echo "‚úÖ EKS cluster validated and kubectl configured"

      - name: Get RDS endpoint and password
        id: rds-info
        run: |
          # Get RDS endpoint from AWS
          RDS_ENDPOINT=$(aws rds describe-db-instances --db-instance-identifier nt114-postgres-prod --region ${{ env.AWS_REGION }} --query 'DBInstances[0].Endpoint.Address' --output text)
          echo "endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "‚úÖ RDS Endpoint: $RDS_ENDPOINT"

          # Use RDS password from GitHub Secrets (managed manually)
          echo "password=${{ secrets.RDS_PASSWORD }}" >> $GITHUB_OUTPUT
          echo "‚úÖ RDS Password retrieved from GitHub Secrets"

      - name: Create prod namespace
        run: |
          echo "Creating prod namespace..."
          kubectl create namespace prod --dry-run=client -o yaml | kubectl apply -f -
          echo "‚úÖ Namespace prod ready"

      - name: Create database secrets
        run: |
          # Replace placeholders in template with values from Terraform
          sed -e "s/RDS_ENDPOINT_PLACEHOLDER/${{ steps.rds-info.outputs.endpoint }}/g" \
              -e "s/RDS_PASSWORD_PLACEHOLDER/${{ steps.rds-info.outputs.password }}/g" \
              k8s/manifests/db-secrets-prod.yaml | kubectl apply -f -

          echo "‚úÖ Database secrets created successfully with Terraform password"

      - name: Create ECR secret in prod namespace
        run: |
          echo "Creating ECR docker-registry secret for prod namespace..."

          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"

          # Create secret
          kubectl create secret docker-registry ecr-secret \
            --docker-server=$ECR_REGISTRY \
            --docker-username=AWS \
            --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
            --namespace=prod \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "‚úÖ ECR secret created for registry: $ECR_REGISTRY"

          # Verify
          kubectl get secret ecr-secret -n prod && echo "‚úÖ Secret verified"

  # Build and push backend services to ECR
  build-backend:
    name: Build Backend Service
    needs: detect-changes
    if: always()
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Continue even if one service fails
      matrix:
        service:
          - name: api-gateway
            path: microservices/api-gateway
          - name: exercises-service
            path: microservices/exercises-service
          - name: scores-service
            path: microservices/scores-service
          - name: user-management-service
            path: microservices/user-management-service
    outputs:
      services_built: ${{ steps.set-output.outputs.services }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd ${{ matrix.service.path }}

            echo "üî® Building Docker image for ${{ matrix.service.name }}..."
            docker buildx build --platform linux/amd64 \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:prod-latest \
              --push .

            echo "‚úÖ Image pushed to ECR:"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:${{ github.sha }}"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:prod-latest"

      - name: Mark service as built
        if: steps.should-build.outputs.enabled == 'true'
        id: set-output
        run: echo "services=${{ matrix.service.name }}" >> $GITHUB_OUTPUT

  # Build and push frontend to ECR
  build-frontend:
    name: Build Frontend
    needs: detect-changes
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Frontend (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd frontend

            echo "üî® Building Frontend Docker image..."
            docker buildx build --platform linux/amd64 \
              -f Dockerfile.prod \
              --build-arg VITE_API_URL="" \
              --build-arg VITE_APP_TITLE="CodeLearn" \
              --build-arg VITE_APP_ENV=production \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/frontend:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/frontend:prod-latest \
              --push .

            echo "‚úÖ Frontend image pushed to ECR"

  # Update Helm values in Git (sequential to avoid conflicts)
  update-helm-values:
    name: Update Helm Values (GitOps)
    needs: [detect-changes, build-backend, build-frontend]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Update all Helm values with new image tags
        run: |
          IMAGE_TAG="${{ github.sha }}"
          UPDATED_SERVICES=""

          # Update backend services
          if [ "${{ needs.detect-changes.outputs.api-gateway }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/api-gateway/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}api-gateway, "
            echo "‚úÖ Updated helm/api-gateway/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.exercises }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/exercises-service/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}exercises-service, "
            echo "‚úÖ Updated helm/exercises-service/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.scores }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/scores-service/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}scores-service, "
            echo "‚úÖ Updated helm/scores-service/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.user-management }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/user-management-service/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}user-management-service, "
            echo "‚úÖ Updated helm/user-management-service/values-prod.yaml"
          fi

          # Update frontend
          if [ "${{ needs.detect-changes.outputs.frontend }}" == "true" ] && [ "${{ needs.build-frontend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/frontend/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}frontend, "
            echo "‚úÖ Updated helm/frontend/values-prod.yaml"
          fi

          # Save updated services list
          echo "UPDATED_SERVICES=${UPDATED_SERVICES%, }" >> $GITHUB_ENV

      - name: Commit and push all Helm value updates (single atomic commit)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add helm/*/values-prod.yaml

          if git diff --cached --quiet; then
            echo "No Helm value changes to commit"
            exit 0
          fi

          # Create and push commit with retry
          git commit -m "chore(prod): update image tags to ${{ github.sha }} [skip ci]" -m "Services: ${{ env.UPDATED_SERVICES }}" -m "Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Try push with retry (3 attempts)
          git pull --rebase origin main && git push origin main || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (echo "ERROR: Failed to push after 3 attempts" && exit 1)

          echo "SUCCESS: Helm values pushed to Git - ArgoCD will auto-sync"
          echo "Updated services: ${{ env.UPDATED_SERVICES }}"

  # Sync ArgoCD and verify deployment
  sync-and-verify:
    name: Sync ArgoCD & Verify Deployment
    needs: [build-backend, build-frontend, update-helm-values, setup-secrets]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    outputs:
      frontend_url: ${{ steps.get-urls.outputs.frontend_url }}
      argocd_url: ${{ steps.get-urls.outputs.argocd_url }}
      argocd_password: ${{ steps.get-urls.outputs.argocd_password }}
      rds_endpoint: ${{ needs.setup-secrets.outputs.rds_endpoint }}
      rds_password: ${{ needs.setup-secrets.outputs.rds_password }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info

      - name: Install ArgoCD if not exists
        run: |
          echo "Checking if ArgoCD is installed..."
          if kubectl get namespace argocd &>/dev/null; then
            echo "‚úÖ ArgoCD namespace exists"
          else
            echo "üì¶ Installing ArgoCD..."
            kubectl create namespace argocd
            kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

            echo "‚è≥ Waiting for ArgoCD to be ready..."
            kubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s || {
              echo "‚ö†Ô∏è Some ArgoCD pods may still be starting, continuing..."
            }

            echo "üåê Exposing ArgoCD UI via LoadBalancer..."
            kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

            echo "‚úÖ ArgoCD installed successfully"
          fi

      - name: Ensure ArgoCD applications exist
        run: |
          echo "Checking ArgoCD applications..."

          # Create prod namespace if it doesn't exist
          kubectl create namespace prod --dry-run=client -o yaml | kubectl apply -f -

          # Create ArgoCD applications for each service
          declare -A services
          services=(
            ["api-gateway"]="helm/api-gateway"
            ["exercises"]="helm/exercises-service"
            ["scores"]="helm/scores-service"
            ["user-management"]="helm/user-management-service"
            ["frontend"]="helm/frontend"
          )

          for service in "${!services[@]}"; do
            APP_NAME="${service}-prod"
            HELM_PATH="${services[$service]}"

            if kubectl get application "$APP_NAME" -n argocd &>/dev/null; then
              echo "‚úÖ Application $APP_NAME already exists"
            else
              echo "üìù Creating ArgoCD application: $APP_NAME (path: $HELM_PATH)"

              cat <<EOF | kubectl apply -f -
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: $APP_NAME
            namespace: argocd
            finalizers:
              - resources-finalizer.argocd.argoproj.io
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}.git
              targetRevision: HEAD
              path: $HELM_PATH
              helm:
                valueFiles:
                  - values-prod.yaml
            destination:
              server: https://kubernetes.default.svc
              namespace: prod
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
              syncOptions:
                - CreateNamespace=true
          EOF
            fi
          done

          echo "‚úÖ All ArgoCD applications verified"

      - name: Force sync ArgoCD applications
        run: |
          echo "Force syncing all prod applications..."
          for app in api-gateway-prod exercises-prod frontend-prod scores-prod user-management-prod; do
            echo "Syncing $app..."
            kubectl patch application $app -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"HEAD"}}}' || echo "App $app may not exist yet"
          done

          echo "Waiting 30s for ArgoCD to process sync requests..."
          sleep 30

      - name: Wait for pods to be healthy
        run: |
          echo "Waiting for all pods to be healthy (max 5 minutes)..."

          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n prod --no-headers 2>/dev/null | grep -c "1/1.*Running" || echo "0")
            TOTAL_PODS=$(kubectl get pods -n prod --no-headers 2>/dev/null | wc -l | tr -d '[:space:]')

            # Ensure numeric values (remove any whitespace/newlines)
            READY_PODS=$(echo "$READY_PODS" | tr -d '[:space:]')
            TOTAL_PODS=$(echo "$TOTAL_PODS" | tr -d '[:space:]')

            # Default to 0 if empty
            READY_PODS=${READY_PODS:-0}
            TOTAL_PODS=${TOTAL_PODS:-0}

            echo "Attempt $i/30: $READY_PODS/$TOTAL_PODS pods ready"

            if [ "$READY_PODS" -ge "8" ] && [ "$READY_PODS" -eq "$TOTAL_PODS" ]; then
              echo "All pods are healthy!"
              kubectl get pods -n prod
              break
            fi

            if [ "$i" == "30" ]; then
              echo "Warning: Not all pods ready after 5 minutes"
              kubectl get pods -n prod
              kubectl get applications -n argocd
            fi

            sleep 10
          done

      - name: Get access URLs
        id: get-urls
        continue-on-error: true
        run: |
          echo "Getting ArgoCD credentials..."
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d || echo "N/A")

          echo "Getting LoadBalancer URLs (waiting up to 3 minutes)..."

          # Get ArgoCD URL
          ARGOCD_URL=""
          for i in {1..18}; do
            ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$ARGOCD_URL" ] && ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$ARGOCD_URL" ]; then
              echo "ArgoCD URL found: $ARGOCD_URL"
              break
            fi
            echo "Waiting for ArgoCD LoadBalancer... ($i/18)"
            sleep 10
          done

          # Get Frontend URL
          FRONTEND_URL=""
          for i in {1..18}; do
            FRONTEND_URL=$(kubectl get svc frontend-prod -n prod -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$FRONTEND_URL" ] && FRONTEND_URL=$(kubectl get svc frontend-prod -n prod -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$FRONTEND_URL" ]; then
              echo "Frontend URL found: $FRONTEND_URL"
              break
            fi
            echo "Waiting for Frontend LoadBalancer... ($i/18)"
            sleep 10
          done

          [ -z "$ARGOCD_URL" ] && ARGOCD_URL="Pending (LoadBalancer provisioning)"
          [ -z "$FRONTEND_URL" ] && FRONTEND_URL="Pending (LoadBalancer provisioning)"

          echo "argocd_password=$ARGOCD_PASSWORD" >> $GITHUB_OUTPUT
          echo "argocd_url=$ARGOCD_URL" >> $GITHUB_OUTPUT
          echo "frontend_url=$FRONTEND_URL" >> $GITHUB_OUTPUT

  # Deploy monitoring stack
  deploy-monitoring:
    name: Deploy Monitoring Stack
    needs: [sync-and-verify]
    runs-on: ubuntu-latest
    outputs:
      grafana_url: ${{ steps.get-grafana-url.outputs.grafana_url }}
      grafana_password: ${{ steps.get-grafana-password.outputs.password }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info

      - name: Create monitoring namespace
        run: |
          echo "Creating monitoring namespace..."
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          echo "‚úÖ Namespace monitoring ready"

      - name: Generate and create Grafana admin secret
        id: create-grafana-secret
        run: |
          # Generate strong password
          GRAFANA_PASSWORD=$(openssl rand -base64 20)

          # Create secret
          kubectl create secret generic grafana-admin-credentials \
            --from-literal=admin-user=admin \
            --from-literal=admin-password="$GRAFANA_PASSWORD" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "‚úÖ Grafana admin credentials created"
          echo "password=$GRAFANA_PASSWORD" >> $GITHUB_OUTPUT

      - name: Add Helm repository
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          echo "‚úÖ Helm repo added and updated"

      - name: Deploy kube-prometheus-stack
        run: |
          echo "Deploying kube-prometheus-stack..."

          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --version 80.9.1 \
            -f helm/kube-prometheus-stack/values-prod.yaml \
            --wait --timeout 10m

          echo "‚úÖ kube-prometheus-stack deployed"

      - name: Wait for monitoring pods to be ready
        run: |
          echo "Waiting for monitoring pods to be ready..."

          # Wait up to 5 minutes for all pods
          kubectl wait --for=condition=Ready pods --all -n monitoring --timeout=300s || {
            echo "‚ö†Ô∏è Some pods still starting, checking status..."
            kubectl get pods -n monitoring
          }

          echo "‚úÖ Monitoring stack pods ready"
          kubectl get pods -n monitoring

      - name: Deploy ArgoCD ServiceMonitors
        run: |
          echo "Deploying ArgoCD ServiceMonitors..."
          kubectl apply -f k8s/monitoring/servicemonitor-argocd.yaml
          echo "‚úÖ ArgoCD ServiceMonitors deployed"

      - name: Get Grafana LoadBalancer URL
        id: get-grafana-url
        run: |
          echo "Waiting for Grafana LoadBalancer..."

          GRAFANA_URL=""
          for i in {1..18}; do
            GRAFANA_URL=$(kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$GRAFANA_URL" ] && GRAFANA_URL=$(kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$GRAFANA_URL" ]; then
              echo "Grafana URL found: $GRAFANA_URL"
              break
            fi
            echo "Waiting for LoadBalancer... ($i/18)"
            sleep 10
          done

          [ -z "$GRAFANA_URL" ] && GRAFANA_URL="Pending (LoadBalancer provisioning)"
          echo "grafana_url=$GRAFANA_URL" >> $GITHUB_OUTPUT

      - name: Get Grafana admin password
        id: get-grafana-password
        run: |
          PASSWORD=$(kubectl get secret grafana-admin-credentials -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d)
          echo "password=$PASSWORD" >> $GITHUB_OUTPUT

      - name: Verify Prometheus targets
        run: |
          echo "Checking Prometheus target status..."
          kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090 &
          PF_PID=$!
          sleep 5

          # Simple connectivity check
          curl -s http://localhost:9090/api/v1/targets || echo "‚ö†Ô∏è Cannot reach Prometheus API"

          kill $PF_PID || true
          echo "‚úÖ Prometheus verification complete"

  # Output access information
  output-access-info:
    name: Output Access Information
    needs: [sync-and-verify, deploy-monitoring]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Display access information
        run: |
          echo "=========================================="
          echo "  DEPLOYMENT COMPLETE!"
          echo "=========================================="
          echo ""
          echo "Frontend Application:"
          echo "  URL: http://${{ needs.sync-and-verify.outputs.frontend_url }}"
          echo ""
          echo "ArgoCD Dashboard:"
          echo "  URL:      http://${{ needs.sync-and-verify.outputs.argocd_url }}"
          echo "  Username: admin"
          echo "  Password: ${{ needs.sync-and-verify.outputs.argocd_password }}"
          echo ""
          echo "RDS PostgreSQL Database:"
          echo "  Host:     ${{ needs.sync-and-verify.outputs.rds_endpoint }}"
          echo "  Port:     5432"
          echo "  Database: postgres"
          echo "  Username: postgres"
          echo "  Password: (stored in GitHub Secret: RDS_PASSWORD)"
          echo ""
          echo "=========================================="

      - name: Deployment summary
        run: |
          echo "## üöÄ Production Deployment Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üåê Access URLs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Frontend Application:**" >> $GITHUB_STEP_SUMMARY
          echo "- http://${{ needs.sync-and-verify.outputs.frontend_url }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ArgoCD Dashboard:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: http://${{ needs.sync-and-verify.outputs.argocd_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: \`${{ needs.sync-and-verify.outputs.argocd_password }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Monitoring Stack" >> $GITHUB_STEP_SUMMARY
          echo "**Grafana Dashboard:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: http://${{ needs.deploy-monitoring.outputs.grafana_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: \`${{ needs.deploy-monitoring.outputs.grafana_password }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Prometheus:**" >> $GITHUB_STEP_SUMMARY
          echo "- Access via: \`kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090\`" >> $GITHUB_STEP_SUMMARY
          echo "- Then visit: http://localhost:9090" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üóÑÔ∏è RDS PostgreSQL Database" >> $GITHUB_STEP_SUMMARY
          echo "- Host: \`${{ needs.sync-and-verify.outputs.rds_endpoint }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Port: \`5432\`" >> $GITHUB_STEP_SUMMARY
          echo "- Database: \`postgres\`" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`postgres\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: **(stored in GitHub Secret: RDS_PASSWORD)**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> üí° To get password: Go to Settings ‚Üí Secrets ‚Üí RDS_PASSWORD" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üì¶ Deployment Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment:** Production" >> $GITHUB_STEP_SUMMARY
          echo "- **Cluster:** eks-prod" >> $GITHUB_STEP_SUMMARY
          echo "- **Namespace:** prod" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ‚úÖ What Was Deployed" >> $GITHUB_STEP_SUMMARY
          echo "- Docker images built and pushed to ECR" >> $GITHUB_STEP_SUMMARY
          echo "- Helm values updated with new image tags" >> $GITHUB_STEP_SUMMARY
          echo "- ArgoCD applications synced" >> $GITHUB_STEP_SUMMARY
          echo "- Prometheus + Grafana monitoring stack deployed (HA mode)" >> $GITHUB_STEP_SUMMARY
          echo "- All pods verified healthy" >> $GITHUB_STEP_SUMMARY
          echo "- LoadBalancers provisioned and accessible" >> $GITHUB_STEP_SUMMARY
