name: Complete CI/CD Pipeline - Production

on:
  workflow_dispatch:  # Manual trigger for production deployment

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY_ALIAS: nt114-devsecops
  ENVIRONMENT: prod
  EKS_CLUSTER_NAME: eks-prod

permissions:
  contents: write
  packages: write
  security-events: write

jobs:
  # =============================================
  # Stage 1: Code Quality & Security Analysis
  # =============================================
  code-analysis:
    name: SonarCloud & Security Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Trivy CLI
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
          trivy --version
      
      - name: Update Trivy Database
        run: |
          echo "=== Updating Trivy vulnerability database ==="
          trivy image --download-db-only
          echo "âœ… Database updated successfully"
      
      # ============================================
      # FRONTEND Analysis
      # ============================================
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Install Frontend Dependencies
        working-directory: frontend
        run: npm ci
      
      - name: ðŸ” Trivy Scan - Frontend Dependencies
        working-directory: frontend
        continue-on-error: true
        run: |
          echo "### ðŸ” Frontend Dependencies Scan" >> $GITHUB_STEP_SUMMARY
          trivy fs --format table --security-checks vuln package-lock.json
      
      - name: SonarCloud Scan - Frontend
        uses: SonarSource/sonarcloud-github-action@v3.1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          projectBaseDir: frontend
          args: >
            -Dsonar.organization=nt114devsecopsproject
            -Dsonar.projectKey=nt114devsecopsproject_frontend
            -Dsonar.sources=src
            -Dsonar.tests=src
            -Dsonar.test.inclusions=**/*.test.js,**/*.test.jsx,**/*.test.ts,**/*.test.tsx
            -Dsonar.exclusions=**/node_modules/**,**/build/**,**/dist/**,**/coverage/**

      # ============================================
      # BACKEND SERVICES Analysis
      # ============================================
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
      
      # API-GATEWAY
      - name: Install API-GATEWAY Dependencies
        working-directory: microservices/api-gateway
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run Tests - API-GATEWAY
        working-directory: microservices/api-gateway
        run: |
          echo "=== Running API Gateway Tests ==="
          python -m pytest --cov=. --cov-report=xml:coverage.xml --cov-report=term -v
          echo ""
          echo "=== Coverage Generated ==="
          ls -la coverage.xml
        continue-on-error: true
      
      - name: SonarCloud Scan - API Gateway
        uses: SonarSource/sonarcloud-github-action@v3.1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          projectBaseDir: microservices/api-gateway
          args: >
            -Dsonar.organization=nt114devsecopsproject
            -Dsonar.projectKey=nt114devsecopsproject_api-gateway
            -Dsonar.python.coverage.reportPaths=coverage.xml
            -Dsonar.python.version=3.9

      - name: Trivy Scan - API-GATEWAY
        working-directory: microservices/api-gateway
        continue-on-error: true
        run: trivy fs --format table --security-checks vuln requirements.txt

      # EXERCISES-SERVICE
      - name: Install Exercises-Service Dependencies
        working-directory: microservices/exercises-service
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run Tests - Exercises-Service
        working-directory: microservices/exercises-service
        run: |
          echo "=== Running Exercises Service Tests ==="
          PYTHONPATH=. python -m pytest --cov=app --cov-report=xml:coverage.xml --cov-report=term -v
          echo ""
          echo "=== Coverage Generated ==="
          ls -la coverage.xml
        continue-on-error: true
      
      - name: Fix Coverage Paths - Exercises Service
        working-directory: microservices/exercises-service
        run: |
          echo "=== Fix Coverage XML paths ==="
          sed -i 's|<source>[^<]*</source>|<source>.</source>|g' coverage.xml
          sed -i 's|filename="\([^"]*\)"|filename="app/\1"|g' coverage.xml
          echo "=== Verify Fixed Coverage ==="
          head -40 coverage.xml

      - name: SonarCloud Scan - Exercises Service
        uses: SonarSource/sonarcloud-github-action@v3.1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          projectBaseDir: microservices/exercises-service
          args: >
            -Dsonar.organization=nt114devsecopsproject
            -Dsonar.projectKey=nt114devsecopsproject_exercises-service
            -Dsonar.sources=app
            -Dsonar.tests=tests
            -Dsonar.python.coverage.reportPaths=coverage.xml
            -Dsonar.python.version=3.9

      - name: Trivy Scan - Exercises Service
        working-directory: microservices/exercises-service
        continue-on-error: true
        run: trivy fs --format table --security-checks vuln requirements.txt

      # SCORES-SERVICE
      - name: Install Scores-Service Dependencies
        working-directory: microservices/scores-service
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run Tests - Scores-Service
        working-directory: microservices/scores-service
        run: |
          echo "=== Running Scores Service Tests ==="
          PYTHONPATH=. python -m pytest --cov=app --cov-report=xml:coverage.xml --cov-report=term -v || true
          echo ""
          echo "=== Coverage Generated ==="
          ls -la coverage.xml
        continue-on-error: true

      - name: Fix Coverage Paths - Scores Service
        working-directory: microservices/scores-service
        run: |
          echo "=== Fix Coverage XML paths ==="
          sed -i 's|<source>[^<]*</source>|<source>.</source>|g' coverage.xml
          sed -i 's|filename="\([^"]*\)"|filename="app/\1"|g' coverage.xml
          echo "=== Verify Fixed Coverage ==="
          head -40 coverage.xml

      - name: SonarCloud Scan - Scores Service
        uses: SonarSource/sonarcloud-github-action@v3.1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          projectBaseDir: microservices/scores-service
          args: >
            -Dsonar.organization=nt114devsecopsproject
            -Dsonar.projectKey=nt114devsecopsproject_scores-service
            -Dsonar.sources=app
            -Dsonar.tests=tests
            -Dsonar.python.coverage.reportPaths=coverage.xml
            -Dsonar.python.version=3.9

      - name: Trivy Scan - Scores Service
        working-directory: microservices/scores-service
        continue-on-error: true
        run: trivy fs --format table --security-checks vuln requirements.txt

      # USER-MANAGEMENT-SERVICE
      - name: Install User-Management-Service Dependencies
        working-directory: microservices/user-management-service
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run Tests - User-Management-Service
        working-directory: microservices/user-management-service
        run: |
          echo "=== Running User Management Service Tests ==="
          PYTHONPATH=. python -m pytest --cov=app --cov-report=xml:coverage.xml --cov-report=term -v || true
          echo ""
          echo "=== Coverage Generated ==="
          ls -la coverage.xml
        continue-on-error: true

      - name: Fix Coverage Paths - User Management Service
        working-directory: microservices/user-management-service
        run: |
          echo "=== Fix Coverage XML paths ==="
          sed -i 's|<source>[^<]*</source>|<source>.</source>|g' coverage.xml
          sed -i 's|filename="\([^"]*\)"|filename="app/\1"|g' coverage.xml
          echo "=== Verify Fixed Coverage ==="
          head -40 coverage.xml
      
      - name: SonarCloud Scan - User Management Service
        uses: SonarSource/sonarcloud-github-action@v3.1.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          projectBaseDir: microservices/user-management-service
          args: >
            -Dsonar.organization=nt114devsecopsproject
            -Dsonar.projectKey=nt114devsecopsproject_user-management-service
            -Dsonar.sources=app
            -Dsonar.tests=tests
            -Dsonar.python.coverage.reportPaths=coverage.xml
            -Dsonar.python.version=3.9

      - name: Trivy Scan - User Management Service
        working-directory: microservices/user-management-service
        continue-on-error: true
        run: trivy fs --format table --security-checks vuln requirements.txt

      - name: ðŸ“Š Generate Security Summary
        continue-on-error: true
        run: |
          echo "## ðŸ›¡ï¸ Security Scan Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ” SonarCloud Analysis" >> $GITHUB_STEP_SUMMARY
          echo "View detailed reports at: https://sonarcloud.io/organizations/nt114devsecopsproject/projects" >> $GITHUB_STEP_SUMMARY

  # =============================================
  # Stage 2: Detect Changed Services
  # =============================================
  detect-changes:
    name: Detect Changed Services
    runs-on: ubuntu-latest
    needs: code-analysis
    outputs:
      backend: ${{ steps.set-outputs.outputs.backend }}
      frontend: ${{ steps.set-outputs.outputs.frontend }}
      api-gateway: ${{ steps.set-outputs.outputs.api-gateway }}
      exercises: ${{ steps.set-outputs.outputs.exercises }}
      scores: ${{ steps.set-outputs.outputs.scores }}
      user-management: ${{ steps.set-outputs.outputs.user-management }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - uses: dorny/paths-filter@v3
        id: filter
        if: github.event_name == 'push'
        with:
          filters: |
            backend:
              - 'microservices/**'
            frontend:
              - 'frontend/**'
            api-gateway:
              - 'microservices/api-gateway/**'
            exercises:
              - 'microservices/exercises-service/**'
            scores:
              - 'microservices/scores-service/**'
            user-management:
              - 'microservices/user-management-service/**'

      - name: Set outputs (force all services on manual dispatch)
        id: set-outputs
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Manual dispatch detected - building all services"
            echo "backend=true" >> $GITHUB_OUTPUT
            echo "frontend=true" >> $GITHUB_OUTPUT
            echo "api-gateway=true" >> $GITHUB_OUTPUT
            echo "exercises=true" >> $GITHUB_OUTPUT
            echo "scores=true" >> $GITHUB_OUTPUT
            echo "user-management=true" >> $GITHUB_OUTPUT
          else
            echo "Push event detected - using path filter results"
            echo "backend=${{ steps.filter.outputs.backend }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ steps.filter.outputs.frontend }}" >> $GITHUB_OUTPUT
            echo "api-gateway=${{ steps.filter.outputs.api-gateway }}" >> $GITHUB_OUTPUT
            echo "exercises=${{ steps.filter.outputs.exercises }}" >> $GITHUB_OUTPUT
            echo "scores=${{ steps.filter.outputs.scores }}" >> $GITHUB_OUTPUT
            echo "user-management=${{ steps.filter.outputs.user-management }}" >> $GITHUB_OUTPUT
          fi

  # =============================================
  # Stage 3: Setup Kubernetes Secrets
  # =============================================
  setup-secrets:
    name: Setup Database Secrets
    needs: detect-changes
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    outputs:
      rds_endpoint: ${{ steps.rds-info.outputs.endpoint }}
      rds_password: ${{ steps.rds-info.outputs.password }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Validate EKS cluster connectivity
        run: |
          echo "âœ… Validating EKS cluster connectivity..."
          echo "Target cluster: ${{ env.EKS_CLUSTER_NAME }}"
          echo "Region: ${{ env.AWS_REGION }}"

          rm -f ~/.kube/config
          mkdir -p ~/.kube

          echo "Checking cluster status..."
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text)
          echo "Cluster status: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "âŒ ERROR: EKS cluster is not ACTIVE (status: $CLUSTER_STATUS)"
            exit 1
          fi

          echo "Configuring kubectl for cluster: ${{ env.EKS_CLUSTER_NAME }}..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

          echo "Current kubeconfig context:"
          kubectl config current-context 2>&1 | grep -v "metrics.k8s.io" || true

          echo "Testing cluster connection..."
          kubectl cluster-info 2>&1 | grep -v "metrics.k8s.io" || {
            echo "âŒ ERROR: Cannot connect to EKS cluster"
            exit 1
          }

          echo "âœ… EKS cluster validated and kubectl configured"

      - name: Get RDS endpoint and password
        id: rds-info
        run: |
          RDS_ENDPOINT=$(aws rds describe-db-instances --db-instance-identifier nt114-postgres-prod --region ${{ env.AWS_REGION }} --query 'DBInstances[0].Endpoint.Address' --output text)
          echo "endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "âœ… RDS Endpoint: $RDS_ENDPOINT"
          echo "password=${{ secrets.RDS_PASSWORD }}" >> $GITHUB_OUTPUT
          echo "âœ… RDS Password retrieved from GitHub Secrets"

      - name: Create prod namespace
        run: |
          echo "Creating prod namespace..."
          kubectl create namespace prod --dry-run=client -o yaml 2>&1 | grep -v "metrics.k8s.io" | kubectl apply -f - 2>&1 | grep -v "metrics.k8s.io" || true
          echo "âœ… Namespace prod ready"

      - name: Create database secrets
        run: |
          sed -e "s/RDS_ENDPOINT_PLACEHOLDER/${{ steps.rds-info.outputs.endpoint }}/g" \
              -e "s/RDS_PASSWORD_PLACEHOLDER/${{ steps.rds-info.outputs.password }}/g" \
              k8s/manifests/db-secrets-prod.yaml | kubectl apply -f - 2>&1 | grep -v "metrics.k8s.io" || true
          echo "âœ… Database secrets created successfully"

      - name: Create ECR secret in prod namespace
        run: |
          echo "Creating ECR docker-registry secret for prod namespace..."
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"

          kubectl create secret docker-registry ecr-secret \
            --docker-server=$ECR_REGISTRY \
            --docker-username=AWS \
            --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
            --namespace=prod \
            --dry-run=client -o yaml 2>&1 | grep -v "metrics.k8s.io" | kubectl apply -f - 2>&1 | grep -v "metrics.k8s.io" || true

          echo "âœ… ECR secret created for registry: $ECR_REGISTRY"
          kubectl get secret ecr-secret -n prod 2>&1 | grep -v "metrics.k8s.io" && echo "âœ… Secret verified"

  # =============================================
  # Stage 4: Build Backend Services
  # =============================================
  build-backend:
    name: Build Backend Service
    needs: [detect-changes, setup-secrets]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        service:
          - name: api-gateway
            path: microservices/api-gateway
          - name: exercises-service
            path: microservices/exercises-service
          - name: scores-service
            path: microservices/scores-service
          - name: user-management-service
            path: microservices/user-management-service
    outputs:
      services_built: ${{ steps.set-output.outputs.services }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install Trivy
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sudo sh -s -- -b /usr/local/bin
          trivy --version

      - name: Build Docker Image
        run: |
          cd ${{ matrix.service.path }}
          docker build -t ${{ matrix.service.name }}:${{ github.sha }} .

      - name: ðŸ” Trivy Scan Docker Image
        continue-on-error: true
        run: |
          trivy image --severity CRITICAL,HIGH --format table ${{ matrix.service.name }}:${{ github.sha }}

      - name: Push to ECR (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd ${{ matrix.service.path }}

            echo "âœ… Building and pushing Docker image for ${{ matrix.service.name }}..."
            docker buildx build --platform linux/amd64 \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:prod-latest \
              --push .

            echo "âœ… Image pushed to ECR:"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:${{ github.sha }}"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/${{ matrix.service.name }}:prod-latest"

      - name: Mark service as built
        id: set-output
        run: echo "services=${{ matrix.service.name }}" >> $GITHUB_OUTPUT

  # =============================================
  # Stage 5: Build Frontend
  # =============================================
  build-frontend:
    name: Build Frontend
    needs: [detect-changes, setup-secrets]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install Trivy
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sudo sh -s -- -b /usr/local/bin
          trivy --version

      - name: Build Docker Image
        run: |
          cd frontend
          docker build -f Dockerfile.prod -t frontend:${{ github.sha }} .

      - name: ðŸ” Trivy Scan Docker Image
        continue-on-error: true
        run: |
          trivy image --severity CRITICAL,HIGH --format table frontend:${{ github.sha }}

      - name: Push to ECR (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd frontend

            echo "âœ… Building Frontend Docker image..."
            docker buildx build --platform linux/amd64 \
              -f Dockerfile.prod \
              --build-arg VITE_API_URL="" \
              --build-arg VITE_APP_TITLE="CodeLearn" \
              --build-arg VITE_APP_ENV=production \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/frontend:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/prod/frontend:prod-latest \
              --push .

            echo "âœ… Frontend image pushed to ECR"

  # =============================================
  # Stage 6: Update Helm Values (GitOps)
  # =============================================
  update-helm-values:
    name: Update Helm Values (GitOps)
    needs: [detect-changes, build-backend, build-frontend]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Update all Helm values with new image tags
        run: |
          IMAGE_TAG="${{ github.sha }}"
          UPDATED_SERVICES=""

          if [ "${{ needs.detect-changes.outputs.api-gateway }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/api-gateway/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}api-gateway, "
            echo "âœ… Updated helm/api-gateway/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.exercises }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/exercises-service/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}exercises-service, "
            echo "âœ… Updated helm/exercises-service/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.scores }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/scores-service/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}scores-service, "
            echo "âœ… Updated helm/scores-service/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.user-management }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/user-management-service/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}user-management-service, "
            echo "âœ… Updated helm/user-management-service/values-prod.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.frontend }}" == "true" ] && [ "${{ needs.build-frontend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/frontend/values-prod.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}frontend, "
            echo "âœ… Updated helm/frontend/values-prod.yaml"
          fi

          echo "UPDATED_SERVICES=${UPDATED_SERVICES%, }" >> $GITHUB_ENV

      - name: Commit and push all Helm value updates
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add helm/*/values-prod.yaml

          if git diff --cached --quiet; then
            echo "No Helm value changes to commit"
            exit 0
          fi

          git commit -m "chore(prod): update image tags to ${{ github.sha }} [skip ci]" \
            -m "Services: ${{ env.UPDATED_SERVICES }}" \
            -m "Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          git pull --rebase origin main && git push origin main || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (echo "âŒ Failed to push after 3 attempts" && exit 1)

          echo "âœ… Helm values pushed to Git - ArgoCD will auto-sync"
          echo "Updated services: ${{ env.UPDATED_SERVICES }}"

  # =============================================
  # Stage 7: Sync ArgoCD & Verify Deployment
  # =============================================
  sync-and-verify:
    name: Sync ArgoCD & Verify Deployment
    needs: [build-backend, build-frontend, update-helm-values, setup-secrets]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    outputs:
      frontend_url: ${{ steps.get-urls.outputs.frontend_url }}
      argocd_url: ${{ steps.get-urls.outputs.argocd_url }}
      argocd_password: ${{ steps.get-urls.outputs.argocd_password }}
      grafana_url: ${{ steps.monitoring-info.outputs.grafana_url }}
      grafana_password: ${{ steps.monitoring-info.outputs.grafana_password }}
      rds_endpoint: ${{ needs.setup-secrets.outputs.rds_endpoint }}
      rds_password: ${{ needs.setup-secrets.outputs.rds_password }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info 2>&1 | grep -v "metrics.k8s.io" || true

      - name: Install Metrics Server
        run: |
          echo "Installing Metrics Server to enable metrics API..."
          set -x

          check_metrics_health() {
            POD_STATUS=$(kubectl get pods -n kube-system -l k8s-app=metrics-server -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
            API_STATUS=$(kubectl get apiservice v1beta1.metrics.k8s.io -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "NotFound")
            if [ "$POD_STATUS" = "Running" ] && [ "$API_STATUS" = "True" ]; then
              return 0
            else
              return 1
            fi
          }

          install_metrics_server() {
            kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 2>&1 | grep -v "metrics.k8s.io" || true
            sleep 5
            kubectl patch deployment metrics-server -n kube-system --type='json' -p='[
              {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"},
              {"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-preferred-address-types=InternalIP"}
            ]' 2>&1 | grep -v "metrics.k8s.io" || true
            kubectl patch deployment metrics-server -n kube-system --type='json' -p='[
              {"op": "add", "path": "/spec/template/spec/tolerations", "value": [{"operator": "Exists"}]}
            ]' 2>&1 | grep -v "metrics.k8s.io" || true
            kubectl wait --for=condition=Ready pods -l k8s-app=metrics-server -n kube-system --timeout=120s 2>&1 | grep -v "metrics.k8s.io" || return 1
            kubectl wait --for=condition=Available apiservice/v1beta1.metrics.k8s.io --timeout=120s || return 1
            return 0
          }

          if check_metrics_health; then
            echo "Metrics Server already healthy"
          else
            install_metrics_server || echo "WARNING: Metrics Server installation failed"
          fi
          set +x

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Checkout code for ArgoCD values
        uses: actions/checkout@v4

      - name: Ensure clean ArgoCD state
        id: argocd-cleanup
        run: |
          set -e

          detect_installation_method() {
            if ! kubectl get deployment argocd-server -n argocd &>/dev/null; then
              echo "not_installed"
              return
            fi
            MANAGED_BY=$(kubectl get deployment argocd-server -n argocd -o jsonpath='{.metadata.labels.app\.kubernetes\.io/managed-by}' 2>/dev/null)
            if [ "$MANAGED_BY" = "Helm" ]; then
              echo "helm_managed"
            else
              echo "kubectl_installed"
            fi
          }

          cleanup_kubectl_argocd() {
            echo "âš ï¸ Cleaning up kubectl-installed ArgoCD..."
            kubectl delete clusterrole -l app.kubernetes.io/part-of=argocd --ignore-not-found=true --wait=false
            kubectl delete clusterrolebinding -l app.kubernetes.io/part-of=argocd --ignore-not-found=true --wait=false
            kubectl delete clusterrole argocd-application-controller argocd-applicationset-controller argocd-server --ignore-not-found=true --wait=false
            kubectl delete clusterrolebinding argocd-application-controller argocd-applicationset-controller argocd-server --ignore-not-found=true --wait=false
            kubectl delete applications --all -n argocd --wait=false --timeout=30s 2>/dev/null || true
            kubectl delete namespace argocd --wait=false --grace-period=0 --force 2>/dev/null || true
            kubectl patch namespace argocd -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
            for i in {1..30}; do
              if ! kubectl get namespace argocd &>/dev/null; then
                echo "âœ… Namespace deleted"
                break
              fi
              sleep 2
            done
            return 0
          }

          INSTALL_METHOD=$(detect_installation_method)
          case "$INSTALL_METHOD" in
            "not_installed") echo "âœ… ArgoCD not installed - will perform fresh installation" ;;
            "helm_managed") echo "âœ… ArgoCD managed by Helm - no cleanup needed" ;;
            "kubectl_installed") cleanup_kubectl_argocd ;;
          esac

      - name: Install or upgrade ArgoCD
        run: |
          set -e

          wait_for_critical_components() {
            kubectl rollout status statefulset/argocd-application-controller -n argocd --timeout=180s || return 1
            for deployment in argocd-server argocd-repo-server; do
              kubectl rollout status deployment/$deployment -n argocd --timeout=120s || return 1
            done
            return 0
          }

          if kubectl get deployment argocd-server -n argocd &>/dev/null; then
            HELM_ACTION="upgrade"
          else
            kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
            HELM_ACTION="install"
          fi

          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update

          if [ "$HELM_ACTION" = "upgrade" ]; then
            helm upgrade argocd argo/argo-cd --namespace argocd -f helm/argocd/values-prod.yaml --timeout 10m --atomic --cleanup-on-fail
          else
            helm install argocd argo/argo-cd --namespace argocd -f helm/argocd/values-prod.yaml --timeout 10m --atomic
          fi

          wait_for_critical_components || exit 1
          echo "âœ… ArgoCD installation/upgrade completed"

      - name: Validate ArgoCD Helm management
        run: |
          MANAGED_BY=$(kubectl get deployment argocd-server -n argocd -o jsonpath='{.metadata.labels.app\.kubernetes\.io/managed-by}')
          if [ "$MANAGED_BY" != "Helm" ]; then
            echo "âŒ ArgoCD not managed by Helm"
            exit 1
          fi
          echo "âœ… ArgoCD correctly managed by Helm"

      - name: Ensure ArgoCD applications exist
        run: |
          set -e
          kubectl create namespace prod --dry-run=client -o yaml | kubectl apply -f -

          declare -A services
          services=(
            ["api-gateway"]="helm/api-gateway"
            ["exercises"]="helm/exercises-service"
            ["scores"]="helm/scores-service"
            ["user-management"]="helm/user-management-service"
            ["frontend"]="helm/frontend"
          )

          for service in "${!services[@]}"; do
            APP_NAME="${service}-prod"
            HELM_PATH="${services[$service]}"

            if kubectl get application "$APP_NAME" -n argocd &>/dev/null; then
              echo "Application $APP_NAME already exists"
            else
              echo "Creating ArgoCD application: $APP_NAME"
              cat <<EOF | kubectl apply -f -
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: $APP_NAME
            namespace: argocd
            finalizers:
              - resources-finalizer.argocd.argoproj.io
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}.git
              targetRevision: HEAD
              path: $HELM_PATH
              helm:
                valueFiles:
                  - values-prod.yaml
            destination:
              server: https://kubernetes.default.svc
              namespace: prod
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
              syncOptions:
                - CreateNamespace=true
          EOF
            fi
          done

          # Setup monitoring
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          if ! kubectl get secret grafana-admin-credentials -n monitoring &>/dev/null; then
            GRAFANA_PASSWORD=$(openssl rand -base64 20)
            kubectl create secret generic grafana-admin-credentials \
              --from-literal=admin-user=admin \
              --from-literal=admin-password="$GRAFANA_PASSWORD" \
              -n monitoring
          fi
          kubectl apply -f argocd/applications/monitoring-production-ready.yaml

      - name: Force sync ArgoCD applications
        run: |
          set -e
          APPS=("api-gateway-prod" "exercises-prod" "frontend-prod" "scores-prod" "user-management-prod" "monitoring")

          for app in "${APPS[@]}"; do
            if kubectl get application $app -n argocd &>/dev/null; then
              kubectl patch application $app -n argocd --type merge \
                -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"HEAD"}}}'
              echo "âœ… Sync triggered for $app"
            fi
          done
          sleep 30

      - name: Verify ArgoCD applications synced
        run: |
          APPS=("api-gateway-prod" "exercises-prod" "frontend-prod" "scores-prod" "user-management-prod" "monitoring")

          for app in "${APPS[@]}"; do
            SYNC_STATUS=$(kubectl get application $app -n argocd -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
            HEALTH_STATUS=$(kubectl get application $app -n argocd -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown")
            echo "$app: Sync=$SYNC_STATUS, Health=$HEALTH_STATUS"
          done
          kubectl get applications -n argocd

      - name: Check cluster capacity
        run: |
          echo "=== Cluster Node Status ==="
          kubectl get nodes -o wide
          echo ""
          echo "=== Pending Pods ==="
          kubectl get pods -A --field-selector status.phase=Pending

      - name: Wait for nodes to be ready
        run: |
          echo "Waiting for cluster nodes to be Ready..."
          MAX_WAIT=600
          ELAPSED=0

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            READY_NODES=$(kubectl get nodes --no-headers 2>/dev/null | grep -c " Ready " || echo "0")
            echo "[$ELAPSED s] Ready nodes: $READY_NODES"
            if [ "$READY_NODES" -ge 2 ]; then
              echo "âœ… $READY_NODES nodes are Ready"
              break
            fi
            sleep 15
            ELAPSED=$((ELAPSED + 15))
          done

      - name: Install Cluster Autoscaler
        run: |
          if kubectl get deployment cluster-autoscaler -n kube-system &>/dev/null; then
            echo "Cluster Autoscaler already installed"
          else
            curl -s https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml | \
            sed "s/<YOUR CLUSTER NAME>/eks-prod/g" | kubectl apply -f -

            kubectl -n kube-system annotate deployment.apps/cluster-autoscaler \
              cluster-autoscaler.kubernetes.io/safe-to-evict="false" --overwrite
            kubectl -n kube-system set image deployment/cluster-autoscaler \
              cluster-autoscaler=registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0
            kubectl -n kube-system patch deployment cluster-autoscaler --type='json' -p='[
              {"op": "add", "path": "/spec/template/spec/tolerations", "value": [{"operator": "Exists"}]}
            ]'
            kubectl wait --for=condition=available --timeout=240s deployment/cluster-autoscaler -n kube-system || true
          fi

      - name: Install eksctl CLI
        run: |
          if command -v eksctl &>/dev/null; then
            echo "eksctl already installed"
          else
            curl -sLO "https://github.com/weaveworks/eksctl/releases/download/v0.167.0/eksctl_Linux_amd64.tar.gz"
            tar -xzf eksctl_Linux_amd64.tar.gz
            sudo mv eksctl /usr/local/bin/
            rm eksctl_Linux_amd64.tar.gz
          fi

      - name: Verify OIDC Provider
        run: |
          OIDC_ISSUER=$(aws eks describe-cluster --name eks-prod --region us-east-1 --query "cluster.identity.oidc.issuer" --output text 2>&1)
          if [ -z "$OIDC_ISSUER" ] || [ "$OIDC_ISSUER" == "None" ]; then
            eksctl utils associate-iam-oidc-provider --cluster eks-prod --region us-east-1 --approve
          fi

      - name: Install AWS EBS CSI Driver
        run: |
          if kubectl get deployment ebs-csi-controller -n kube-system &>/dev/null; then
            echo "EBS CSI Driver already installed"
          else
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            POLICY_NAME="AmazonEBSCSIDriverPolicy-prod"

            if ! aws iam get-policy --policy-arn "arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_NAME}" &>/dev/null; then
              curl -o /tmp/ebs_csi_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json
              aws iam create-policy --policy-name ${POLICY_NAME} --policy-document file:///tmp/ebs_csi_policy.json
            fi

            eksctl create iamserviceaccount \
              --cluster=eks-prod --namespace=kube-system --name=ebs-csi-controller-sa \
              --role-name AmazonEKSEBSCSIDriverRole-prod \
              --attach-policy-arn=arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_NAME} \
              --approve --override-existing-serviceaccounts --region us-east-1

            helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
            helm repo update
            helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
              --namespace kube-system \
              --set controller.serviceAccount.create=false \
              --set controller.serviceAccount.name=ebs-csi-controller-sa \
              --set controller.tolerations[0].operator=Exists \
              --set node.tolerations[0].operator=Exists \
              --wait --timeout 5m
          fi

      - name: Create gp3 StorageClass
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: gp3
          provisioner: ebs.csi.aws.com
          parameters:
            type: gp3
            encrypted: "true"
            fsType: ext4
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          reclaimPolicy: Delete
          EOF

      - name: Install AWS Load Balancer Controller
        run: |
          if kubectl get deployment aws-load-balancer-controller -n kube-system &>/dev/null; then
            echo "AWS Load Balancer Controller already installed"
          else
            POLICY_NAME="AWSLoadBalancerControllerIAMPolicy-prod"
            ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

            if ! aws iam get-policy --policy-arn "arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_NAME}" &>/dev/null; then
              curl -o /tmp/iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json
              aws iam create-policy --policy-name ${POLICY_NAME} --policy-document file:///tmp/iam_policy.json
            fi

            eksctl create iamserviceaccount \
              --cluster=eks-prod --namespace=kube-system --name=aws-load-balancer-controller \
              --role-name AmazonEKSLoadBalancerControllerRole-prod \
              --attach-policy-arn=arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_NAME} \
              --approve --override-existing-serviceaccounts --region us-east-1

            VPC_ID=$(aws eks describe-cluster --name eks-prod --region us-east-1 --query 'cluster.resourcesVpcConfig.vpcId' --output text)

            helm repo add eks https://aws.github.io/eks-charts
            helm repo update
            helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=eks-prod \
              --set vpcId=$VPC_ID \
              --set region=us-east-1 \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --set tolerations[0].operator=Exists \
              --wait --timeout 10m
          fi

      - name: Wait for pods to be healthy
        run: |
          echo "Waiting for all pods to be healthy..."
          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n prod --no-headers 2>/dev/null | grep -c "1/1.*Running" || echo "0")
            TOTAL_PODS=$(kubectl get pods -n prod --no-headers 2>/dev/null | wc -l | xargs || echo "0")
            echo "Attempt $i/30: $READY_PODS/$TOTAL_PODS pods ready"
            if [ "$READY_PODS" -ge "8" ] && [ "$READY_PODS" == "$TOTAL_PODS" ]; then
              echo "âœ… All pods are healthy!"
              break
            fi
            sleep 10
          done

      - name: Get access URLs
        id: get-urls
        continue-on-error: true
        run: |
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d || echo "N/A")

          ARGOCD_URL=""
          for i in {1..18}; do
            ARGOCD_URL=$(kubectl get ingress argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [ -n "$ARGOCD_URL" ]; then
              ARGOCD_URL="http://$ARGOCD_URL (Internal VPC only)"
              break
            fi
            sleep 10
          done
          [ -z "$ARGOCD_URL" ] && ARGOCD_URL="Pending"

          FRONTEND_URL=""
          for i in {1..18}; do
            FRONTEND_URL=$(kubectl get svc frontend-prod -n prod -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [ -n "$FRONTEND_URL" ]; then
              break
            fi
            sleep 10
          done
          [ -z "$FRONTEND_URL" ] && FRONTEND_URL="Pending"

          echo "argocd_password=$ARGOCD_PASSWORD" >> $GITHUB_OUTPUT
          echo "argocd_url=$ARGOCD_URL" >> $GITHUB_OUTPUT
          echo "frontend_url=$FRONTEND_URL" >> $GITHUB_OUTPUT

      - name: Get monitoring info
        id: monitoring-info
        run: |
          GRAFANA_PASSWORD=$(kubectl get secret grafana-admin-credentials -n monitoring -o jsonpath="{.data.admin-password}" 2>/dev/null | base64 -d || echo "N/A")
          GRAFANA_URL=$(kubectl get ingress monitoring-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Pending")

          echo "grafana_password=$GRAFANA_PASSWORD" >> $GITHUB_OUTPUT
          echo "grafana_url=$GRAFANA_URL" >> $GITHUB_OUTPUT

  # =============================================
  # Stage 8: Pipeline Summary
  # =============================================
  pipeline-summary:
    name: ðŸ“Š Pipeline Summary
    runs-on: ubuntu-latest
    needs: [code-analysis, detect-changes, build-backend, build-frontend, sync-and-verify]
    if: always()
    
    steps:
      - name: Configure AWS credentials
        if: needs.sync-and-verify.result == 'success'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Configure kubectl
        if: needs.sync-and-verify.result == 'success'
        run: |
          aws eks update-kubeconfig --region us-east-1 --name eks-prod

      - name: Get monitoring info
        id: monitoring-info
        if: needs.sync-and-verify.result == 'success'
        run: |
          GRAFANA_PASSWORD=$(kubectl get secret grafana-admin-credentials -n monitoring -o jsonpath="{.data.admin-password}" 2>/dev/null | base64 -d || echo "N/A")
          echo "grafana_password=$GRAFANA_PASSWORD" >> $GITHUB_OUTPUT

      - name: Generate Pipeline Summary
        run: |
          echo "## ðŸš€ Complete CI/CD Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Pipeline Stages" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Analysis & Security | ${{ needs.code-analysis.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Detect Changes | ${{ needs.detect-changes.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Build Backend | ${{ needs.build-backend.result == 'success' && 'âœ…' || (needs.build-backend.result == 'skipped' && 'â­ï¸' || 'âŒ') }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Build Frontend | ${{ needs.build-frontend.result == 'success' && 'âœ…' || (needs.build-frontend.result == 'skipped' && 'â­ï¸' || 'âŒ') }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deploy to Production | ${{ needs.sync-and-verify.result == 'success' && 'âœ…' || (needs.sync-and-verify.result == 'skipped' && 'â­ï¸' || 'âŒ') }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.sync-and-verify.result }}" == "success" ]; then
            echo "### ðŸŒ Access URLs" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Frontend Application (Public):**" >> $GITHUB_STEP_SUMMARY
            echo "- http://${{ needs.sync-and-verify.outputs.frontend_url }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**ArgoCD Dashboard (Internal VPC only):**" >> $GITHUB_STEP_SUMMARY
            echo "- URL: ${{ needs.sync-and-verify.outputs.argocd_url }}" >> $GITHUB_STEP_SUMMARY
            echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
            echo "- Password: \`${{ needs.sync-and-verify.outputs.argocd_password }}\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Grafana Dashboard (Internal VPC only):**" >> $GITHUB_STEP_SUMMARY
            echo "- URL: ${{ needs.sync-and-verify.outputs.grafana_url }}" >> $GITHUB_STEP_SUMMARY
            echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
            echo "- Password: \`${{ steps.monitoring-info.outputs.grafana_password }}\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**RDS PostgreSQL Database:**" >> $GITHUB_STEP_SUMMARY
            echo "- Host: \`${{ needs.sync-and-verify.outputs.rds_endpoint }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- Port: \`5432\`" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ” Security Reports" >> $GITHUB_STEP_SUMMARY
          echo "- SonarCloud: https://sonarcloud.io/organizations/nt114devsecopsproject/projects" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“‹ Deployment Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY

      - name: Display access information
        if: needs.sync-and-verify.result == 'success'
        run: |
          echo "=========================================="
          echo "  PRODUCTION DEPLOYMENT COMPLETE!"
          echo "=========================================="
          echo ""
          echo "Frontend Application (Public):"
          echo "  URL: http://${{ needs.sync-and-verify.outputs.frontend_url }}"
          echo ""
          echo "ArgoCD Dashboard (Internal VPC only):"
          echo "  URL:      ${{ needs.sync-and-verify.outputs.argocd_url }}"
          echo "  Username: admin"
          echo "  Password: ${{ needs.sync-and-verify.outputs.argocd_password }}"
          echo ""
          echo "Grafana Dashboard (Internal VPC only):"
          echo "  Username: admin"
          echo "  Password: ${{ steps.monitoring-info.outputs.grafana_password }}"
          echo ""
          echo "RDS PostgreSQL Database:"
          echo "  Host:     ${{ needs.sync-and-verify.outputs.rds_endpoint }}"
          echo "  Port:     5432"
          echo "=========================================="
