name: Deploy to Development (GitOps)

on:
  push:
    branches:
      - main
    paths:
      - 'microservices/**'
      - 'frontend/**'
      - 'helm/**'
      - '.github/workflows/deploy-dev.yml'
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY_ALIAS: nt114-devsecops
  ENVIRONMENT: dev
  EKS_CLUSTER_NAME: eks-1

permissions:
  contents: write  # Required to push image tag updates to Git

jobs:
  # Detect which services changed
  detect-changes:
    name: Detect Changed Services
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.set-outputs.outputs.backend }}
      frontend: ${{ steps.set-outputs.outputs.frontend }}
      api-gateway: ${{ steps.set-outputs.outputs.api-gateway }}
      exercises: ${{ steps.set-outputs.outputs.exercises }}
      scores: ${{ steps.set-outputs.outputs.scores }}
      user-management: ${{ steps.set-outputs.outputs.user-management }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - uses: dorny/paths-filter@v3
        id: filter
        if: github.event_name == 'push'
        with:
          filters: |
            backend:
              - 'microservices/**'
            frontend:
              - 'frontend/**'
            api-gateway:
              - 'microservices/api-gateway/**'
            exercises:
              - 'microservices/exercises-service/**'
            scores:
              - 'microservices/scores-service/**'
            user-management:
              - 'microservices/user-management-service/**'

      - name: Set outputs (force all services on manual dispatch)
        id: set-outputs
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Manual dispatch detected - building all services"
            echo "backend=true" >> $GITHUB_OUTPUT
            echo "frontend=true" >> $GITHUB_OUTPUT
            echo "api-gateway=true" >> $GITHUB_OUTPUT
            echo "exercises=true" >> $GITHUB_OUTPUT
            echo "scores=true" >> $GITHUB_OUTPUT
            echo "user-management=true" >> $GITHUB_OUTPUT
          else
            echo "Push event detected - using path filter results"
            echo "backend=${{ steps.filter.outputs.backend }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ steps.filter.outputs.frontend }}" >> $GITHUB_OUTPUT
            echo "api-gateway=${{ steps.filter.outputs.api-gateway }}" >> $GITHUB_OUTPUT
            echo "exercises=${{ steps.filter.outputs.exercises }}" >> $GITHUB_OUTPUT
            echo "scores=${{ steps.filter.outputs.scores }}" >> $GITHUB_OUTPUT
            echo "user-management=${{ steps.filter.outputs.user-management }}" >> $GITHUB_OUTPUT
          fi

  # Setup Kubernetes secrets for database connections
  setup-secrets:
    name: Setup Database Secrets
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      rds_endpoint: ${{ steps.rds-info.outputs.endpoint }}
      rds_password: ${{ steps.rds-info.outputs.password }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Debug - Verify AWS Account
        run: |
          echo "üîç Checking AWS account information..."
          AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
          AWS_USER_ARN=$(aws sts get-caller-identity --query Arn --output text)
          echo "AWS Account ID: $AWS_ACCOUNT"
          echo "AWS User ARN: $AWS_USER_ARN"
          echo ""
          echo "Expected Account: 039612870452"
          echo "Expected User: nt114-devsecops-github-actions-user"
          echo ""
          if [ "$AWS_ACCOUNT" != "039612870452" ]; then
            echo "‚ùå ERROR: GitHub Secrets are using credentials from WRONG AWS account!"
            echo "Current: $AWS_ACCOUNT"
            echo "Expected: 039612870452"
            echo ""
            echo "FIX: Update GitHub Secrets with credentials for account 039612870452"
            exit 1
          fi
          echo "‚úÖ AWS Account verified"
          echo ""
          echo "Listing EKS clusters in this account..."
          aws eks list-clusters --region ${{ env.AWS_REGION }} || echo "No clusters found or no permission to list"

      - name: Validate EKS cluster connectivity
        run: |
          echo "üîç Validating EKS cluster connectivity..."
          echo "Target cluster: ${{ env.EKS_CLUSTER_NAME }}"
          echo "Region: ${{ env.AWS_REGION }}"

          # Clear any existing kubeconfig to avoid cache issues
          rm -f ~/.kube/config
          mkdir -p ~/.kube

          # Check cluster status
          echo "Checking cluster status..."
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text)
          echo "Cluster status: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "‚ùå ERROR: EKS cluster is not ACTIVE (status: $CLUSTER_STATUS)"
            exit 1
          fi

          # Configure kubectl
          echo "Configuring kubectl for cluster: ${{ env.EKS_CLUSTER_NAME }}..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

          # Verify kubeconfig
          echo "Current kubeconfig context:"
          kubectl config current-context

          # Test connection
          echo "Testing cluster connection..."
          kubectl cluster-info || {
            echo "‚ùå ERROR: Cannot connect to EKS cluster"
            echo "Possible causes:"
            echo "  1. IAM user lacks EKS access entry"
            echo "  2. Security group blocking access"
            echo "  3. Network connectivity issue"
            exit 1
          }

          echo "‚úÖ EKS cluster validated and kubectl configured"

      - name: Get RDS endpoint and password
        id: rds-info
        run: |
          # Get RDS endpoint from AWS
          RDS_ENDPOINT=$(aws rds describe-db-instances --db-instance-identifier nt114-postgres-dev --region ${{ env.AWS_REGION }} --query 'DBInstances[0].Endpoint.Address' --output text)
          echo "endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "‚úÖ RDS Endpoint: $RDS_ENDPOINT"

          # Use RDS password from GitHub Secrets (managed manually)
          echo "password=${{ secrets.RDS_PASSWORD }}" >> $GITHUB_OUTPUT
          echo "‚úÖ RDS Password retrieved from GitHub Secrets"

      - name: Create dev namespace
        run: |
          echo "Creating dev namespace..."
          kubectl create namespace dev --dry-run=client -o yaml | kubectl apply -f -
          echo "‚úÖ Namespace dev ready"

      - name: Create database secrets
        run: |
          # Replace placeholders in template with values from Terraform
          sed -e "s/RDS_ENDPOINT_PLACEHOLDER/${{ steps.rds-info.outputs.endpoint }}/g" \
              -e "s/RDS_PASSWORD_PLACEHOLDER/${{ steps.rds-info.outputs.password }}/g" \
              k8s/manifests/db-secrets-dev.yaml | kubectl apply -f -

          echo "‚úÖ Database secrets created successfully with Terraform password"

      - name: Create ECR pull secret
        run: |
          echo "Creating ECR pull secret for dev namespace..."

          # Get ECR authentication token
          ECR_PASSWORD=$(aws ecr get-login-password --region ${{ env.AWS_REGION }})

          # Create docker-registry secret for ECR
          kubectl create secret docker-registry ecr-secret \
            --docker-server=${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com \
            --docker-username=AWS \
            --docker-password="$ECR_PASSWORD" \
            -n dev \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "‚úÖ ECR pull secret created successfully"
          kubectl get secret ecr-secret -n dev

  # Build and push backend services to ECR
  build-backend:
    name: Build Backend Service
    needs: detect-changes
    if: always()
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Continue even if one service fails
      matrix:
        service:
          - name: api-gateway
            path: microservices/api-gateway
          - name: exercises-service
            path: microservices/exercises-service
          - name: scores-service
            path: microservices/scores-service
          - name: user-management-service
            path: microservices/user-management-service
    outputs:
      services_built: ${{ steps.set-output.outputs.services }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd ${{ matrix.service.path }}

            echo " Building Docker image for ${{ matrix.service.name }}..."
            docker buildx build --platform linux/amd64 \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:latest \
              --push .

            echo " Image pushed to ECR:"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:${{ github.sha }}"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:latest"

      - name: Mark service as built
        if: steps.should-build.outputs.enabled == 'true'
        id: set-output
        run: echo "services=${{ matrix.service.name }}" >> $GITHUB_OUTPUT

  # Build and push frontend to ECR
  build-frontend:
    name: Build Frontend
    needs: detect-changes
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Frontend (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd frontend

            echo " Building Frontend Docker image..."
            docker buildx build --platform linux/amd64 \
              -f Dockerfile.prod \
              --build-arg VITE_API_URL="" \
              --build-arg VITE_APP_TITLE="CodeLearn" \
              --build-arg VITE_APP_ENV=development \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/frontend:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/frontend:latest \
              --push .

            echo " Frontend image pushed to ECR"

  # Update Helm values in Git (sequential to avoid conflicts)
  update-helm-values:
    name: Update Helm Values (GitOps)
    needs: [detect-changes, build-backend, build-frontend]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Update all Helm values with new image tags
        run: |
          IMAGE_TAG="${{ github.sha }}"
          UPDATED_SERVICES=""

          # Update backend services
          if [ "${{ needs.detect-changes.outputs.api-gateway }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/api-gateway/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}api-gateway, "
            echo " Updated helm/api-gateway/values-dev.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.exercises }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/exercises-service/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}exercises-service, "
            echo " Updated helm/exercises-service/values-dev.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.scores }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/scores-service/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}scores-service, "
            echo " Updated helm/scores-service/values-dev.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.user-management }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/user-management-service/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}user-management-service, "
            echo " Updated helm/user-management-service/values-dev.yaml"
          fi

          # Update frontend
          if [ "${{ needs.detect-changes.outputs.frontend }}" == "true" ] && [ "${{ needs.build-frontend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/frontend/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}frontend, "
            echo " Updated helm/frontend/values-dev.yaml"
          fi

          # Save updated services list
          echo "UPDATED_SERVICES=${UPDATED_SERVICES%, }" >> $GITHUB_ENV

      - name: Commit and push all Helm value updates (single atomic commit)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add helm/*/values-dev.yaml

          if git diff --cached --quiet; then
            echo "No Helm value changes to commit"
            exit 0
          fi

          # Create and push commit with retry
          git commit -m "chore(dev): update image tags to ${{ github.sha }} [skip ci]" -m "Services: ${{ env.UPDATED_SERVICES }}" -m "Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Try push with retry (3 attempts)
          git pull --rebase origin main && git push origin main || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (echo "ERROR: Failed to push after 3 attempts" && exit 1)

          echo "SUCCESS: Helm values pushed to Git - ArgoCD will auto-sync"
          echo "Updated services: ${{ env.UPDATED_SERVICES }}"

  # Sync ArgoCD and verify deployment
  sync-and-verify:
    name: Sync ArgoCD & Verify Deployment
    needs: [build-backend, build-frontend, update-helm-values, setup-secrets]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    outputs:
      frontend_url: ${{ steps.get-urls.outputs.frontend_url }}
      argocd_url: ${{ steps.get-urls.outputs.argocd_url }}
      argocd_password: ${{ steps.get-urls.outputs.argocd_password }}
      rds_endpoint: ${{ needs.setup-secrets.outputs.rds_endpoint }}
      rds_password: ${{ needs.setup-secrets.outputs.rds_password }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Checkout code for ArgoCD values
        uses: actions/checkout@v4

      - name: Install or upgrade ArgoCD (Optimized)
        run: |
          set -e

          # ============================================
          # Function Definitions
          # ============================================

          check_argocd_tolerations() {
            echo "üîç Checking if ArgoCD tolerations are already configured..."

            if ! kubectl get deployment argocd-server -n argocd &>/dev/null; then
              echo "‚ùå ArgoCD not installed"
              return 1
            fi

            CURRENT_TOLERATIONS=$(kubectl get deployment argocd-server -n argocd -o jsonpath='{.spec.template.spec.tolerations}' 2>/dev/null)

            if echo "$CURRENT_TOLERATIONS" | grep -q "workload.*argocd"; then
              echo "‚úÖ ArgoCD already has correct tolerations configured"
              return 0
            else
              echo "‚ö†Ô∏è  ArgoCD needs toleration update"
              return 1
            fi
          }

          wait_for_critical_components() {
            echo "‚è≥ Waiting for critical ArgoCD components..."

            # Note: argocd-application-controller is a StatefulSet, not a Deployment
            # Changed in ArgoCD v1.8+ for sharding support

            # Wait for StatefulSet (application controller)
            echo "  ‚îú‚îÄ Waiting for argocd-application-controller (StatefulSet)..."
            if kubectl rollout status statefulset/argocd-application-controller -n argocd --timeout=180s; then
              echo "  ‚îú‚îÄ ‚úÖ argocd-application-controller ready"
            else
              echo "  ‚îú‚îÄ ‚ùå argocd-application-controller failed to become ready"
              kubectl describe statefulset/argocd-application-controller -n argocd
              kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller --tail=50 || true
              return 1
            fi

            # Wait for Deployments (server and repo-server)
            CRITICAL_DEPLOYMENTS=(
              "argocd-server"
              "argocd-repo-server"
            )

            for deployment in "${CRITICAL_DEPLOYMENTS[@]}"; do
              echo "  ‚îú‚îÄ Waiting for $deployment (Deployment)..."
              if kubectl rollout status deployment/$deployment -n argocd --timeout=120s; then
                echo "  ‚îú‚îÄ ‚úÖ $deployment ready"
              else
                echo "  ‚îú‚îÄ ‚ùå $deployment failed to become ready"
                kubectl describe deployment/$deployment -n argocd
                kubectl logs -n argocd -l app.kubernetes.io/name=$deployment --tail=50 || true
                return 1
              fi
            done

            echo "‚úÖ All critical components ready"
            return 0
          }

          show_pod_status() {
            echo "üìä Current ArgoCD Pod Status:"
            kubectl get pods -n argocd -o wide

            echo ""
            echo "üìã Deployment Status:"
            kubectl get deployments -n argocd
          }

          # ============================================
          # Main Logic
          # ============================================

          echo "üöÄ Starting ArgoCD installation/upgrade process..."

          # Check if ArgoCD exists and if upgrade is needed
          if kubectl get deployment argocd-server -n argocd &>/dev/null; then
            if check_argocd_tolerations; then
              echo "‚úÖ ArgoCD already configured correctly, skipping upgrade"
              show_pod_status
              exit 0
            fi

            echo "üì¶ ArgoCD installed but needs configuration update..."
            HELM_ACTION="upgrade"
          else
            echo "üì¶ Installing ArgoCD for the first time..."
            kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
            HELM_ACTION="install"
          fi

          # Add ArgoCD Helm repository
          echo "üìö Adding ArgoCD Helm repository..."
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update

          # Execute Helm command without --wait (faster)
          echo "‚öôÔ∏è  Executing Helm $HELM_ACTION..."
          if [ "$HELM_ACTION" = "upgrade" ]; then
            helm upgrade argocd argo/argo-cd \
              --namespace argocd \
              -f helm/argocd/values-dev.yaml \
              --timeout 3m \
              --atomic \
              --cleanup-on-fail
          else
            # Note: --cleanup-on-fail not supported by helm install
            # Using --atomic which purges installation on failure
            helm install argocd argo/argo-cd \
              --namespace argocd \
              -f helm/argocd/values-dev.yaml \
              --timeout 3m \
              --atomic
          fi

          echo "‚úÖ Helm command completed successfully"

          # Wait for critical components only (not all pods)
          if ! wait_for_critical_components; then
            echo "‚ùå ArgoCD deployment verification failed"
            show_pod_status
            exit 1
          fi

          # Final status report
          echo ""
          echo "üéâ ArgoCD installation/upgrade completed successfully!"
          show_pod_status

      - name: Ensure ArgoCD applications exist
        run: |
          echo "Checking ArgoCD applications..."

          # Create dev namespace if it doesn't exist
          kubectl create namespace dev --dry-run=client -o yaml | kubectl apply -f -

          # Create ArgoCD applications for each service
          declare -A services
          services=(
            ["api-gateway"]="helm/api-gateway"
            ["exercises"]="helm/exercises-service"
            ["scores"]="helm/scores-service"
            ["user-management"]="helm/user-management-service"
            ["frontend"]="helm/frontend"
          )

          for service in "${!services[@]}"; do
            APP_NAME="${service}-dev"
            HELM_PATH="${services[$service]}"

            if kubectl get application "$APP_NAME" -n argocd &>/dev/null; then
              echo "‚úÖ Application $APP_NAME already exists"
            else
              echo "üìù Creating ArgoCD application: $APP_NAME (path: $HELM_PATH)"

              cat <<EOF | kubectl apply -f -
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: $APP_NAME
            namespace: argocd
            finalizers:
              - resources-finalizer.argocd.argoproj.io
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}.git
              targetRevision: HEAD
              path: $HELM_PATH
              helm:
                valueFiles:
                  - values-dev.yaml
            destination:
              server: https://kubernetes.default.svc
              namespace: dev
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
              syncOptions:
                - CreateNamespace=true
          EOF
            fi
          done

          echo "‚úÖ All ArgoCD applications verified"

      - name: Force sync ArgoCD applications
        run: |
          echo "Force syncing all dev applications..."
          for app in api-gateway-dev exercises-dev frontend-dev scores-dev user-management-dev; do
            echo "Syncing $app..."
            kubectl patch application $app -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"HEAD"}}}' || echo "App $app may not exist yet"
          done

          echo "Waiting 30s for ArgoCD to process sync requests..."
          sleep 30

      - name: Install Metrics Server for HPA
        run: |
          echo "üìä Installing Metrics Server for HorizontalPodAutoscaler support..."

          # Check if metrics-server already exists
          if kubectl get deployment metrics-server -n kube-system &>/dev/null; then
            echo "‚úÖ Metrics Server already installed"
          else
            echo "üì¶ Installing Metrics Server..."

            # Install metrics-server using official manifest
            kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

            # Wait for metrics-server to be ready
            echo "‚è≥ Waiting for Metrics Server to be ready..."
            kubectl wait --for=condition=Ready pods \
              -l k8s-app=metrics-server \
              -n kube-system \
              --timeout=120s || {
                echo "‚ö†Ô∏è Metrics Server not ready yet, continuing..."
                kubectl get pods -n kube-system -l k8s-app=metrics-server
              }

            echo "‚úÖ Metrics Server installed"
          fi

          # Verify metrics API is available
          echo "üîç Verifying Metrics API..."
          kubectl top nodes || echo "‚ö†Ô∏è Metrics API not ready yet (this is normal, may take 1-2 minutes)"

      - name: Wait for pods to be healthy
        run: |
          echo "Waiting for all pods to be healthy (max 5 minutes)..."

          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n dev --no-headers 2>/dev/null | grep -c "1/1.*Running" || echo "0")
            TOTAL_PODS=$(kubectl get pods -n dev --no-headers 2>/dev/null | wc -l | xargs || echo "0")

            echo "Attempt $i/30: $READY_PODS/$TOTAL_PODS pods ready"

            if [ "$READY_PODS" -ge "8" ] && [ "$READY_PODS" == "$TOTAL_PODS" ]; then
              echo "All pods are healthy!"
              kubectl get pods -n dev
              break
            fi

            if [ "$i" == "30" ]; then
              echo "‚ö†Ô∏è Warning: Not all pods ready after 5 minutes"
              echo ""
              echo "=== Pod Status ==="
              kubectl get pods -n dev

              echo ""
              echo "=== ArgoCD Applications ==="
              kubectl get applications -n argocd

              echo ""
              echo "=== Recent Pod Events ==="
              kubectl get events -n dev --sort-by='.lastTimestamp' --field-selector type=Warning | tail -20

              echo ""
              echo "=== Pod Details (Not Running) ==="
              kubectl get pods -n dev --field-selector status.phase!=Running -o wide 2>/dev/null || echo "No pods in non-running state"
            fi

            sleep 10
          done

      - name: Get access URLs
        id: get-urls
        continue-on-error: true
        run: |
          echo "Getting ArgoCD credentials..."
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d || echo "N/A")

          echo "Getting LoadBalancer URLs (waiting up to 3 minutes)..."

          # Get ArgoCD URL
          ARGOCD_URL=""
          for i in {1..18}; do
            ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$ARGOCD_URL" ] && ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$ARGOCD_URL" ]; then
              echo "ArgoCD URL found: $ARGOCD_URL"
              break
            fi
            echo "Waiting for ArgoCD LoadBalancer... ($i/18)"
            sleep 10
          done

          # Get Frontend URL
          FRONTEND_URL=""
          for i in {1..18}; do
            FRONTEND_URL=$(kubectl get svc frontend-dev -n dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$FRONTEND_URL" ] && FRONTEND_URL=$(kubectl get svc frontend-dev -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$FRONTEND_URL" ]; then
              echo "Frontend URL found: $FRONTEND_URL"
              break
            fi
            echo "Waiting for Frontend LoadBalancer... ($i/18)"
            sleep 10
          done

          [ -z "$ARGOCD_URL" ] && ARGOCD_URL="Pending (LoadBalancer provisioning)"
          [ -z "$FRONTEND_URL" ] && FRONTEND_URL="Pending (LoadBalancer provisioning)"

          echo "argocd_password=$ARGOCD_PASSWORD" >> $GITHUB_OUTPUT
          echo "argocd_url=$ARGOCD_URL" >> $GITHUB_OUTPUT
          echo "frontend_url=$FRONTEND_URL" >> $GITHUB_OUTPUT

  # Deploy monitoring stack
  deploy-monitoring:
    name: Deploy Monitoring Stack
    needs: [sync-and-verify]
    runs-on: ubuntu-latest
    outputs:
      grafana_url: ${{ steps.get-grafana-url.outputs.grafana_url }}
      grafana_password: ${{ steps.get-grafana-password.outputs.password }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info

      - name: Verify and install EBS CSI Driver
        run: |
          echo "üîç Checking EBS CSI driver status..."

          # Get full addon details (status, health)
          ADDON_INFO=$(aws eks describe-addon \
            --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --region ${{ env.AWS_REGION }} \
            --output json 2>/dev/null || echo '{"addon":{"status":"NOT_INSTALLED"}}')

          ADDON_STATUS=$(echo "$ADDON_INFO" | jq -r '.addon.status // "NOT_INSTALLED"')
          ADDON_HEALTH=$(echo "$ADDON_INFO" | jq -r '.addon.health.issues // []')

          echo "Current addon status: $ADDON_STATUS"

          # Handle different addon states
          if [ "$ADDON_STATUS" = "NOT_INSTALLED" ]; then
            echo "üì¶ EBS CSI driver not installed, installing now..."
            INSTALL_ADDON=true

          elif [ "$ADDON_STATUS" = "ACTIVE" ]; then
            echo "‚úÖ EBS CSI driver already active, skipping installation"
            INSTALL_ADDON=false

          elif [ "$ADDON_STATUS" = "DEGRADED" ] || [ "$ADDON_STATUS" = "CREATE_FAILED" ] || [ "$ADDON_STATUS" = "UPDATE_FAILED" ]; then
            echo "‚ö†Ô∏è Addon in $ADDON_STATUS state, attempting to fix..."
            echo "Health issues: $ADDON_HEALTH"

            # Try updating the addon to fix degraded state
            echo "Updating addon to resolve issues..."
            aws eks update-addon \
              --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
              --addon-name aws-ebs-csi-driver \
              --addon-version v1.36.0-eksbuild.1 \
              --service-account-role-arn arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ env.EKS_CLUSTER_NAME }}-ebs-csi-controller \
              --region ${{ env.AWS_REGION }} \
              --resolve-conflicts OVERWRITE \
              --configuration-values '{"controller":{"tolerations":[{"key":"workload","operator":"Exists","effect":"NoSchedule"}]}}' || {
                echo "‚ö†Ô∏è Update failed, will wait for status change..."
              }
            INSTALL_ADDON=false

          else
            echo "‚ö†Ô∏è Unexpected addon status: $ADDON_STATUS"
            echo "Attempting to update anyway..."
            INSTALL_ADDON=false
          fi

          # Install if needed
          if [ "$INSTALL_ADDON" = "true" ]; then
            aws eks create-addon \
              --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
              --addon-name aws-ebs-csi-driver \
              --addon-version v1.36.0-eksbuild.1 \
              --service-account-role-arn arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ env.EKS_CLUSTER_NAME }}-ebs-csi-controller \
              --region ${{ env.AWS_REGION }} \
              --resolve-conflicts OVERWRITE \
              --configuration-values '{"controller":{"tolerations":[{"key":"workload","operator":"Exists","effect":"NoSchedule"}]}}' || {
                echo "‚ö†Ô∏è Addon creation may have failed, checking status..."
              }
          fi

          # Wait for addon to become ACTIVE (increased timeout to 60 attempts = 10 minutes)
          echo "‚è≥ Waiting for addon to become active (max 10 minutes)..."
          for i in {1..60}; do
            STATUS=$(aws eks describe-addon \
              --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
              --addon-name aws-ebs-csi-driver \
              --region ${{ env.AWS_REGION }} \
              --query 'addon.status' \
              --output text 2>/dev/null || echo "UNKNOWN")

            echo "Attempt $i/60: Addon status = $STATUS"

            if [ "$STATUS" = "ACTIVE" ]; then
              echo "‚úÖ EBS CSI driver addon is active"
              break
            fi

            if [ "$i" = "60" ]; then
              echo "‚ùå ERROR: EBS CSI driver addon did not become active in time"
              echo "Final status: $STATUS"

              # Show diagnostic information
              echo "=== Addon Details ==="
              aws eks describe-addon \
                --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
                --addon-name aws-ebs-csi-driver \
                --region ${{ env.AWS_REGION }} || true

              exit 1
            fi

            sleep 10
          done

          # Verify controller pods are running
          echo "üîç Verifying EBS CSI driver pods..."
          kubectl wait --for=condition=Ready pods \
            -l app.kubernetes.io/name=aws-ebs-csi-driver \
            -n kube-system \
            --timeout=180s || {
              echo "‚ùå ERROR: EBS CSI driver pods not ready!"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver
              exit 1
            }

          echo "‚úÖ EBS CSI driver is operational"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver

      - name: Create monitoring namespace
        run: |
          echo "Creating monitoring namespace..."
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          echo "‚úÖ Namespace monitoring ready"

      - name: Generate and create Grafana admin secret
        id: create-grafana-secret
        run: |
          # Generate strong password
          GRAFANA_PASSWORD=$(openssl rand -base64 20)

          # Create secret
          kubectl create secret generic grafana-admin-credentials \
            --from-literal=admin-user=admin \
            --from-literal=admin-password="$GRAFANA_PASSWORD" \
            -n monitoring \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "‚úÖ Grafana admin credentials created"
          echo "password=$GRAFANA_PASSWORD" >> $GITHUB_OUTPUT

      - name: Create gp3 StorageClass
        run: |
          echo "Creating gp3 StorageClass..."
          kubectl apply -f k8s/storage/gp3-storageclass.yaml

          # Set gp3 as default and remove default from gp2
          kubectl patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' || true
          kubectl patch storageclass gp3 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

          echo "‚úÖ gp3 StorageClass created and set as default"
          kubectl get storageclass

      - name: Add Helm repository
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          echo "‚úÖ Helm repo added and updated"

      - name: Deploy kube-prometheus-stack (Optimized)
        run: |
          echo "üöÄ Deploying kube-prometheus-stack..."

          # Deploy with extended timeout, no --atomic on first install
          # Note: First install may take 10-15min due to PVC provisioning
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --version 80.9.1 \
            -f helm/kube-prometheus-stack/values-dev.yaml \
            --timeout 15m \
            --create-namespace

          echo "‚úÖ Helm command completed"
          echo "‚è≥ Waiting for critical components..."

          # Wait for Prometheus Operator first (manages other components)
          kubectl rollout status deployment/kube-prometheus-stack-operator -n monitoring --timeout=180s || echo "‚ö†Ô∏è Operator not ready yet"

          # Wait for kube-state-metrics (required for monitoring)
          kubectl rollout status deployment/kube-prometheus-stack-kube-state-metrics -n monitoring --timeout=180s || echo "‚ö†Ô∏è Kube-state-metrics not ready yet"

          # Check overall pod status
          echo "üìä Current monitoring pod status:"
          kubectl get pods -n monitoring

          echo "‚úÖ Monitoring stack deployment initiated, pods will become ready shortly"

      - name: Wait for monitoring pods to be ready
        run: |
          echo "Waiting for monitoring pods to be ready..."

          # Wait up to 5 minutes for all pods
          kubectl wait --for=condition=Ready pods --all -n monitoring --timeout=300s || {
            echo "‚ö†Ô∏è Some pods still starting, checking status..."
            echo ""
            echo "=== Monitoring Pod Status ==="
            kubectl get pods -n monitoring

            echo ""
            echo "=== PVC Status ==="
            kubectl get pvc -n monitoring

            echo ""
            echo "=== Recent Warning Events ==="
            kubectl get events -n monitoring --sort-by='.lastTimestamp' --field-selector type=Warning | tail -20

            echo ""
            echo "=== Pods Not Running ==="
            kubectl get pods -n monitoring --field-selector status.phase!=Running -o wide 2>/dev/null || echo "All pods are running or completed"
          }

          echo "‚úÖ Monitoring stack pods ready"
          kubectl get pods -n monitoring

      - name: Deploy ArgoCD ServiceMonitors
        run: |
          echo "Deploying ArgoCD ServiceMonitors..."
          kubectl apply -f k8s/monitoring/servicemonitor-argocd.yaml
          echo "‚úÖ ArgoCD ServiceMonitors deployed"

      - name: Get Grafana LoadBalancer URL
        id: get-grafana-url
        run: |
          echo "Waiting for Grafana LoadBalancer..."

          GRAFANA_URL=""
          for i in {1..18}; do
            GRAFANA_URL=$(kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$GRAFANA_URL" ] && GRAFANA_URL=$(kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$GRAFANA_URL" ]; then
              echo "Grafana URL found: $GRAFANA_URL"
              break
            fi
            echo "Waiting for LoadBalancer... ($i/18)"
            sleep 10
          done

          [ -z "$GRAFANA_URL" ] && GRAFANA_URL="Pending (LoadBalancer provisioning)"
          echo "grafana_url=$GRAFANA_URL" >> $GITHUB_OUTPUT

      - name: Get Grafana admin password
        id: get-grafana-password
        run: |
          PASSWORD=$(kubectl get secret grafana-admin-credentials -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d)
          echo "password=$PASSWORD" >> $GITHUB_OUTPUT

      - name: Verify Prometheus targets
        run: |
          echo "Checking Prometheus target status..."
          kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090 &
          PF_PID=$!
          sleep 5

          # Simple connectivity check
          curl -s http://localhost:9090/api/v1/targets || echo "‚ö†Ô∏è Cannot reach Prometheus API"

          kill $PF_PID || true
          echo "‚úÖ Prometheus verification complete"

  # Output access information
  output-access-info:
    name: Output Access Information
    needs: [sync-and-verify, deploy-monitoring]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Display access information
        run: |
          echo "=========================================="
          echo "  DEPLOYMENT COMPLETE!"
          echo "=========================================="
          echo ""
          echo "Frontend Application:"
          echo "  URL: http://${{ needs.sync-and-verify.outputs.frontend_url }}"
          echo ""
          echo "ArgoCD Dashboard:"
          echo "  URL:      http://${{ needs.sync-and-verify.outputs.argocd_url }}"
          echo "  Username: admin"
          echo "  Password: ${{ needs.sync-and-verify.outputs.argocd_password }}"
          echo ""
          echo "RDS PostgreSQL Database:"
          echo "  Host:     ${{ needs.sync-and-verify.outputs.rds_endpoint }}"
          echo "  Port:     5432"
          echo "  Database: postgres"
          echo "  Username: postgres"
          echo "  Password: (stored in GitHub Secret: RDS_PASSWORD)"
          echo ""
          echo "=========================================="

      - name: Deployment summary
        run: |
          echo "## üöÄ Development Deployment Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üåê Access URLs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Frontend Application:**" >> $GITHUB_STEP_SUMMARY
          echo "- http://${{ needs.sync-and-verify.outputs.frontend_url }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ArgoCD Dashboard:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: http://${{ needs.sync-and-verify.outputs.argocd_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: \`${{ needs.sync-and-verify.outputs.argocd_password }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Monitoring Stack" >> $GITHUB_STEP_SUMMARY
          echo "**Grafana Dashboard:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: http://${{ needs.deploy-monitoring.outputs.grafana_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: \`${{ needs.deploy-monitoring.outputs.grafana_password }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Prometheus:**" >> $GITHUB_STEP_SUMMARY
          echo "- Access via: \`kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090\`" >> $GITHUB_STEP_SUMMARY
          echo "- Then visit: http://localhost:9090" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üóÑÔ∏è RDS PostgreSQL Database" >> $GITHUB_STEP_SUMMARY
          echo "- Host: \`${{ needs.sync-and-verify.outputs.rds_endpoint }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Port: \`5432\`" >> $GITHUB_STEP_SUMMARY
          echo "- Database: \`postgres\`" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`postgres\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: **(stored in GitHub Secret: RDS_PASSWORD)**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> üí° To get password: Go to Settings ‚Üí Secrets ‚Üí RDS_PASSWORD" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üì¶ Deployment Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment:** Development" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ‚úÖ What Was Deployed" >> $GITHUB_STEP_SUMMARY
          echo "- Docker images built and pushed to ECR" >> $GITHUB_STEP_SUMMARY
          echo "- Helm values updated with new image tags" >> $GITHUB_STEP_SUMMARY
          echo "- ArgoCD applications synced" >> $GITHUB_STEP_SUMMARY
          echo "- Prometheus + Grafana monitoring stack deployed" >> $GITHUB_STEP_SUMMARY
          echo "- All pods verified healthy" >> $GITHUB_STEP_SUMMARY
          echo "- LoadBalancers provisioned and accessible" >> $GITHUB_STEP_SUMMARY
