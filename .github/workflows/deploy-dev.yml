name: Deploy to Development (GitOps)

on:
  push:
    branches:
      - main
    paths:
      - 'microservices/**'
      - 'frontend/**'
      - 'helm/**'
      - '.github/workflows/deploy-dev.yml'
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY_ALIAS: nt114-devsecops
  ENVIRONMENT: dev
  EKS_CLUSTER_NAME: eks-1

permissions:
  contents: write  # Required to push image tag updates to Git

jobs:
  # Detect which services changed
  detect-changes:
    name: Detect Changed Services
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.set-outputs.outputs.backend }}
      frontend: ${{ steps.set-outputs.outputs.frontend }}
      api-gateway: ${{ steps.set-outputs.outputs.api-gateway }}
      exercises: ${{ steps.set-outputs.outputs.exercises }}
      scores: ${{ steps.set-outputs.outputs.scores }}
      user-management: ${{ steps.set-outputs.outputs.user-management }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - uses: dorny/paths-filter@v3
        id: filter
        if: github.event_name == 'push'
        with:
          filters: |
            backend:
              - 'microservices/**'
            frontend:
              - 'frontend/**'
            api-gateway:
              - 'microservices/api-gateway/**'
            exercises:
              - 'microservices/exercises-service/**'
            scores:
              - 'microservices/scores-service/**'
            user-management:
              - 'microservices/user-management-service/**'

      - name: Set outputs (force all services on manual dispatch)
        id: set-outputs
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Manual dispatch detected - building all services"
            echo "backend=true" >> $GITHUB_OUTPUT
            echo "frontend=true" >> $GITHUB_OUTPUT
            echo "api-gateway=true" >> $GITHUB_OUTPUT
            echo "exercises=true" >> $GITHUB_OUTPUT
            echo "scores=true" >> $GITHUB_OUTPUT
            echo "user-management=true" >> $GITHUB_OUTPUT
          else
            echo "Push event detected - using path filter results"
            echo "backend=${{ steps.filter.outputs.backend }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ steps.filter.outputs.frontend }}" >> $GITHUB_OUTPUT
            echo "api-gateway=${{ steps.filter.outputs.api-gateway }}" >> $GITHUB_OUTPUT
            echo "exercises=${{ steps.filter.outputs.exercises }}" >> $GITHUB_OUTPUT
            echo "scores=${{ steps.filter.outputs.scores }}" >> $GITHUB_OUTPUT
            echo "user-management=${{ steps.filter.outputs.user-management }}" >> $GITHUB_OUTPUT
          fi

  # Setup Kubernetes secrets for database connections
  setup-secrets:
    name: Setup Database Secrets
    needs: detect-changes
    runs-on: ubuntu-latest
    outputs:
      rds_endpoint: ${{ steps.rds-info.outputs.endpoint }}
      rds_password: ${{ steps.rds-info.outputs.password }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Debug - Verify AWS Account
        run: |
          echo "üîç Checking AWS account information..."
          AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
          AWS_USER_ARN=$(aws sts get-caller-identity --query Arn --output text)
          echo "AWS Account ID: $AWS_ACCOUNT"
          echo "AWS User ARN: $AWS_USER_ARN"
          echo ""
          echo "Expected Account: 039612870452"
          echo "Expected User: nt114-devsecops-github-actions-user"
          echo ""
          if [ "$AWS_ACCOUNT" != "039612870452" ]; then
            echo "‚ùå ERROR: GitHub Secrets are using credentials from WRONG AWS account!"
            echo "Current: $AWS_ACCOUNT"
            echo "Expected: 039612870452"
            echo ""
            echo "FIX: Update GitHub Secrets with credentials for account 039612870452"
            exit 1
          fi
          echo "‚úÖ AWS Account verified"
          echo ""
          echo "Listing EKS clusters in this account..."
          aws eks list-clusters --region ${{ env.AWS_REGION }} || echo "No clusters found or no permission to list"

      - name: Validate EKS cluster connectivity
        run: |
          echo "üîç Validating EKS cluster connectivity..."
          echo "Target cluster: ${{ env.EKS_CLUSTER_NAME }}"
          echo "Region: ${{ env.AWS_REGION }}"

          # Clear any existing kubeconfig to avoid cache issues
          rm -f ~/.kube/config
          mkdir -p ~/.kube

          # Check cluster status
          echo "Checking cluster status..."
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.status' --output text)
          echo "Cluster status: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "‚ùå ERROR: EKS cluster is not ACTIVE (status: $CLUSTER_STATUS)"
            exit 1
          fi

          # Configure kubectl
          echo "Configuring kubectl for cluster: ${{ env.EKS_CLUSTER_NAME }}..."
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

          # Verify kubeconfig
          echo "Current kubeconfig context:"
          kubectl config current-context

          # Test connection
          echo "Testing cluster connection..."
          kubectl cluster-info || {
            echo "‚ùå ERROR: Cannot connect to EKS cluster"
            echo "Possible causes:"
            echo "  1. IAM user lacks EKS access entry"
            echo "  2. Security group blocking access"
            echo "  3. Network connectivity issue"
            exit 1
          }

          echo "‚úÖ EKS cluster validated and kubectl configured"

      - name: Get RDS endpoint and password
        id: rds-info
        run: |
          # Get RDS endpoint from AWS
          RDS_ENDPOINT=$(aws rds describe-db-instances --db-instance-identifier nt114-postgres-dev --region ${{ env.AWS_REGION }} --query 'DBInstances[0].Endpoint.Address' --output text)
          echo "endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          echo "‚úÖ RDS Endpoint: $RDS_ENDPOINT"

          # Use RDS password from GitHub Secrets (managed manually)
          echo "password=${{ secrets.RDS_PASSWORD }}" >> $GITHUB_OUTPUT
          echo "‚úÖ RDS Password retrieved from GitHub Secrets"

      - name: Create dev namespace
        run: |
          echo "Creating dev namespace..."
          kubectl create namespace dev --dry-run=client -o yaml | kubectl apply -f -
          echo "‚úÖ Namespace dev ready"

      - name: Create database secrets
        run: |
          # Replace placeholders in template with values from Terraform
          sed -e "s/RDS_ENDPOINT_PLACEHOLDER/${{ steps.rds-info.outputs.endpoint }}/g" \
              -e "s/RDS_PASSWORD_PLACEHOLDER/${{ steps.rds-info.outputs.password }}/g" \
              k8s/manifests/db-secrets-dev.yaml | kubectl apply -f -

          echo "‚úÖ Database secrets created successfully with Terraform password"

  # Build and push backend services to ECR
  build-backend:
    name: Build Backend Service
    needs: detect-changes
    if: always()
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Continue even if one service fails
      matrix:
        service:
          - name: api-gateway
            path: microservices/api-gateway
          - name: exercises-service
            path: microservices/exercises-service
          - name: scores-service
            path: microservices/scores-service
          - name: user-management-service
            path: microservices/user-management-service
    outputs:
      services_built: ${{ steps.set-output.outputs.services }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Docker image (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd ${{ matrix.service.path }}

            echo " Building Docker image for ${{ matrix.service.name }}..."
            docker buildx build --platform linux/amd64 \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:latest \
              --push .

            echo " Image pushed to ECR:"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:${{ github.sha }}"
            echo "   - ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/${{ matrix.service.name }}:latest"

      - name: Mark service as built
        if: steps.should-build.outputs.enabled == 'true'
        id: set-output
        run: echo "services=${{ matrix.service.name }}" >> $GITHUB_OUTPUT

  # Build and push frontend to ECR
  build-frontend:
    name: Build Frontend
    needs: detect-changes
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push Frontend (with retry)
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 15
          max_attempts: 3
          retry_on: error
          command: |
            cd frontend

            echo " Building Frontend Docker image..."
            docker buildx build --platform linux/amd64 \
              -f Dockerfile.prod \
              --build-arg VITE_API_URL="" \
              --build-arg VITE_APP_TITLE="CodeLearn" \
              --build-arg VITE_APP_ENV=development \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/frontend:${{ github.sha }} \
              -t ${{ steps.login-ecr.outputs.registry }}/nt114-devsecops/frontend:latest \
              --push .

            echo " Frontend image pushed to ECR"

  # Update Helm values in Git (sequential to avoid conflicts)
  update-helm-values:
    name: Update Helm Values (GitOps)
    needs: [detect-changes, build-backend, build-frontend]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Update all Helm values with new image tags
        run: |
          IMAGE_TAG="${{ github.sha }}"
          UPDATED_SERVICES=""

          # Update backend services
          if [ "${{ needs.detect-changes.outputs.api-gateway }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/api-gateway/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}api-gateway, "
            echo " Updated helm/api-gateway/values-dev.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.exercises }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/exercises-service/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}exercises-service, "
            echo " Updated helm/exercises-service/values-dev.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.scores }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/scores-service/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}scores-service, "
            echo " Updated helm/scores-service/values-dev.yaml"
          fi

          if [ "${{ needs.detect-changes.outputs.user-management }}" == "true" ] && [ "${{ needs.build-backend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/user-management-service/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}user-management-service, "
            echo " Updated helm/user-management-service/values-dev.yaml"
          fi

          # Update frontend
          if [ "${{ needs.detect-changes.outputs.frontend }}" == "true" ] && [ "${{ needs.build-frontend.result }}" == "success" ]; then
            sed -i 's|tag: .*|tag: "'$IMAGE_TAG'"|g' helm/frontend/values-dev.yaml
            UPDATED_SERVICES="${UPDATED_SERVICES}frontend, "
            echo " Updated helm/frontend/values-dev.yaml"
          fi

          # Save updated services list
          echo "UPDATED_SERVICES=${UPDATED_SERVICES%, }" >> $GITHUB_ENV

      - name: Commit and push all Helm value updates (single atomic commit)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add helm/*/values-dev.yaml

          if git diff --cached --quiet; then
            echo "No Helm value changes to commit"
            exit 0
          fi

          # Create and push commit with retry
          git commit -m "chore(dev): update image tags to ${{ github.sha }} [skip ci]" -m "Services: ${{ env.UPDATED_SERVICES }}" -m "Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Try push with retry (3 attempts)
          git pull --rebase origin main && git push origin main || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (sleep 5 && git pull --rebase origin main && git push origin main) || \
          (echo "ERROR: Failed to push after 3 attempts" && exit 1)

          echo "SUCCESS: Helm values pushed to Git - ArgoCD will auto-sync"
          echo "Updated services: ${{ env.UPDATED_SERVICES }}"

  # Sync ArgoCD and verify deployment
  sync-and-verify:
    name: Sync ArgoCD & Verify Deployment
    needs: [build-backend, build-frontend, update-helm-values, setup-secrets]
    if: always() && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    runs-on: ubuntu-latest
    outputs:
      frontend_url: ${{ steps.get-urls.outputs.frontend_url }}
      argocd_url: ${{ steps.get-urls.outputs.argocd_url }}
      argocd_password: ${{ steps.get-urls.outputs.argocd_password }}
      rds_endpoint: ${{ needs.setup-secrets.outputs.rds_endpoint }}
      rds_password: ${{ needs.setup-secrets.outputs.rds_password }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Checkout code for ArgoCD values
        uses: actions/checkout@v4

      - name: Install or upgrade ArgoCD (Optimized)
        run: |
          set -e

          # ============================================
          # Function Definitions
          # ============================================

          check_argocd_tolerations() {
            echo "üîç Checking if ArgoCD tolerations are already configured..."

            if ! kubectl get deployment argocd-server -n argocd &>/dev/null; then
              echo "‚ùå ArgoCD not installed"
              return 1
            fi

            CURRENT_TOLERATIONS=$(kubectl get deployment argocd-server -n argocd -o jsonpath='{.spec.template.spec.tolerations}' 2>/dev/null)

            if echo "$CURRENT_TOLERATIONS" | grep -q "workload.*argocd"; then
              echo "‚úÖ ArgoCD already has correct tolerations configured"
              return 0
            else
              echo "‚ö†Ô∏è  ArgoCD needs toleration update"
              return 1
            fi
          }

          wait_for_critical_components() {
            echo "‚è≥ Waiting for critical ArgoCD components..."

            # Note: argocd-application-controller is a StatefulSet, not a Deployment
            # Changed in ArgoCD v1.8+ for sharding support

            # Wait for StatefulSet (application controller)
            echo "  ‚îú‚îÄ Waiting for argocd-application-controller (StatefulSet)..."
            if kubectl rollout status statefulset/argocd-application-controller -n argocd --timeout=180s; then
              echo "  ‚îú‚îÄ ‚úÖ argocd-application-controller ready"
            else
              echo "  ‚îú‚îÄ ‚ùå argocd-application-controller failed to become ready"
              kubectl describe statefulset/argocd-application-controller -n argocd
              kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller --tail=50 || true
              return 1
            fi

            # Wait for Deployments (server and repo-server)
            CRITICAL_DEPLOYMENTS=(
              "argocd-server"
              "argocd-repo-server"
            )

            for deployment in "${CRITICAL_DEPLOYMENTS[@]}"; do
              echo "  ‚îú‚îÄ Waiting for $deployment (Deployment)..."
              if kubectl rollout status deployment/$deployment -n argocd --timeout=120s; then
                echo "  ‚îú‚îÄ ‚úÖ $deployment ready"
              else
                echo "  ‚îú‚îÄ ‚ùå $deployment failed to become ready"
                kubectl describe deployment/$deployment -n argocd
                kubectl logs -n argocd -l app.kubernetes.io/name=$deployment --tail=50 || true
                return 1
              fi
            done

            echo "‚úÖ All critical components ready"
            return 0
          }

          show_pod_status() {
            echo "üìä Current ArgoCD Pod Status:"
            kubectl get pods -n argocd -o wide

            echo ""
            echo "üìã Deployment Status:"
            kubectl get deployments -n argocd
          }

          # ============================================
          # Main Logic
          # ============================================

          echo "üöÄ Starting ArgoCD installation/upgrade process..."

          # Check if ArgoCD exists and if upgrade is needed
          if kubectl get deployment argocd-server -n argocd &>/dev/null; then
            if check_argocd_tolerations; then
              echo "‚úÖ ArgoCD already configured correctly, skipping upgrade"
              show_pod_status
              exit 0
            fi

            echo "üì¶ ArgoCD installed but needs configuration update..."
            HELM_ACTION="upgrade"
          else
            echo "üì¶ Installing ArgoCD for the first time..."
            kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
            HELM_ACTION="install"
          fi

          # Add ArgoCD Helm repository
          echo "üìö Adding ArgoCD Helm repository..."
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update

          # Execute Helm command without --wait (faster)
          echo "‚öôÔ∏è  Executing Helm $HELM_ACTION..."
          if [ "$HELM_ACTION" = "upgrade" ]; then
            helm upgrade argocd argo/argo-cd \
              --namespace argocd \
              -f helm/argocd/values-dev.yaml \
              --timeout 3m \
              --atomic \
              --cleanup-on-fail
          else
            # Note: --cleanup-on-fail not supported by helm install
            # Using --atomic which purges installation on failure
            helm install argocd argo/argo-cd \
              --namespace argocd \
              -f helm/argocd/values-dev.yaml \
              --timeout 3m \
              --atomic
          fi

          echo "‚úÖ Helm command completed successfully"

          # Wait for critical components only (not all pods)
          if ! wait_for_critical_components; then
            echo "‚ùå ArgoCD deployment verification failed"
            show_pod_status
            exit 1
          fi

          # Final status report
          echo ""
          echo "üéâ ArgoCD installation/upgrade completed successfully!"
          show_pod_status

      - name: Ensure ArgoCD applications exist
        run: |
          echo "Checking ArgoCD applications..."

          # Create dev namespace if it doesn't exist
          kubectl create namespace dev --dry-run=client -o yaml | kubectl apply -f -

          # Create ArgoCD applications for each service
          declare -A services
          services=(
            ["api-gateway"]="helm/api-gateway"
            ["exercises"]="helm/exercises-service"
            ["scores"]="helm/scores-service"
            ["user-management"]="helm/user-management-service"
            ["frontend"]="helm/frontend"
          )

          for service in "${!services[@]}"; do
            APP_NAME="${service}-dev"
            HELM_PATH="${services[$service]}"

            if kubectl get application "$APP_NAME" -n argocd &>/dev/null; then
              echo "‚úÖ Application $APP_NAME already exists"
            else
              echo "üìù Creating ArgoCD application: $APP_NAME (path: $HELM_PATH)"

              cat <<EOF | kubectl apply -f -
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: $APP_NAME
            namespace: argocd
            finalizers:
              - resources-finalizer.argocd.argoproj.io
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}.git
              targetRevision: HEAD
              path: $HELM_PATH
              helm:
                valueFiles:
                  - values-dev.yaml
            destination:
              server: https://kubernetes.default.svc
              namespace: dev
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
              syncOptions:
                - CreateNamespace=true
          EOF
            fi
          done

          # Setup monitoring prerequisites
          echo "üìù Setting up monitoring prerequisites..."

          # Create namespace (ArgoCD will also create it, but we need it for secret)
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

          # Generate Grafana admin secret (only if not exists)
          if ! kubectl get secret grafana-admin-credentials -n monitoring &>/dev/null; then
            GRAFANA_PASSWORD=$(openssl rand -base64 20)
            kubectl create secret generic grafana-admin-credentials \
              --from-literal=admin-user=admin \
              --from-literal=admin-password="$GRAFANA_PASSWORD" \
              -n monitoring
            echo "‚úÖ Grafana secret created"
          else
            echo "‚úÖ Grafana secret already exists"
          fi

          # Apply monitoring ArgoCD application (ArgoCD handles the rest!)
          echo "üìù Applying monitoring ArgoCD application..."
          kubectl apply -f argocd/applications/monitoring.yaml
          echo "‚úÖ Monitoring application created - ArgoCD will handle deployment"

          echo "‚úÖ All ArgoCD applications verified"

      - name: Force sync ArgoCD applications
        run: |
          echo "Force syncing all applications..."
          for app in api-gateway-dev exercises-dev frontend-dev scores-dev user-management-dev monitoring; do
            echo "Syncing $app..."
            kubectl patch application $app -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"HEAD"}}}' || echo "App $app may not exist yet"
          done

          echo "Waiting 30s for ArgoCD to process sync requests..."
          sleep 30

      - name: Verify monitoring app created
        run: |
          echo "‚úÖ Monitoring app delegated to ArgoCD"
          echo "ArgoCD will handle deployment asynchronously"
          echo ""
          echo "Check status with:"
          echo "  kubectl get application monitoring -n argocd"
          echo "  kubectl get pods -n monitoring"

      - name: Check cluster capacity
        run: |
          echo "=== Cluster Node Status ==="
          kubectl get nodes -o wide

          echo ""
          echo "=== Pods per Node ==="
          kubectl get pods -A -o wide --no-headers | awk '{print $8}' | sort | uniq -c

          echo ""
          echo "=== Pending Pods ==="
          kubectl get pods -A --field-selector status.phase=Pending

          echo ""
          echo "=== Cluster Autoscaler Status ==="
          kubectl get deployment cluster-autoscaler -n kube-system || echo "‚ö†Ô∏è Cluster Autoscaler not found!"
          kubectl logs -n kube-system -l app.kubernetes.io/name=cluster-autoscaler --tail=20 || echo "No logs"

      - name: Install Metrics Server for HPA
        run: |
          echo "üìä Installing Metrics Server for HorizontalPodAutoscaler support..."

          # Check if metrics-server already exists
          if kubectl get deployment metrics-server -n kube-system &>/dev/null; then
            echo "‚úÖ Metrics Server already installed"
          else
            echo "üì¶ Installing Metrics Server..."

            # Install metrics-server using official manifest
            kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

            # Wait for metrics-server to be ready
            echo "‚è≥ Waiting for Metrics Server to be ready..."
            kubectl wait --for=condition=Ready pods \
              -l k8s-app=metrics-server \
              -n kube-system \
              --timeout=120s || {
                echo "‚ö†Ô∏è Metrics Server not ready yet, continuing..."
                kubectl get pods -n kube-system -l k8s-app=metrics-server
              }

            echo "‚úÖ Metrics Server installed"
          fi

          # Verify metrics API is available
          echo "üîç Verifying Metrics API..."
          kubectl top nodes || echo "‚ö†Ô∏è Metrics API not ready yet (this is normal, may take 1-2 minutes)"

      - name: Install Cluster Autoscaler
        run: |
          echo "[Step 1] Installing Cluster Autoscaler for automatic node scaling..."

          # Check if Cluster Autoscaler already exists
          if kubectl get deployment cluster-autoscaler -n kube-system &>/dev/null; then
            echo "INFO: Cluster Autoscaler already installed"
          else
            echo "[Step 2] Downloading and applying Cluster Autoscaler manifest..."

            # Download the Cluster Autoscaler manifest for EKS
            curl -s https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml | \
            sed "s/<YOUR CLUSTER NAME>/eks-1/g" | \
            kubectl apply -f -

            # Add annotation to skip eviction (prevent autoscaler from being evicted)
            kubectl -n kube-system annotate deployment.apps/cluster-autoscaler \
              cluster-autoscaler.kubernetes.io/safe-to-evict="false" --overwrite

            # Patch the deployment to use the correct image version for EKS 1.33
            kubectl -n kube-system set image deployment/cluster-autoscaler \
              cluster-autoscaler=registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0

            # Add options for better autoscaling behavior
            kubectl -n kube-system patch deployment cluster-autoscaler \
              -p '{"spec":{"template":{"spec":{"containers":[{"name":"cluster-autoscaler","command":["./cluster-autoscaler","--v=4","--stderrthreshold=info","--cloud-provider=aws","--skip-nodes-with-local-storage=false","--expander=least-waste","--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/eks-1","--balance-similar-node-groups","--skip-nodes-with-system-pods=false"]}]}}}}'

            echo "[Step 3] Waiting for Cluster Autoscaler to be ready..."
            kubectl wait --for=condition=available \
              --timeout=180s \
              deployment/cluster-autoscaler \
              -n kube-system || {
                echo "WARNING: Cluster Autoscaler not ready yet"
                kubectl get pods -n kube-system -l app=cluster-autoscaler
              }

            echo "SUCCESS: Cluster Autoscaler installed"
          fi

          echo "[Step 4] Verifying Cluster Autoscaler status..."
          kubectl get deployment cluster-autoscaler -n kube-system
          kubectl logs -n kube-system deployment/cluster-autoscaler --tail=10 || echo "INFO: Logs will be available once pod starts"

      - name: Wait for pods to be healthy
        run: |
          echo "Waiting for all pods to be healthy (max 5 minutes)..."

          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n dev --no-headers 2>/dev/null | grep -c "1/1.*Running" || echo "0")
            TOTAL_PODS=$(kubectl get pods -n dev --no-headers 2>/dev/null | wc -l | xargs || echo "0")

            echo "Attempt $i/30: $READY_PODS/$TOTAL_PODS pods ready"

            if [ "$READY_PODS" -ge "8" ] && [ "$READY_PODS" == "$TOTAL_PODS" ]; then
              echo "All pods are healthy!"
              kubectl get pods -n dev
              break
            fi

            if [ "$i" == "30" ]; then
              echo "‚ö†Ô∏è Warning: Not all pods ready after 5 minutes"
              echo ""
              echo "=== Pod Status ==="
              kubectl get pods -n dev

              echo ""
              echo "=== ArgoCD Applications ==="
              kubectl get applications -n argocd

              echo ""
              echo "=== Recent Pod Events ==="
              kubectl get events -n dev --sort-by='.lastTimestamp' --field-selector type=Warning | tail -20

              echo ""
              echo "=== Pod Details (Not Running) ==="
              kubectl get pods -n dev --field-selector status.phase!=Running -o wide 2>/dev/null || echo "No pods in non-running state"
            fi

            sleep 10
          done

      - name: Get access URLs
        id: get-urls
        continue-on-error: true
        run: |
          echo "Getting ArgoCD credentials..."
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d || echo "N/A")

          echo "Getting LoadBalancer URLs (waiting up to 3 minutes)..."

          # Get ArgoCD URL
          ARGOCD_URL=""
          for i in {1..18}; do
            ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$ARGOCD_URL" ] && ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$ARGOCD_URL" ]; then
              echo "ArgoCD URL found: $ARGOCD_URL"
              break
            fi
            echo "Waiting for ArgoCD LoadBalancer... ($i/18)"
            sleep 10
          done

          # Get Frontend URL
          FRONTEND_URL=""
          for i in {1..18}; do
            FRONTEND_URL=$(kubectl get svc frontend-dev -n dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            [ -z "$FRONTEND_URL" ] && FRONTEND_URL=$(kubectl get svc frontend-dev -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")

            if [ -n "$FRONTEND_URL" ]; then
              echo "Frontend URL found: $FRONTEND_URL"
              break
            fi
            echo "Waiting for Frontend LoadBalancer... ($i/18)"
            sleep 10
          done

          [ -z "$ARGOCD_URL" ] && ARGOCD_URL="Pending (LoadBalancer provisioning)"
          [ -z "$FRONTEND_URL" ] && FRONTEND_URL="Pending (LoadBalancer provisioning)"

          echo "argocd_password=$ARGOCD_PASSWORD" >> $GITHUB_OUTPUT
          echo "argocd_url=$ARGOCD_URL" >> $GITHUB_OUTPUT
          echo "frontend_url=$FRONTEND_URL" >> $GITHUB_OUTPUT

  # Output access information
  output-access-info:
    name: Output Access Information
    needs: [sync-and-verify]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

      - name: Get monitoring info
        id: monitoring-info
        run: |
          echo "‚è≥ Checking monitoring stack status (deployed by ArgoCD)..."

          # Wait briefly for Grafana service (max 30s, non-blocking)
          for i in {1..6}; do
            if kubectl get svc kube-prometheus-stack-grafana -n monitoring &>/dev/null; then
              echo "‚úÖ Grafana service found"
              break
            fi
            echo "Attempt $i/6: Waiting for Grafana service..."
            sleep 5
          done

          # Get Grafana URL (may not be ready yet)
          GRAFANA_URL=$(kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          [ -z "$GRAFANA_URL" ] && GRAFANA_URL=$(kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
          [ -z "$GRAFANA_URL" ] && GRAFANA_URL="Deploying via ArgoCD - Check: kubectl get svc -n monitoring"

          # Get Grafana password
          GRAFANA_PASSWORD=$(kubectl get secret grafana-admin-credentials -n monitoring -o jsonpath="{.data.admin-password}" 2>/dev/null | base64 -d || echo "Check secret: grafana-admin-credentials")

          echo "grafana_url=$GRAFANA_URL" >> $GITHUB_OUTPUT
          echo "grafana_password=$GRAFANA_PASSWORD" >> $GITHUB_OUTPUT

      - name: Display access information
        run: |
          echo "=========================================="
          echo "  DEPLOYMENT COMPLETE!"
          echo "=========================================="
          echo ""
          echo "Frontend Application:"
          echo "  URL: http://${{ needs.sync-and-verify.outputs.frontend_url }}"
          echo ""
          echo "ArgoCD Dashboard:"
          echo "  URL:      http://${{ needs.sync-and-verify.outputs.argocd_url }}"
          echo "  Username: admin"
          echo "  Password: ${{ needs.sync-and-verify.outputs.argocd_password }}"
          echo ""
          echo "Grafana Dashboard:"
          echo "  URL:      http://${{ steps.monitoring-info.outputs.grafana_url }}"
          echo "  Username: admin"
          echo "  Password: ${{ steps.monitoring-info.outputs.grafana_password }}"
          echo ""
          echo "RDS PostgreSQL Database:"
          echo "  Host:     ${{ needs.sync-and-verify.outputs.rds_endpoint }}"
          echo "  Port:     5432"
          echo "  Database: postgres"
          echo "  Username: postgres"
          echo "  Password: (stored in GitHub Secret: RDS_PASSWORD)"
          echo ""
          echo "=========================================="

      - name: Deployment summary
        run: |
          echo "## üöÄ Development Deployment Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üåê Access URLs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Frontend Application:**" >> $GITHUB_STEP_SUMMARY
          echo "- http://${{ needs.sync-and-verify.outputs.frontend_url }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ArgoCD Dashboard:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: http://${{ needs.sync-and-verify.outputs.argocd_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: \`${{ needs.sync-and-verify.outputs.argocd_password }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Monitoring Stack (Managed by ArgoCD)" >> $GITHUB_STEP_SUMMARY
          echo "**Grafana Dashboard:**" >> $GITHUB_STEP_SUMMARY
          echo "- URL: http://${{ steps.monitoring-info.outputs.grafana_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`admin\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: \`${{ steps.monitoring-info.outputs.grafana_password }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Prometheus:**" >> $GITHUB_STEP_SUMMARY
          echo "- Access via: \`kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090\`" >> $GITHUB_STEP_SUMMARY
          echo "- Then visit: http://localhost:9090" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üóÑÔ∏è RDS PostgreSQL Database" >> $GITHUB_STEP_SUMMARY
          echo "- Host: \`${{ needs.sync-and-verify.outputs.rds_endpoint }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Port: \`5432\`" >> $GITHUB_STEP_SUMMARY
          echo "- Database: \`postgres\`" >> $GITHUB_STEP_SUMMARY
          echo "- Username: \`postgres\`" >> $GITHUB_STEP_SUMMARY
          echo "- Password: **(stored in GitHub Secret: RDS_PASSWORD)**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> üí° To get password: Go to Settings ‚Üí Secrets ‚Üí RDS_PASSWORD" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üì¶ Deployment Details" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment:** Development" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ‚úÖ What Was Deployed" >> $GITHUB_STEP_SUMMARY
          echo "- Docker images built and pushed to ECR" >> $GITHUB_STEP_SUMMARY
          echo "- Helm values updated with new image tags" >> $GITHUB_STEP_SUMMARY
          echo "- ArgoCD applications synced (including monitoring stack)" >> $GITHUB_STEP_SUMMARY
          echo "- All applications managed via GitOps (ArgoCD)" >> $GITHUB_STEP_SUMMARY
          echo "- LoadBalancers provisioned and accessible" >> $GITHUB_STEP_SUMMARY
