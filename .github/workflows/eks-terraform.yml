name: EKS Terraform Deployment

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Choose action to perform'
        required: true
        default: 'plan'
        type: choice
        options:
          - plan
          - apply
          - destroy
      environment:
        description: 'Target environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - prod

env:
  AWS_REGION: us-east-1
  TF_VERSION: '1.9.0'
  TF_VAR_bastion_public_key: ${{ secrets.BASTION_PUBLIC_KEY }}
  TF_VAR_rds_password: ${{ secrets.RDS_PASSWORD || 'demo123456789' }}

permissions:
  contents: read
  id-token: write
  pull-requests: write

jobs:
  terraform-deploy:
    name: Terraform ${{ github.event.inputs.action || 'plan' }}
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: terraform/environments/${{ github.event.inputs.environment || 'dev' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.GITHUB_ACTIONS_IAM_ROLE_ARN || secrets.AWS_IAM_ROLE_ARN }}
          role-duration-seconds: 7200
          role-session-name: github-actions-eks-deployment-${{ github.run_id }}

      - name: Verify AWS credentials and SCP status
        run: |
          echo "### AWS Identity Verification" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          aws sts get-caller-identity >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "âœ… AWS credentials configured successfully"
          echo ""
          echo "### EKS Cluster Status Check" >> $GITHUB_STEP_SUMMARY

          # Wait a moment for credentials to propagate
          sleep 5

          # Check if we can describe the cluster
          CLUSTER_NAME="eks-1"
          if [ "${{ github.event.inputs.environment }}" == "prod" ]; then
            CLUSTER_NAME="eks-prod"
          fi

          if aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} >> $GITHUB_STEP_SUMMARY 2>&1; then
            echo "âœ… EKS cluster access verified for $CLUSTER_NAME"
          else
            echo "âš ï¸ EKS cluster access blocked - checking alternative permissions..."
            aws eks list-clusters --region ${{ env.AWS_REGION }} >> $GITHUB_STEP_SUMMARY 2>&1 || echo "EKS API access restricted"
          fi

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Terraform Init
        id: init
        run: terraform init -upgrade

      - name: Import Existing EKS Access Entries
        if: github.event.inputs.action == 'apply'
        continue-on-error: true
        run: |
          echo "ğŸ”„ Checking for existing EKS access entries to import..."

          # Get current IAM user/role ARN
          CURRENT_ARN=$(aws sts get-caller-identity --query Arn --output text)
          echo "Current IAM Principal: $CURRENT_ARN"

          # List existing access entries
          CLUSTER_NAME="eks-1"
          if [ "${{ github.event.inputs.environment }}" == "prod" ]; then
            CLUSTER_NAME="eks-prod"
          fi
          echo "Checking access entries for cluster: $CLUSTER_NAME"

          # Check if access entry exists
          if aws eks describe-access-entry \
            --cluster-name "$CLUSTER_NAME" \
            --principal-arn "$CURRENT_ARN" \
            --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "ğŸ“¥ Access entry already exists - importing into Terraform state..."

            # Import the access entry into Terraform state
            terraform import \
              'module.iam_access.aws_eks_access_entry.current_user_access' \
              "${CLUSTER_NAME}:${CURRENT_ARN}" 2>&1 | grep -v "Refreshing state" || true

            echo "âœ… Import completed (or already in state)"
          else
            echo "â„¹ï¸ No existing access entry found - Terraform will create it"
          fi

          echo "ğŸ¯ Access entry check completed"

      - name: Terraform Plan
        id: plan
        if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'apply'
        continue-on-error: true
        run: |
          echo "Running Terraform plan with enhanced error handling..."
          if terraform plan -out=tfplan 2>&1 | tee tfplan.log; then
            echo "âœ… Terraform plan completed successfully"
          else
            echo "âŒ Terraform plan failed. Checking for SCP-related errors..."
            if grep -q "AccessDeniedException" tfplan.log; then
              echo "âŒ EKS access blocked by Service Control Policy"
              echo "ğŸ”„ Please ensure AWS Organizations SCP allows EKS service access"
            fi
            if grep -q "inline_policy is deprecated" tfplan.log; then
              echo "âš ï¸ inline_policy deprecation warning detected (non-blocking)"
            fi
            echo "ğŸ”„ Plan failed - will trigger auto-destroy if action is apply"
            exit 1
          fi

      - name: Terraform Apply
        id: apply
        if: github.event.inputs.action == 'apply'
        continue-on-error: true
        run: |
          echo "Running Terraform apply with enhanced error handling..."
          if terraform apply -auto-approve tfplan 2>&1 | tee tfapply.log; then
            echo "âœ… Terraform apply completed successfully"
          else
            echo "âŒ Terraform apply failed. Checking for SCP-related errors..."
            if grep -q "AccessDeniedException" tfapply.log; then
              echo "âŒ EKS access blocked by Service Control Policy"
              echo "ğŸ”„ Please ensure AWS Organizations SCP allows EKS service access"
            fi
            echo "ğŸ”„ Deployment failed - will trigger auto-destroy"
            exit 1
          fi

      - name: Terraform Destroy
        id: destroy
        if: github.event.inputs.action == 'destroy'
        run: |
          echo "Running Terraform destroy with enhanced error handling..."
          terraform destroy -auto-approve 2>&1 | tee tfdestroy.log || {
            echo "Terraform destroy failed. Checking for SCP-related errors..."
            if grep -q "AccessDeniedException" tfdestroy.log; then
              echo "âŒ EKS access blocked by Service Control Policy"
              echo "ğŸ”„ Please ensure AWS Organizations SCP allows EKS service access"
              exit 1
            fi
          }

      - name: Configure kubectl and Verify EKS Access
        if: github.event.inputs.action != 'destroy'
        run: |
          echo "### Configuring kubectl for EKS Cluster Access" >> $GITHUB_STEP_SUMMARY

          # Get cluster name from Terraform output
          CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "eks-1")
          echo "Cluster Name: $CLUSTER_NAME" >> $GITHUB_STEP_SUMMARY

          # Configure kubectl with enhanced error handling
          echo "Updating kubeconfig..." >> $GITHUB_STEP_SUMMARY
          if aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name $CLUSTER_NAME \
            --alias $CLUSTER_NAME; then
            echo "âœ… kubeconfig updated successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Failed to update kubeconfig" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          # Verify kubectl configuration
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### kubectl Configuration Verification" >> $GITHUB_STEP_SUMMARY
          echo "Current context:" >> $GITHUB_STEP_SUMMARY
          kubectl config current-context >> $GITHUB_STEP_SUMMARY 2>&1
          echo "" >> $GITHUB_STEP_SUMMARY

          # Test EKS token generation
          echo "### EKS Token Generation Test" >> $GITHUB_STEP_SUMMARY
          if aws eks get-token --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} >> $GITHUB_STEP_SUMMARY 2>&1; then
            echo "âœ… EKS token generation successful" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ EKS token generation failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          # Wait for credentials to propagate
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Waiting for credentials to propagate..." >> $GITHUB_STEP_SUMMARY
          sleep 10

          # Test kubectl access with retry logic
          echo "### Testing kubectl Access" >> $GITHUB_STEP_SUMMARY
          MAX_RETRIES=3
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES" >> $GITHUB_STEP_SUMMARY

            if kubectl get nodes --v=2 >> $GITHUB_STEP_SUMMARY 2>&1; then
              echo "âœ… kubectl access verified successfully" >> $GITHUB_STEP_SUMMARY
              break
            else
              echo "âš ï¸ kubectl access failed, retrying..." >> $GITHUB_STEP_SUMMARY
              RETRY_COUNT=$((RETRY_COUNT + 1))
              sleep 15

              # Refresh AWS credentials before retry
              aws sts get-caller-identity > /dev/null 2>&1

              # Refresh kubectl token
              aws eks update-kubeconfig \
                --region ${{ env.AWS_REGION }} \
                --name $CLUSTER_NAME \
                --alias $CLUSTER_NAME
            fi
          done

          if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
            echo "âŒ All kubectl access attempts failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Debugging Information" >> $GITHUB_STEP_SUMMARY
            echo "AWS Identity:" >> $GITHUB_STEP_SUMMARY
            aws sts get-caller-identity >> $GITHUB_STEP_SUMMARY 2>&1
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "kubectl config:" >> $GITHUB_STEP_SUMMARY
            kubectl config view --raw >> $GITHUB_STEP_SUMMARY 2>&1
            exit 1
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Cluster Information" >> $GITHUB_STEP_SUMMARY
          echo "Node status:" >> $GITHUB_STEP_SUMMARY
          kubectl get nodes -o wide >> $GITHUB_STEP_SUMMARY 2>&1
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Namespace status:" >> $GITHUB_STEP_SUMMARY
          kubectl get namespaces >> $GITHUB_STEP_SUMMARY 2>&1

      - name: Deployment Summary
        if: github.event.inputs.action != 'destroy'
        run: |
          echo "### Terraform ${{ github.event.inputs.action }} completed successfully ğŸ‰" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ github.event.inputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Region:** ${{ env.AWS_REGION }}" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ github.event.inputs.action }}" != "destroy" ]]; then
            echo "**Cluster:** $(terraform output -raw cluster_name)" >> $GITHUB_STEP_SUMMARY
            echo "**RDS Endpoint:** $(terraform output -raw rds_instance_endpoint)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Auto Destroy on Failure
        if: |
          always() &&
          github.event.inputs.action != 'destroy' &&
          github.event.inputs.action != 'plan' &&
          (steps.init.outcome == 'failure' ||
           steps.plan.outcome == 'failure' ||
           steps.apply.outcome == 'failure')
        run: |
          echo "âŒ Deployment failed. Auto-destroying resources..."
          echo "Failed step: Init=${{ steps.init.outcome }}, Plan=${{ steps.plan.outcome }}, Apply=${{ steps.apply.outcome }}"

          # Always re-init to ensure proper state
          terraform init -upgrade

          # Force destroy all resources to prevent costs
          echo "ğŸ”¥ Destroying all resources to prevent costs..."
          terraform destroy -auto-approve 2>&1 | tee "tfdestroy.log" || {
            echo "âš ï¸ Destroy failed, attempting with lock override after 30s wait..."
            sleep 30
            terraform destroy -auto-approve -lock=false || true
          }

          echo "ğŸ§¹ Auto-destroy completed successfully"
          echo "ğŸ’° AWS resources cleaned up to prevent further costs"

          # Mark job as failed to indicate deployment didn't succeed
          exit 1
