name: Deploy Applications to EKS

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        options:
          - dev
          - staging
          - prod
        default: 'dev'
      deployment_method:
        description: 'Deployment method'
        required: true
        type: choice
        options:
          - argocd
          - helm
        default: 'helm'
      services:
        description: 'Services to deploy (comma-separated or "all")'
        required: true
        default: 'all'
  workflow_call:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: false
        type: string
        default: 'dev'
      deployment_method:
        description: 'Deployment method'
        required: false
        type: string
        default: 'helm'
      services:
        description: 'Services to deploy (comma-separated or "all")'
        required: false
        type: string
        default: 'all'

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: eks-1

permissions:
  contents: read

jobs:
  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set ECR Account ID
        id: aws-account
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "account_id=$ACCOUNT_ID" >> $GITHUB_OUTPUT
          echo "ECR Account ID: $ACCOUNT_ID"

      - name: Verify IAM Identity for EKS Access
        id: verify-identity
        run: |
          echo "=== Verifying IAM Identity for EKS Access ==="

          # Get current IAM identity
          CALLER_IDENTITY=$(aws sts get-caller-identity --output json)
          echo "Current IAM Identity:"
          echo "$CALLER_IDENTITY" | jq .

          CALLER_ARN=$(echo "$CALLER_IDENTITY" | jq -r '.Arn')
          CALLER_USER_ID=$(echo "$CALLER_IDENTITY" | jq -r '.UserId')
          CALLER_ACCOUNT=$(echo "$CALLER_IDENTITY" | jq -r '.Account')

          echo ""
          echo "Caller ARN: $CALLER_ARN"
          echo "Caller User ID: $CALLER_USER_ID"
          echo "Account: $CALLER_ACCOUNT"

          # Expected IAM user for EKS access
          EXPECTED_USER="nt114-devsecops-github-actions-user"
          EXPECTED_ARN="arn:aws:iam::${CALLER_ACCOUNT}:user/${EXPECTED_USER}"

          echo ""
          echo "Expected IAM User: $EXPECTED_USER"
          echo "Expected ARN: $EXPECTED_ARN"

          # Check if current identity matches expected
          if [[ "$CALLER_ARN" == "$EXPECTED_ARN" ]]; then
            echo ""
            echo "âœ… Identity verification PASSED!"
            echo "Current credentials match the IAM user with EKS access entry."
            echo "identity_verified=true" >> $GITHUB_OUTPUT
          else
            echo ""
            echo "âŒ WARNING: Identity mismatch detected!"
            echo "Current ARN: $CALLER_ARN"
            echo "Expected ARN: $EXPECTED_ARN"
            echo ""
            echo "The AWS credentials may not match the IAM user configured for EKS access."
            echo "This may cause kubectl authentication failures."
            echo ""
            echo "Please verify that AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY"
            echo "in GitHub Secrets belong to user: $EXPECTED_USER"
            echo "identity_verified=false" >> $GITHUB_OUTPUT
          fi

          # List EKS access entries for verification
          echo ""
          echo "=== EKS Access Entries for cluster: ${{ env.CLUSTER_NAME }} ==="
          aws eks list-access-entries \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --output json | jq -r '.accessEntries[]'

          # Check if current identity has access entry
          echo ""
          echo "Checking if current identity has EKS access entry..."
          if aws eks list-access-entries \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --output json | jq -r '.accessEntries[]' | grep -q "$CALLER_ARN"; then
            echo "âœ… Access entry found for current identity"
          else
            echo "âŒ No access entry found for: $CALLER_ARN"
            echo ""
            echo "To grant access, add an EKS access entry for this identity:"
            echo "aws eks create-access-entry \\"
            echo "  --cluster-name ${{ env.CLUSTER_NAME }} \\"
            echo "  --principal-arn $CALLER_ARN \\"
            echo "  --region ${{ env.AWS_REGION }}"
          fi

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Check EKS Cluster Exists
        run: |
          echo "Checking if EKS cluster '${{ env.CLUSTER_NAME }}' exists..."
          if aws eks describe-cluster --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }} > /dev/null 2>&1; then
            echo "Cluster found: ${{ env.CLUSTER_NAME }}"
          else
            echo "ERROR: Cluster '${{ env.CLUSTER_NAME }}' not found!"
            echo ""
            echo "Available clusters:"
            aws eks list-clusters --region ${{ env.AWS_REGION }}
            echo ""
            echo "Please create the EKS cluster first by running:"
            echo "  gh workflow run eks-terraform.yml -f action=apply -f environment=dev"
            exit 1
          fi

      - name: Configure kubectl
        run: |
          echo "=== Configuring kubectl for cluster: ${{ env.CLUSTER_NAME }} ==="

          # Update kubeconfig
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          echo ""
          echo "=== Testing kubectl authentication ==="

          # Test kubectl access with detailed error handling
          if kubectl cluster-info 2>&1 | tee /tmp/kubectl-output.txt; then
            echo ""
            echo "âœ… kubectl authentication successful!"
          else
            echo ""
            echo "âŒ kubectl authentication FAILED!"
            echo ""
            echo "=== Troubleshooting Information ==="

            # Show error output
            echo "Error output:"
            cat /tmp/kubectl-output.txt

            echo ""
            echo "Current IAM identity:"
            aws sts get-caller-identity

            echo ""
            echo "EKS access entries:"
            aws eks list-access-entries \
              --cluster-name ${{ env.CLUSTER_NAME }} \
              --region ${{ env.AWS_REGION }}

            echo ""
            echo "Kubeconfig content:"
            cat ~/.kube/config | grep -A 20 "current-context"

            echo ""
            echo "=== Possible Solutions ==="
            echo "1. Verify AWS credentials match IAM user: nt114-devsecops-github-actions-user"
            echo "2. Check EKS access entry exists for this IAM user"
            echo "3. Verify EKS access policy association grants required permissions"
            echo "4. Check if aws-auth ConfigMap needs updating (for API_AND_CONFIG_MAP mode)"

            exit 1
          fi

          echo ""
          echo "=== Node status ==="
          kubectl get nodes

      - name: Create namespace
        run: |
          kubectl create namespace ${{ inputs.environment }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Create ECR Secret
        run: |
          echo "Creating ECR secret for account ${{ steps.aws-account.outputs.account_id }}..."
          kubectl create secret docker-registry ecr-secret \
            --docker-server=${{ steps.aws-account.outputs.account_id }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com \
            --docker-username=AWS \
            --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
            --namespace=${{ inputs.environment }} \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "ECR secret created successfully"

      - name: Install AWS Load Balancer Controller
        id: alb-install
        run: |
          echo "=== Installing/Checking AWS Load Balancer Controller ==="

          # Get cluster info
          CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
          VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.resourcesVpcConfig.vpcId" --output text)
          echo "VPC ID: $VPC_ID"

          # Check if ALB controller is already running
          if kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --no-headers 2>/dev/null | grep -q "Running"; then
            echo "ALB Controller is already running"
            echo "alb_ready=true" >> $GITHUB_OUTPUT
          else
            echo "Installing AWS Load Balancer Controller..."

            # Add eks-charts repo
            helm repo add eks https://aws.github.io/eks-charts
            helm repo update

            # Create IAM service account for ALB controller (if not exists)
            # Note: This requires the IAM role to be created by Terraform first

            # Check if service account exists
            if ! kubectl get serviceaccount aws-load-balancer-controller -n kube-system &>/dev/null; then
              echo "Creating service account..."

              # Get OIDC provider
              OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text | sed 's|https://||')

              # Create service account with IAM role annotation
              cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: aws-load-balancer-controller
            namespace: kube-system
            annotations:
              eks.amazonaws.com/role-arn: arn:aws:iam::${{ steps.aws-account.outputs.account_id }}:role/${{ env.CLUSTER_NAME }}-alb-controller
          EOF
            fi

            # Install or upgrade ALB controller
            helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=$CLUSTER_NAME \
              --set region=${{ env.AWS_REGION }} \
              --set vpcId=$VPC_ID \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --wait --timeout 5m || true

            # Wait for controller to be ready
            echo "Waiting for ALB Controller to be ready..."
            kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s || true

            echo "alb_ready=true" >> $GITHUB_OUTPUT
          fi

          # Create IngressClass if not exists
          if ! kubectl get ingressclass alb &>/dev/null; then
            echo "Creating ALB IngressClass..."
            cat <<EOF | kubectl apply -f -
          apiVersion: networking.k8s.io/v1
          kind: IngressClass
          metadata:
            name: alb
            annotations:
              ingressclass.kubernetes.io/is-default-class: "true"
          spec:
            controller: ingress.k8s.aws/alb
          EOF
          fi

          echo ""
          echo "=== ALB Controller Status ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          echo ""
          echo "Ingress Classes:"
          kubectl get ingressclass

      - name: Deploy with Helm (Direct)
        if: inputs.deployment_method == 'helm'
        run: |
          echo "=== Deploying with Helm (Direct) ==="

          # Cleanup stuck Helm releases
          echo "Checking for stuck Helm releases..."
          SERVICES=("api-gateway" "user-management-service" "exercises-service" "scores-service" "frontend")

          for service in "${SERVICES[@]}"; do
            if helm status "$service" -n ${{ inputs.environment }} 2>/dev/null | grep -q "STATUS: pending"; then
              echo "Found stuck release: $service. Cleaning up..."
              helm uninstall "$service" -n ${{ inputs.environment }} --wait || true
            elif helm status "$service" -n ${{ inputs.environment }} 2>/dev/null | grep -q "STATUS: failed"; then
              echo "Found failed release: $service. Cleaning up..."
              helm uninstall "$service" -n ${{ inputs.environment }} --wait || true
            fi
          done

          echo "Cleanup complete"

          # Set common Helm values
          ECR_REPO="${{ steps.aws-account.outputs.account_id }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/nt114-devsecops"

          # Update values-eks.yaml with AWS Account ID
          for service in api-gateway user-management-service exercises-service scores-service frontend; do
            VALUES_FILE="helm/${service}/values-eks.yaml"
            if [ -f "$VALUES_FILE" ]; then
              sed -i "s/\${AWS_ACCOUNT_ID}/${{ steps.aws-account.outputs.account_id }}/g" "$VALUES_FILE"
            fi
          done

          # Deploy services in correct order with dependency management
          deploy_service() {
            local service=$1
            local wait_for_ready=${2:-true}

            echo ""
            echo "Deploying $service..."

            # Remove existing failed deployment if exists
            if helm status "$service" -n ${{ inputs.environment }} 2>/dev/null | grep -q "STATUS: failed\|STATUS: pending"; then
              echo "Removing failed deployment of $service..."
              helm uninstall "$service" -n ${{ inputs.environment }} --wait || true
              sleep 10
            fi

            # Deploy with correct values (AWS account ID already substituted)
            if helm upgrade --install "$service" "./helm/$service" \
              --namespace ${{ inputs.environment }} \
              --set image.tag=latest \
              --set image.pullPolicy=Always \
              -f "./helm/$service/values-eks.yaml" \
              --wait --timeout 10m; then
              echo "âœ… $service deployed successfully"

              # Additional health check if requested
              if [ "$wait_for_ready" = true ]; then
                echo "Waiting for $service pods to be ready..."
                kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} --timeout=300s || true
              fi
            else
              echo "âŒ $service deployment failed!"
              echo ""
              echo "=== POD STATUS ==="
              kubectl get pods -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} || true
              echo ""
              echo "=== POD DESCRIBE ==="
              kubectl describe pod -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} || true
              echo ""
              echo "=== POD LOGS ==="
              kubectl logs -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} --tail=100 || true
              echo ""
              echo "=== EVENTS ==="
              kubectl get events -n ${{ inputs.environment }} --sort-by='.lastTimestamp' --field-selector involvedObject.kind=Pod | grep "$service" | tail -10 || true
              return 1
            fi
          }

          # Deploy services in dependency order
          DEPLOY_FAILED=false

          echo "ðŸš€ Starting deployment in dependency order..."

          # Phase 1: Backend Services (no dependencies)
          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"user-management-service"* ]]; then
            echo ""
            echo "=== Phase 1: User Management Service ==="
            deploy_service "user-management-service" || DEPLOY_FAILED=true
          fi

          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"exercises-service"* ]]; then
            echo ""
            echo "=== Phase 1: Exercises Service ==="
            deploy_service "exercises-service" || DEPLOY_FAILED=true
          fi

          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"scores-service"* ]]; then
            echo ""
            echo "=== Phase 1: Scores Service ==="
            deploy_service "scores-service" || DEPLOY_FAILED=true
          fi

          # Phase 2: API Gateway (depends on backend services)
          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"api-gateway"* ]]; then
            echo ""
            echo "=== Phase 2: API Gateway ==="
            deploy_service "api-gateway" || DEPLOY_FAILED=true
          fi

          # Phase 3: Frontend (depends on API Gateway)
          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"frontend"* ]]; then
            echo ""
            echo "=== Phase 3: Frontend ==="
            deploy_service "frontend" || DEPLOY_FAILED=true
          fi

          if [ "$DEPLOY_FAILED" = true ]; then
            echo ""
            echo "âŒ One or more deployments failed!"
            exit 1
          fi

          echo ""
          echo "ðŸŽ‰ All services deployed successfully!"

      - name: Deploy with ArgoCD
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Deploying with ArgoCD (GitOps) ==="

          # Check if ArgoCD is installed
          if ! kubectl get namespace argocd &>/dev/null; then
            echo "Installing ArgoCD..."
            kubectl create namespace argocd
            kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

            echo "Waiting for ArgoCD to be ready..."
            kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd
          else
            echo "ArgoCD already installed"
          fi

          # AWS Account ID already substituted in values-eks.yaml files
          echo "Using pre-configured values-eks.yaml files with AWS Account ID..."

          # Apply ArgoCD applications
          echo "Applying ArgoCD applications..."
          kubectl apply -f argocd/argocd-applications.yaml

          echo ""
          echo "=== ArgoCD Applications Status ==="
          sleep 10
          kubectl get applications -n argocd

          echo ""
          echo "ArgoCD will now sync applications automatically."
          echo "Access ArgoCD UI to monitor deployment progress."

      - name: Wait for ALB Provisioning
        run: |
          echo "=== Waiting for ALB Ingress to be provisioned ==="

          # Wait up to 5 minutes for ingress to get an address
          for i in {1..30}; do
            INGRESS_COUNT=$(kubectl get ingress -n ${{ inputs.environment }} --no-headers 2>/dev/null | wc -l)

            if [ "$INGRESS_COUNT" -gt 0 ]; then
              echo "Checking ingress addresses..."
              kubectl get ingress -n ${{ inputs.environment }}

              # Check if any ingress has an address
              ADDRESSES=$(kubectl get ingress -n ${{ inputs.environment }} -o jsonpath='{.items[*].status.loadBalancer.ingress[0].hostname}' 2>/dev/null)

              if [ ! -z "$ADDRESSES" ] && [ "$ADDRESSES" != "" ]; then
                echo "ALB addresses found!"
                break
              fi
            fi

            echo "Waiting for ALB to be provisioned... (attempt $i/30)"
            sleep 10
          done

          echo ""
          echo "=== Final Ingress Status ==="
          kubectl get ingress -n ${{ inputs.environment }} -o wide || echo "No ingress resources found"

      - name: Get Deployment Status
        run: |
          echo "### Deployment Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ inputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Method:** ${{ inputs.deployment_method }}" >> $GITHUB_STEP_SUMMARY
          echo "**Region:** ${{ env.AWS_REGION }}" >> $GITHUB_STEP_SUMMARY
          echo "**Cluster:** ${{ env.CLUSTER_NAME }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### Deployed Services" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get deployments -n ${{ inputs.environment }} >> $GITHUB_STEP_SUMMARY || echo "No deployments found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### Service Endpoints" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get services -n ${{ inputs.environment }} >> $GITHUB_STEP_SUMMARY || echo "No services found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### ALB Ingress URLs" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get ingress -n ${{ inputs.environment }} -o wide >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No ingress found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Pod Status" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get pods -n ${{ inputs.environment }} -o wide >> $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Display Access URLs
        run: |
          echo ""
          echo "=============================================="
          echo "         APPLICATION ACCESS URLS              "
          echo "=============================================="
          echo ""

          # Get Frontend URL
          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || FRONTEND_URL=""
          if [ ! -z "$FRONTEND_URL" ]; then
            echo "Frontend URL: http://${FRONTEND_URL}"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Access URLs" >> $GITHUB_STEP_SUMMARY
            echo "- **Frontend:** http://${FRONTEND_URL}" >> $GITHUB_STEP_SUMMARY
          else
            echo "Frontend: Ingress not ready yet"
          fi

          # Get API Gateway URL
          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || API_URL=""
          if [ ! -z "$API_URL" ]; then
            echo "API Gateway URL: http://${API_URL}"
            echo "- **API Gateway:** http://${API_URL}" >> $GITHUB_STEP_SUMMARY
          else
            echo "API Gateway: Ingress not ready yet"
          fi

          echo ""
          echo "Note: ALB provisioning may take 2-5 minutes."
          echo "If URLs are empty, run: kubectl get ingress -n ${{ inputs.environment }}"
          echo ""

      - name: Get ArgoCD Access Info
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### ArgoCD Access" >> $GITHUB_STEP_SUMMARY

          # Get ArgoCD password
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d) || ARGOCD_PASSWORD="<not available>"

          # Get ArgoCD URL
          ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || ARGOCD_URL="<pending>"

          echo "- **ArgoCD URL:** https://${ARGOCD_URL}" >> $GITHUB_STEP_SUMMARY
          echo "- **Username:** admin" >> $GITHUB_STEP_SUMMARY
          echo "- **Password:** See workflow output (not shown in summary for security)" >> $GITHUB_STEP_SUMMARY

          echo ""
          echo "=== ArgoCD Access Information ==="
          echo "URL: https://${ARGOCD_URL}"
          echo "Username: admin"
          echo "Password: ${ARGOCD_PASSWORD}"

          echo ""
          echo "=== ArgoCD Application Status ==="
          kubectl get applications -n argocd

  verify:
    name: Verify Deployment
    runs-on: ubuntu-latest
    needs: [deploy]
    if: success()
    continue-on-error: true

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          echo "Configuring kubectl for cluster: ${{ env.CLUSTER_NAME }}"
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Test kubectl access
          if ! kubectl cluster-info > /dev/null 2>&1; then
            echo "âŒ kubectl authentication failed in verify job"
            echo "Current IAM identity:"
            aws sts get-caller-identity
            exit 1
          fi
          echo "âœ… kubectl configured successfully"

      - name: Wait for Pods Ready
        run: |
          echo "Waiting for pods to be ready..."

          # Wait up to 10 minutes for pods
          for i in {1..60}; do
            PENDING_PODS=$(kubectl get pods -n ${{ inputs.environment }} --no-headers 2>/dev/null | grep -v "Running\|Completed" | wc -l)

            if [ "$PENDING_PODS" -eq 0 ]; then
              echo "All pods are running!"
              break
            fi

            echo "Waiting for $PENDING_PODS pods to be ready... (attempt $i/60)"
            sleep 10
          done

          echo ""
          echo "=== Final Pod Status ==="
          kubectl get pods -n ${{ inputs.environment }} -o wide

          echo ""
          echo "=== Deployment Status ==="
          kubectl get deployments -n ${{ inputs.environment }}

      - name: Verify ALB and Display URLs
        run: |
          echo "### Final Access URLs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "=== Ingress Status ==="
          kubectl get ingress -n ${{ inputs.environment }} -o wide

          # Frontend
          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || true
          if [ ! -z "$FRONTEND_URL" ]; then
            echo ""
            echo "Frontend URL: http://${FRONTEND_URL}"
            echo "- **Frontend:** http://${FRONTEND_URL}" >> $GITHUB_STEP_SUMMARY
          fi

          # API Gateway
          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || true
          if [ ! -z "$API_URL" ]; then
            echo "API Gateway URL: http://${API_URL}"
            echo "- **API Gateway:** http://${API_URL}" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Run Health Checks
        run: |
          echo "### Health Check Results" >> $GITHUB_STEP_SUMMARY

          # Get Frontend URL and test
          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || true
          if [ ! -z "$FRONTEND_URL" ]; then
            echo "Testing Frontend at http://${FRONTEND_URL}..."
            if curl -sf --max-time 10 "http://${FRONTEND_URL}" > /dev/null; then
              echo "Frontend health check PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "Frontend health check FAILED or not reachable yet" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          # Get API Gateway URL and test
          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null) || true
          if [ ! -z "$API_URL" ]; then
            echo "Testing API Gateway at http://${API_URL}/health..."
            if curl -sf --max-time 10 "http://${API_URL}/health"; then
              echo "API Gateway health check PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "API Gateway health check FAILED or not reachable yet" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Events (Last 20)" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get events -n ${{ inputs.environment }} --sort-by='.lastTimestamp' | tail -20 >> $GITHUB_STEP_SUMMARY || echo "No events" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
