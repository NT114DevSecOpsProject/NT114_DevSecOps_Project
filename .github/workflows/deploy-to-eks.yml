name: Deploy Applications to EKS

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        options:
          - dev
          - staging
          - prod
        default: 'dev'
      deployment_method:
        description: 'Deployment method'
        required: true
        type: choice
        options:
          - argocd
          - helm
        default: 'helm'
      services:
        description: 'Services to deploy (comma-separated or "all")'
        required: true
        default: 'all'
  workflow_call:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: false
        type: string
        default: 'dev'
      deployment_method:
        description: 'Deployment method'
        required: false
        type: string
        default: 'helm'
      services:
        description: 'Services to deploy (comma-separated or "all")'
        required: false
        type: string
        default: 'all'

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: eks-1

permissions:
  contents: read

jobs:
  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set ECR Account ID
        id: aws-account
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "account_id=$ACCOUNT_ID" >> $GITHUB_OUTPUT
          echo "ECR Account ID: $ACCOUNT_ID"

      - name: Verify IAM Identity for EKS Access
        id: verify-identity
        run: |
          echo "=== Verifying IAM Identity for EKS Access ==="

          # Get current IAM identity
          CALLER_IDENTITY=$(aws sts get-caller-identity --output json)
          echo "Current IAM Identity:"
          echo "$CALLER_IDENTITY" | jq .

          CALLER_ARN=$(echo "$CALLER_IDENTITY" | jq -r '.Arn')
          CALLER_USER_ID=$(echo "$CALLER_IDENTITY" | jq -r '.UserId')
          CALLER_ACCOUNT=$(echo "$CALLER_IDENTITY" | jq -r '.Account')

          echo ""
          echo "Caller ARN: $CALLER_ARN"
          echo "Caller User ID: $CALLER_USER_ID"
          echo "Account: $CALLER_ACCOUNT"

          # Expected IAM user for EKS access
          EXPECTED_USER="nt114-devsecops-github-actions-user"
          EXPECTED_ARN="arn:aws:iam::${CALLER_ACCOUNT}:user/${EXPECTED_USER}"

          echo ""
          echo "Expected IAM User: $EXPECTED_USER"
          echo "Expected ARN: $EXPECTED_ARN"

          # Check if current identity matches expected
          if [[ "$CALLER_ARN" == "$EXPECTED_ARN" ]]; then
            echo ""
            echo "‚úÖ Identity verification PASSED!"
            echo "Current credentials match the IAM user with EKS access entry."
            echo "identity_verified=true" >> $GITHUB_OUTPUT
          else
            echo ""
            echo "‚ùå WARNING: Identity mismatch detected!"
            echo "Current ARN: $CALLER_ARN"
            echo "Expected ARN: $EXPECTED_ARN"
            echo ""
            echo "The AWS credentials may not match the IAM user configured for EKS access."
            echo "This may cause kubectl authentication failures."
            echo ""
            echo "Please verify that AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY"
            echo "in GitHub Secrets belong to user: $EXPECTED_USER"
            echo "identity_verified=false" >> $GITHUB_OUTPUT
          fi

          # List EKS access entries for verification
          echo ""
          echo "=== EKS Access Entries for cluster: ${{ env.CLUSTER_NAME }} ==="
          aws eks list-access-entries \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --output json | jq -r '.accessEntries[]'

          # Check if current identity has access entry
          echo ""
          echo "Checking if current identity has EKS access entry..."
          if aws eks list-access-entries \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --output json | jq -r '.accessEntries[]' | grep -q "$CALLER_ARN"; then
            echo "‚úÖ Access entry found for current identity"
          else
            echo "‚ùå No access entry found for: $CALLER_ARN"
            echo ""
            echo "To grant access, add an EKS access entry for this identity:"
            echo "aws eks create-access-entry \\"
            echo "  --cluster-name ${{ env.CLUSTER_NAME }} \\"
            echo "  --principal-arn $CALLER_ARN \\"
            echo "  --region ${{ env.AWS_REGION }}"
          fi

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Check EKS Cluster Exists
        run: |
          echo "Checking if EKS cluster '${{ env.CLUSTER_NAME }}' exists..."
          if aws eks describe-cluster --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }} > /dev/null 2>&1; then
            echo "Cluster found: ${{ env.CLUSTER_NAME }}"
          else
            echo "ERROR: Cluster '${{ env.CLUSTER_NAME }}' not found!"
            echo ""
            echo "Available clusters:"
            aws eks list-clusters --region ${{ env.AWS_REGION }}
            echo ""
            echo "Please create the EKS cluster first by running:"
            echo "  gh workflow run eks-terraform.yml -f action=apply -f environment=dev"
            exit 1
          fi

      - name: Configure kubectl
        run: |
          echo "=== Configuring kubectl for cluster: ${{ env.CLUSTER_NAME }} ==="

          # Update kubeconfig
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          echo ""
          echo "=== Testing kubectl authentication ==="

          # Test kubectl access with detailed error handling
          if kubectl cluster-info 2>&1 | tee /tmp/kubectl-output.txt; then
            echo ""
            echo "‚úÖ kubectl authentication successful!"
          else
            echo ""
            echo "‚ùå kubectl authentication FAILED!"
            echo ""
            echo "=== Troubleshooting Information ==="

            # Show error output
            echo "Error output:"
            cat /tmp/kubectl-output.txt

            echo ""
            echo "Current IAM identity:"
            aws sts get-caller-identity

            echo ""
            echo "EKS access entries:"
            aws eks list-access-entries \
              --cluster-name ${{ env.CLUSTER_NAME }} \
              --region ${{ env.AWS_REGION }}

            echo ""
            echo "Kubeconfig content:"
            cat ~/.kube/config | grep -A 20 "current-context"

            echo ""
            echo "=== Possible Solutions ==="
            echo "1. Verify AWS credentials match IAM user: nt114-devsecops-github-actions-user"
            echo "2. Check EKS access entry exists for this IAM user"
            echo "3. Verify EKS access policy association grants required permissions"
            echo "4. Check if aws-auth ConfigMap needs updating (for API_AND_CONFIG_MAP mode)"

            exit 1
          fi

          echo ""
          echo "=== Node status ==="
          kubectl get nodes

      - name: Create namespace
        run: |
          kubectl create namespace ${{ inputs.environment }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Create ECR Secret
        run: |
          echo "Creating ECR secret for account ${{ steps.aws-account.outputs.account_id }}..."
          kubectl create secret docker-registry ecr-secret \
            --docker-server=${{ steps.aws-account.outputs.account_id }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com \
            --docker-username=AWS \
            --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
            --namespace=${{ inputs.environment }} \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "ECR secret created successfully"

      - name: Create Database Secret
        run: |
          echo "=== Creating Database Secret ==="

          # Get RDS endpoint from Terraform state (via AWS)
          RDS_IDENTIFIER="nt114-postgres-${{ inputs.environment }}"

          echo "Looking for RDS instance: $RDS_IDENTIFIER"
          RDS_ENDPOINT=$(aws rds describe-db-instances \
            --db-instance-identifier "$RDS_IDENTIFIER" \
            --region ${{ env.AWS_REGION }} \
            --query 'DBInstances[0].Endpoint.Address' \
            --output text 2>/dev/null || echo "")

          if [ -z "$RDS_ENDPOINT" ] || [ "$RDS_ENDPOINT" = "None" ]; then
            echo "‚ùå ERROR: RDS instance '$RDS_IDENTIFIER' not found!"
            echo "Available RDS instances:"
            aws rds describe-db-instances --region ${{ env.AWS_REGION }} --query 'DBInstances[*].DBInstanceIdentifier' --output table
            echo ""
            echo "Please create RDS instance first by running Terraform"
            exit 1
          fi

          echo "Found RDS endpoint: $RDS_ENDPOINT"

          # Get DB password from SSM Parameter Store (if stored there) or use default
          # NOTE: In production, password should be retrieved from Terraform output or SSM
          DB_PASSWORD=$(aws ssm get-parameter \
            --name "/rds/${{ inputs.environment }}/master-password" \
            --with-decryption \
            --region ${{ env.AWS_REGION }} \
            --query 'Parameter.Value' \
            --output text 2>/dev/null || echo "postgres")

          echo "Creating Kubernetes secret..."
          kubectl create secret generic user-management-db-secret \
            --from-literal=DB_HOST="$RDS_ENDPOINT" \
            --from-literal=DB_PORT=5432 \
            --from-literal=DB_NAME=auth_db \
            --from-literal=DB_USER=postgres \
            --from-literal=DB_PASSWORD="$DB_PASSWORD" \
            --namespace=${{ inputs.environment }} \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "‚úÖ Database secret created successfully"
          echo "   Host: $RDS_ENDPOINT"
          echo "   Database: auth_db"
          echo "   User: postgres"

      - name: Validate RDS Connectivity
        run: |
          echo "=== Validating RDS Connectivity ==="

          RDS_IDENTIFIER="nt114-postgres-${{ inputs.environment }}"

          # 1. Check RDS instance status
          echo "1. Checking RDS instance status..."
          RDS_STATUS=$(aws rds describe-db-instances \
            --db-instance-identifier "$RDS_IDENTIFIER" \
            --region ${{ env.AWS_REGION }} \
            --query 'DBInstances[0].DBInstanceStatus' \
            --output text 2>/dev/null || echo "not-found")

          if [ "$RDS_STATUS" = "not-found" ]; then
            echo "‚ùå ERROR: RDS instance '$RDS_IDENTIFIER' not found!"
            exit 1
          fi

          if [ "$RDS_STATUS" != "available" ]; then
            echo "‚ùå ERROR: RDS instance is in state: $RDS_STATUS (expected: available)"
            echo "Wait for RDS to become available or check for issues"
            exit 1
          fi

          echo "‚úÖ RDS instance is available"

          # 2. Get RDS endpoint and security group
          RDS_INFO=$(aws rds describe-db-instances \
            --db-instance-identifier "$RDS_IDENTIFIER" \
            --region ${{ env.AWS_REGION }} \
            --query 'DBInstances[0].[Endpoint.Address,Endpoint.Port,VpcSecurityGroups[0].VpcSecurityGroupId,DBSubnetGroup.VpcId]' \
            --output text)

          RDS_ENDPOINT=$(echo "$RDS_INFO" | awk '{print $1}')
          RDS_PORT=$(echo "$RDS_INFO" | awk '{print $2}')
          RDS_SG=$(echo "$RDS_INFO" | awk '{print $3}')
          RDS_VPC=$(echo "$RDS_INFO" | awk '{print $4}')

          echo "   Endpoint: $RDS_ENDPOINT:$RDS_PORT"
          echo "   Security Group: $RDS_SG"
          echo "   VPC: $RDS_VPC"

          # 3. Get EKS node security group
          echo ""
          echo "2. Checking EKS cluster security groups..."
          CLUSTER_SG=$(aws eks describe-cluster \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' \
            --output text)

          echo "   EKS Cluster SG: $CLUSTER_SG"

          # Get node security groups
          NODE_GROUPS=$(aws eks list-nodegroups \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --query 'nodegroups[0]' \
            --output text)

          if [ ! -z "$NODE_GROUPS" ]; then
            NODE_SG=$(aws ec2 describe-instances \
              --region ${{ env.AWS_REGION }} \
              --filters "Name=tag:eks:nodegroup-name,Values=$NODE_GROUPS" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null || echo "")

            if [ ! -z "$NODE_SG" ] && [ "$NODE_SG" != "None" ]; then
              echo "   EKS Node SG: $NODE_SG"
            fi
          fi

          # 4. Check RDS security group allows traffic from EKS
          echo ""
          echo "3. Validating security group rules..."

          SG_RULES=$(aws ec2 describe-security-group-rules \
            --region ${{ env.AWS_REGION }} \
            --filters "Name=group-id,Values=$RDS_SG" \
            --query "SecurityGroupRules[?IsEgress==\`false\` && IpProtocol==\`tcp\` && (FromPort<=\`$RDS_PORT\` && ToPort>=\`$RDS_PORT\`)].[GroupId,ReferencedGroupInfo.GroupId,CidrIpv4]" \
            --output text)

          echo "   RDS Inbound Rules (Port $RDS_PORT):"
          echo "$SG_RULES" | while read line; do
            echo "   - $line"
          done

          # Check if EKS cluster SG is allowed and auto-remediate if needed
          SG_RULE_EXISTS=false

          if echo "$SG_RULES" | grep -q "$CLUSTER_SG"; then
            echo "‚úÖ EKS Cluster SG ($CLUSTER_SG) is allowed to access RDS"
            SG_RULE_EXISTS=true
          elif [ ! -z "$NODE_SG" ] && echo "$SG_RULES" | grep -q "$NODE_SG"; then
            echo "‚úÖ EKS Node SG ($NODE_SG) is allowed to access RDS"
            SG_RULE_EXISTS=true
          fi

          if [ "$SG_RULE_EXISTS" = false ]; then
            echo ""
            echo "‚ö†Ô∏è  EKS security groups do not have access to RDS"
            echo "   Auto-remediating: Adding security group rule..."

            # Try to add cluster SG first
            SOURCE_SG="$CLUSTER_SG"
            SG_DESCRIPTION="Allow EKS cluster access to RDS (auto-added by workflow)"

            echo "   Adding ingress rule:"
            echo "   - Source SG: $SOURCE_SG"
            echo "   - Target SG: $RDS_SG"
            echo "   - Port: $RDS_PORT"

            if aws ec2 authorize-security-group-ingress \
              --group-id "$RDS_SG" \
              --protocol tcp \
              --port "$RDS_PORT" \
              --source-group "$SOURCE_SG" \
              --region ${{ env.AWS_REGION }} 2>&1 | tee /tmp/sg-add-output.txt; then

              echo ""
              echo "‚úÖ Security group rule added successfully!"
              echo "   EKS cluster can now access RDS"

              # Wait a moment for AWS to propagate the change
              sleep 5
            else
              # Check if error is because rule already exists
              if grep -q "already exists" /tmp/sg-add-output.txt; then
                echo "‚úÖ Security group rule already exists (detected during add)"
              else
                echo ""
                echo "‚ùå Failed to add security group rule automatically"
                echo "   Please add manually:"
                echo "   aws ec2 authorize-security-group-ingress \\"
                echo "     --group-id $RDS_SG \\"
                echo "     --protocol tcp \\"
                echo "     --port $RDS_PORT \\"
                echo "     --source-group $SOURCE_SG \\"
                echo "     --region ${{ env.AWS_REGION }}"
                echo ""
                echo "   Or via AWS Console:"
                echo "   1. Go to EC2 > Security Groups"
                echo "   2. Select RDS SG: $RDS_SG"
                echo "   3. Add Inbound Rule: PostgreSQL (TCP $RDS_PORT) from $SOURCE_SG"
                exit 1
              fi
            fi
          fi

          # 5. Test connectivity from a test pod
          echo ""
          echo "4. Testing database connectivity from cluster..."

          # Create test pod
          kubectl run db-connection-test \
            --image=postgres:15-alpine \
            --restart=Never \
            --namespace=${{ inputs.environment }} \
            --env="PGPASSWORD=postgres" \
            --command -- sleep 60 &

          # Wait for pod to be ready
          sleep 10
          kubectl wait --for=condition=ready pod/db-connection-test \
            --namespace=${{ inputs.environment }} \
            --timeout=60s || {
            echo "‚ö†Ô∏è  Test pod not ready, skipping connectivity test"
            kubectl delete pod db-connection-test --namespace=${{ inputs.environment }} --ignore-not-found=true
            exit 0
          }

          # Test connection
          if kubectl exec db-connection-test --namespace=${{ inputs.environment }} -- \
            psql -h "$RDS_ENDPOINT" -U postgres -d postgres -c "SELECT 1" > /dev/null 2>&1; then
            echo "‚úÖ Database connectivity test PASSED"
          else
            echo "‚ùå Database connectivity test FAILED"
            echo "   Unable to connect to $RDS_ENDPOINT:$RDS_PORT from EKS pod"
            echo "   Check security group rules and network configuration"
            kubectl delete pod db-connection-test --namespace=${{ inputs.environment }} --ignore-not-found=true
            exit 1
          fi

          # Cleanup test pod
          kubectl delete pod db-connection-test --namespace=${{ inputs.environment }} --ignore-not-found=true

          echo ""
          echo "‚úÖ All RDS connectivity checks passed!"

      - name: Initialize Database Schema
        run: |
          echo "=== Initializing Database Schema ==="

          # Substitute variables in DB init job manifest
          NAMESPACE=${{ inputs.environment }}

          # Process manifest with variable substitution
          sed -e "s/\${K8S_NAMESPACE}/$NAMESPACE/g" \
              k8s/database-schema-job.yaml > /tmp/database-schema-job-processed.yaml

          # Delete old job if exists (to allow re-running)
          kubectl delete job database-schema-setup \
            -n $NAMESPACE \
            --ignore-not-found=true

          # Apply DB init job
          kubectl apply -f /tmp/database-schema-job-processed.yaml

          # Wait for job to complete
          echo "Waiting for database initialization to complete..."
          kubectl wait --for=condition=complete --timeout=300s \
            job/database-schema-setup -n $NAMESPACE || {
            echo "‚ùå Database initialization failed!"
            echo ""
            echo "=== Job Status ==="
            kubectl get job database-schema-setup -n $NAMESPACE
            echo ""
            echo "=== Pod Status ==="
            kubectl get pods -n $NAMESPACE -l job-name=database-schema-setup
            echo ""
            echo "=== Job Logs ==="
            kubectl logs -n $NAMESPACE job/database-schema-setup --tail=100
            exit 1
          }

          echo ""
          echo "‚úÖ Database schema initialized successfully!"

          # Show logs
          echo ""
          echo "=== Initialization Logs ==="
          kubectl logs -n $NAMESPACE job/database-schema-setup --tail=30

      - name: Install AWS Load Balancer Controller
        id: alb-install
        run: |
          echo "=== Installing/Checking AWS Load Balancer Controller ==="

          # Get cluster info
          CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
          VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.resourcesVpcConfig.vpcId" --output text)
          echo "VPC ID: $VPC_ID"

          # Check if ALB controller is already running
          if kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --no-headers 2>/dev/null | grep -q "Running"; then
            echo "ALB Controller is already running"
            echo "alb_ready=true" >> $GITHUB_OUTPUT
          else
            echo "Installing AWS Load Balancer Controller..."

            # Add eks-charts repo
            helm repo add eks https://aws.github.io/eks-charts
            helm repo update

            # Create IAM service account for ALB controller (if not exists)
            # Note: This requires the IAM role to be created by Terraform first

            # Check if service account exists
            if ! kubectl get serviceaccount aws-load-balancer-controller -n kube-system &>/dev/null; then
              echo "Creating service account..."

              # Get OIDC provider
              OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text | sed 's|https://||')

              # Create service account with IAM role annotation
              cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: aws-load-balancer-controller
            namespace: kube-system
            annotations:
              eks.amazonaws.com/role-arn: arn:aws:iam::${{ steps.aws-account.outputs.account_id }}:role/${{ env.CLUSTER_NAME }}-alb-controller
          EOF
            fi

            # Install or upgrade ALB controller
            helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=$CLUSTER_NAME \
              --set region=${{ env.AWS_REGION }} \
              --set vpcId=$VPC_ID \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --wait --timeout 5m || true

            # Wait for controller to be ready
            echo "Waiting for ALB Controller to be ready..."
            kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s || true

            echo "alb_ready=true" >> $GITHUB_OUTPUT
          fi

          # Create IngressClass if not exists
          if ! kubectl get ingressclass alb &>/dev/null; then
            echo "Creating ALB IngressClass..."
            cat <<EOF | kubectl apply -f -
          apiVersion: networking.k8s.io/v1
          kind: IngressClass
          metadata:
            name: alb
            annotations:
              ingressclass.kubernetes.io/is-default-class: "true"
          spec:
            controller: ingress.k8s.aws/alb
          EOF
          fi

          echo ""
          echo "=== ALB Controller Status ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          echo ""
          echo "Ingress Classes:"
          kubectl get ingressclass

      - name: Validate ECR Images
        run: |
          echo "=== Validating ECR Images ==="

          # List of required services
          SERVICES=("api-gateway" "user-management-service" "exercises-service" "scores-service" "frontend")
          IMAGE_TAG="latest"
          ACCOUNT_ID="${{ steps.aws-account.outputs.account_id }}"
          ECR_REPO_PREFIX="${ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/nt114-devsecops"

          MISSING_IMAGES=()

          for service in "${SERVICES[@]}"; do
            IMAGE_REPO="nt114-devsecops/${service}"

            echo ""
            echo "Checking image: $service:$IMAGE_TAG"

            # Check if image exists
            if aws ecr describe-images \
              --repository-name "$IMAGE_REPO" \
              --image-ids imageTag="$IMAGE_TAG" \
              --region ${{ env.AWS_REGION }} \
              --output json > /dev/null 2>&1; then

              # Get image details
              IMAGE_DIGEST=$(aws ecr describe-images \
                --repository-name "$IMAGE_REPO" \
                --image-ids imageTag="$IMAGE_TAG" \
                --region ${{ env.AWS_REGION }} \
                --query 'imageDetails[0].imageDigest' \
                --output text)

              IMAGE_PUSHED=$(aws ecr describe-images \
                --repository-name "$IMAGE_REPO" \
                --image-ids imageTag="$IMAGE_TAG" \
                --region ${{ env.AWS_REGION }} \
                --query 'imageDetails[0].imagePushedAt' \
                --output text)

              echo "‚úÖ Image found: $ECR_REPO_PREFIX/${service}:$IMAGE_TAG"
              echo "   Digest: $IMAGE_DIGEST"
              echo "   Pushed: $IMAGE_PUSHED"
            else
              echo "‚ùå Image NOT found: $ECR_REPO_PREFIX/${service}:$IMAGE_TAG"
              MISSING_IMAGES+=("$service")
            fi
          done

          # Check if any images are missing
          if [ ${#MISSING_IMAGES[@]} -gt 0 ]; then
            echo ""
            echo "=============================================="
            echo "‚ùå ERROR: Missing ECR Images"
            echo "=============================================="
            echo ""
            echo "The following images are not available in ECR:"
            for service in "${MISSING_IMAGES[@]}"; do
              echo "  - $ECR_REPO_PREFIX/${service}:$IMAGE_TAG"
            done
            echo ""
            echo "Please build and push these images first by running:"
            echo "  gh workflow run build-and-push.yml"
            echo ""
            echo "Or manually build and push:"
            echo "  docker build -t $ECR_REPO_PREFIX/<service>:latest ."
            echo "  docker push $ECR_REPO_PREFIX/<service>:latest"
            echo ""
            exit 1
          fi

          echo ""
          echo "‚úÖ All required ECR images are available!"

      - name: Deploy with Helm (Direct)
        if: inputs.deployment_method == 'helm'
        run: |
          echo "=== Deploying with Helm (Direct) ==="

          # Cleanup stuck Helm releases
          echo "Checking for stuck Helm releases..."
          SERVICES=("api-gateway" "user-management-service" "exercises-service" "scores-service" "frontend")

          for service in "${SERVICES[@]}"; do
            if helm status "$service" -n ${{ inputs.environment }} 2>/dev/null | grep -q "STATUS: pending"; then
              echo "Found stuck release: $service. Cleaning up..."
              helm uninstall "$service" -n ${{ inputs.environment }} --wait || true
            elif helm status "$service" -n ${{ inputs.environment }} 2>/dev/null | grep -q "STATUS: failed"; then
              echo "Found failed release: $service. Cleaning up..."
              helm uninstall "$service" -n ${{ inputs.environment }} --wait || true
            fi
          done

          echo "Cleanup complete"

          # Set common Helm values
          ECR_REPO="${{ steps.aws-account.outputs.account_id }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/nt114-devsecops"

          # Update values-eks.yaml with AWS Account ID
          for service in api-gateway user-management-service exercises-service scores-service frontend; do
            VALUES_FILE="helm/${service}/values-eks.yaml"
            if [ -f "$VALUES_FILE" ]; then
              sed -i "s/\${AWS_ACCOUNT_ID}/${{ steps.aws-account.outputs.account_id }}/g" "$VALUES_FILE"
            fi
          done

          # Deploy services in correct order with dependency management
          deploy_service() {
            local service=$1
            local wait_for_ready=${2:-true}

            echo ""
            echo "Deploying $service..."

            # Remove existing failed deployment if exists
            if helm status "$service" -n ${{ inputs.environment }} 2>/dev/null | grep -q "STATUS: failed\|STATUS: pending"; then
              echo "Removing failed deployment of $service..."
              helm uninstall "$service" -n ${{ inputs.environment }} --wait || true
              sleep 10
            fi

            # Deploy with correct values (AWS account ID already substituted)
            if helm upgrade --install "$service" "./helm/$service" \
              --namespace ${{ inputs.environment }} \
              --set image.tag=latest \
              --set image.pullPolicy=Always \
              -f "./helm/$service/values-eks.yaml" \
              --wait --timeout 10m; then
              echo "‚úÖ $service deployed successfully"

              # Additional health check if requested
              if [ "$wait_for_ready" = true ]; then
                echo "Waiting for $service pods to be ready..."
                kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} --timeout=300s || true
              fi
            else
              echo "‚ùå $service deployment failed!"
              echo ""
              echo "=== POD STATUS ==="
              kubectl get pods -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} || true
              echo ""
              echo "=== POD DESCRIBE ==="
              kubectl describe pod -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} || true
              echo ""
              echo "=== POD LOGS ==="
              kubectl logs -l app.kubernetes.io/name=$service -n ${{ inputs.environment }} --tail=100 || true
              echo ""
              echo "=== EVENTS ==="
              kubectl get events -n ${{ inputs.environment }} --sort-by='.lastTimestamp' --field-selector involvedObject.kind=Pod | grep "$service" | tail -10 || true
              return 1
            fi
          }

          # Deploy services in dependency order
          DEPLOY_FAILED=false

          echo "üöÄ Starting deployment in dependency order..."

          # Phase 1: Backend Services (no dependencies)
          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"user-management-service"* ]]; then
            echo ""
            echo "=== Phase 1: User Management Service ==="
            deploy_service "user-management-service" || DEPLOY_FAILED=true
          fi

          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"exercises-service"* ]]; then
            echo ""
            echo "=== Phase 1: Exercises Service ==="
            deploy_service "exercises-service" || DEPLOY_FAILED=true
          fi

          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"scores-service"* ]]; then
            echo ""
            echo "=== Phase 1: Scores Service ==="
            deploy_service "scores-service" || DEPLOY_FAILED=true
          fi

          # Phase 2: API Gateway (depends on backend services)
          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"api-gateway"* ]]; then
            echo ""
            echo "=== Phase 2: API Gateway ==="
            deploy_service "api-gateway" || DEPLOY_FAILED=true
          fi

          # Phase 3: Frontend (depends on API Gateway)
          if [[ "${{ inputs.services }}" == "all" ]] || [[ "${{ inputs.services }}" == *"frontend"* ]]; then
            echo ""
            echo "=== Phase 3: Frontend ==="
            deploy_service "frontend" || DEPLOY_FAILED=true
          fi

          if [ "$DEPLOY_FAILED" = true ]; then
            echo ""
            echo "‚ùå One or more deployments failed!"
            exit 1
          fi

          echo ""
          echo "üéâ All services deployed successfully!"

      - name: Install ArgoCD
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Installing ArgoCD ==="

          # Check if ArgoCD is already installed
          if kubectl get namespace argocd &>/dev/null; then
            echo "ArgoCD namespace exists, checking installation..."
            if kubectl get deployment argocd-server -n argocd &>/dev/null; then
              echo "‚úÖ ArgoCD already installed"
            else
              echo "Namespace exists but ArgoCD not installed, installing..."
              INSTALL_ARGOCD=true
            fi
          else
            echo "ArgoCD not installed, proceeding with installation..."
            INSTALL_ARGOCD=true
          fi

          if [ "$INSTALL_ARGOCD" = true ]; then
            # Add ArgoCD Helm repo
            helm repo add argo https://argoproj.github.io/argo-helm
            helm repo update

            # Create namespace
            kubectl create namespace argocd

            # Install ArgoCD with custom values
            helm install argocd argo/argo-cd \
              --namespace argocd \
              --set server.service.type=ClusterIP \
              --set server.ingress.enabled=false \
              --set server.config.tls.minVersion=VersionTLS12 \
              --set configs.params."server\.insecure"=false \
              --wait --timeout 10m

            echo "‚úÖ ArgoCD installed successfully"
          fi

          # Wait for ArgoCD to be ready
          echo "Waiting for ArgoCD components to be ready..."
          kubectl wait --for=condition=available --timeout=300s \
            deployment/argocd-server \
            deployment/argocd-repo-server \
            deployment/argocd-applicationset-controller \
            deployment/argocd-dex-server \
            -n argocd

          echo ""
          echo "=== ArgoCD Status ==="
          kubectl get pods -n argocd

      - name: Create ArgoCD Ingress
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Creating ArgoCD Ingress ==="

          # Apply Ingress manifest
          kubectl apply -f argocd/argocd-ingress.yaml

          # Wait for Ingress to be created
          kubectl wait --for=jsonpath='{.status.loadBalancer.ingress}' \
            ingress/argocd-server -n argocd --timeout=300s || true

          echo ""
          echo "=== Ingress Status ==="
          kubectl get ingress argocd-server -n argocd

      - name: Get ArgoCD Access Info
        if: inputs.deployment_method == 'argocd'
        run: |
          echo ""
          echo "=== ArgoCD Access Information ==="

          # Get admin password
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret \
            -o jsonpath="{.data.password}" | base64 -d)

          # Get Ingress URL
          ARGOCD_URL=$(kubectl get ingress argocd-server -n argocd \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

          if [ -z "$ARGOCD_URL" ]; then
            echo "‚è≥ Waiting for ALB to be provisioned..."
            sleep 30
            ARGOCD_URL=$(kubectl get ingress argocd-server -n argocd \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          fi

          echo ""
          echo "=============================================="
          echo "         ARGOCD ACCESS CREDENTIALS           "
          echo "=============================================="
          echo ""
          echo "URL:      http://${ARGOCD_URL}"
          echo "Username: admin"
          echo "Password: ${ARGOCD_PASSWORD}"
          echo ""
          echo "=============================================="
          echo ""

          # Add to GitHub summary
          echo "### ArgoCD Access" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **URL:** http://${ARGOCD_URL}" >> $GITHUB_STEP_SUMMARY
          echo "- **Username:** admin" >> $GITHUB_STEP_SUMMARY
          echo "- **Password:** \`${ARGOCD_PASSWORD}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Save to workflow output
          echo "argocd_url=http://${ARGOCD_URL}" >> $GITHUB_OUTPUT
          echo "argocd_password=${ARGOCD_PASSWORD}" >> $GITHUB_OUTPUT

      - name: Verify ArgoCD Installation
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Verifying ArgoCD Installation ==="

          # Check all ArgoCD pods are running
          echo "Checking ArgoCD pods..."
          kubectl get pods -n argocd

          ARGOCD_PODS_READY=$(kubectl get pods -n argocd -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | grep -o "True" | wc -l)
          ARGOCD_PODS_TOTAL=$(kubectl get pods -n argocd --no-headers | wc -l)

          echo "Pods ready: $ARGOCD_PODS_READY / $ARGOCD_PODS_TOTAL"

          if [ "$ARGOCD_PODS_READY" -eq "$ARGOCD_PODS_TOTAL" ]; then
            echo "‚úÖ All ArgoCD pods are ready"
          else
            echo "‚ùå Some ArgoCD pods are not ready"
            kubectl describe pods -n argocd
            exit 1
          fi

          # Test ArgoCD server health
          echo ""
          echo "Testing ArgoCD server health..."
          ARGOCD_URL=$(kubectl get ingress argocd-server -n argocd \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

          if [ ! -z "$ARGOCD_URL" ]; then
            # Wait a bit for ALB target registration
            sleep 30

            if curl -sf --max-time 10 "http://${ARGOCD_URL}/healthz" > /dev/null; then
              echo "‚úÖ ArgoCD server is healthy"
            else
              echo "‚ö†Ô∏è ArgoCD server not reachable yet (ALB may still be provisioning)"
              echo "Try accessing: http://${ARGOCD_URL}"
            fi
          else
            echo "‚ö†Ô∏è ALB URL not available yet"
            echo "Run: kubectl get ingress argocd-server -n argocd"
          fi

      - name: Deploy with ArgoCD
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Deploying with ArgoCD (GitOps) ==="

          # AWS Account ID already substituted in values-eks.yaml files
          echo "Using pre-configured values-eks.yaml files with AWS Account ID..."

          # Apply ArgoCD applications
          echo "Applying ArgoCD applications..."
          kubectl apply -f argocd/argocd-applications.yaml

          echo ""
          echo "=== ArgoCD Applications Status ==="
          sleep 10
          kubectl get applications -n argocd

      - name: Apply Kubernetes Manifests
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Applying Kubernetes Manifests ==="

          # Substitute variables in manifests
          ACCOUNT_ID=${{ steps.aws-account.outputs.account_id }}
          NAMESPACE=${{ inputs.environment }}

          # Create temp directory for processed manifests
          mkdir -p /tmp/k8s-processed

          # Process each manifest with variable substitution
          for file in k8s/*.yaml; do
            if [ -f "$file" ]; then
              echo "Processing $(basename $file)..."
              sed -e "s/\${AWS_ACCOUNT_ID}/$ACCOUNT_ID/g" \
                  -e "s/\${K8S_NAMESPACE}/$NAMESPACE/g" \
                  "$file" > "/tmp/k8s-processed/$(basename $file)"
            fi
          done

          # Apply database schema job
          if [ -f /tmp/k8s-processed/database-schema-job.yaml ]; then
            kubectl apply -f /tmp/k8s-processed/database-schema-job.yaml
          fi

          # Apply ECR token refresh CronJob
          if [ -f /tmp/k8s-processed/ecr-token-refresh-cronjob.yaml ]; then
            kubectl apply -f /tmp/k8s-processed/ecr-token-refresh-cronjob.yaml
          fi

          # Wait for DB schema job to complete
          echo "Waiting for database schema setup to complete..."
          kubectl wait --for=condition=complete --timeout=300s \
            job/database-schema-setup -n ${{ inputs.environment }} || {
            echo "Database schema setup failed, checking logs..."
            kubectl logs -n ${{ inputs.environment }} job/database-schema-setup --tail=50
            exit 1
          }

          echo "‚úÖ Database schema setup completed"

          # Verify ECR CronJob created
          kubectl get cronjob ecr-token-refresh -n ${{ inputs.environment }}
          echo "‚úÖ ECR token refresh CronJob created"

      - name: Validate Database Schema
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Validating Database Schema ==="

          # Get RDS endpoint from secret
          RDS_ENDPOINT=$(kubectl get secret user-management-db-secret \
            -n ${{ inputs.environment }} \
            -o jsonpath='{.data.DB_HOST}' | base64 -d)

          DB_PASSWORD=$(kubectl get secret user-management-db-secret \
            -n ${{ inputs.environment }} \
            -o jsonpath='{.data.DB_PASSWORD}' | base64 -d)

          echo "Testing connection to: $RDS_ENDPOINT"

          # Create validation pod
          kubectl run db-schema-validation \
            --image=postgres:15-alpine \
            --restart=Never \
            --namespace=${{ inputs.environment }} \
            --env="PGPASSWORD=$DB_PASSWORD" \
            --command -- sleep 120 &

          # Wait for pod to be ready
          sleep 10
          kubectl wait --for=condition=ready pod/db-schema-validation \
            --namespace=${{ inputs.environment }} \
            --timeout=60s || {
            echo "‚ùå Validation pod not ready"
            kubectl delete pod db-schema-validation --namespace=${{ inputs.environment }} --ignore-not-found=true
            exit 1
          }

          # Check if database exists
          echo ""
          echo "1. Checking if database 'auth_db' exists..."
          DB_EXISTS=$(kubectl exec db-schema-validation --namespace=${{ inputs.environment }} -- \
            psql -h "$RDS_ENDPOINT" -U postgres -d postgres \
            -tAc "SELECT 1 FROM pg_database WHERE datname='auth_db'" 2>/dev/null || echo "")

          if [ "$DB_EXISTS" = "1" ]; then
            echo "‚úÖ Database 'auth_db' exists"
          else
            echo "‚ùå Database 'auth_db' not found!"
            kubectl delete pod db-schema-validation --namespace=${{ inputs.environment }} --ignore-not-found=true
            exit 1
          fi

          # Check if required tables exist
          echo ""
          echo "2. Checking required tables..."
          REQUIRED_TABLES=("users" "exercises" "user_progress" "scores")

          for table in "${REQUIRED_TABLES[@]}"; do
            echo "   Checking table: $table"
            TABLE_EXISTS=$(kubectl exec db-schema-validation --namespace=${{ inputs.environment }} -- \
              psql -h "$RDS_ENDPOINT" -U postgres -d auth_db \
              -tAc "SELECT 1 FROM information_schema.tables WHERE table_schema='public' AND table_name='$table'" 2>/dev/null || echo "")

            if [ "$TABLE_EXISTS" = "1" ]; then
              echo "   ‚úÖ Table '$table' exists"

              # Get row count
              ROW_COUNT=$(kubectl exec db-schema-validation --namespace=${{ inputs.environment }} -- \
                psql -h "$RDS_ENDPOINT" -U postgres -d auth_db \
                -tAc "SELECT COUNT(*) FROM $table" 2>/dev/null || echo "0")

              echo "      Rows: $ROW_COUNT"
            else
              echo "   ‚ùå Table '$table' NOT found!"
              kubectl delete pod db-schema-validation --namespace=${{ inputs.environment }} --ignore-not-found=true
              exit 1
            fi
          done

          # Verify table structures
          echo ""
          echo "3. Verifying table structures..."

          # Check users table columns
          echo "   Checking 'users' table structure..."
          USERS_COLUMNS=$(kubectl exec db-schema-validation --namespace=${{ inputs.environment }} -- \
            psql -h "$RDS_ENDPOINT" -U postgres -d auth_db \
            -tAc "SELECT column_name FROM information_schema.columns WHERE table_name='users' ORDER BY ordinal_position" 2>/dev/null || echo "")

          if echo "$USERS_COLUMNS" | grep -q "email"; then
            echo "   ‚úÖ Users table has required columns"
          else
            echo "   ‚ö†Ô∏è  Users table may be missing columns"
          fi

          # Cleanup validation pod
          kubectl delete pod db-schema-validation --namespace=${{ inputs.environment }} --ignore-not-found=true

          echo ""
          echo "‚úÖ Database schema validation completed successfully!"
          echo "   Database: auth_db"
          echo "   Tables: ${REQUIRED_TABLES[@]}"

      - name: Deploy Applications with ArgoCD
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Deploying Applications with ArgoCD ==="

          # Apply ArgoCD Applications
          kubectl apply -f argocd/argocd-applications.yaml

          echo "‚úÖ ArgoCD Applications created"
          echo ""
          echo "=== Application Status ==="
          kubectl get applications -n argocd

          # Wait a bit for initial sync to start
          sleep 10

          echo ""
          echo "=== Detailed Application Status ==="
          kubectl get applications -n argocd -o wide

      - name: Monitor ArgoCD Sync
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Monitoring ArgoCD Sync Progress ==="

          # Function to check if all apps are synced
          check_sync_status() {
            TOTAL_APPS=$(kubectl get applications -n argocd --no-headers | wc -l)
            SYNCED_APPS=$(kubectl get applications -n argocd \
              -o jsonpath='{.items[?(@.status.sync.status=="Synced")].metadata.name}' | wc -w)

            echo "Apps synced: $SYNCED_APPS / $TOTAL_APPS"

            if [ "$SYNCED_APPS" -eq "$TOTAL_APPS" ]; then
              return 0
            else
              return 1
            fi
          }

          # Wait up to 10 minutes for sync
          for i in {1..60}; do
            echo ""
            echo "Check $i/60..."

            if check_sync_status; then
              echo "‚úÖ All applications synced successfully!"
              break
            fi

            # Show current status
            kubectl get applications -n argocd -o custom-columns=NAME:.metadata.name,SYNC:.status.sync.status,HEALTH:.status.health.status,MESSAGE:.status.conditions[0].message

            if [ $i -eq 60 ]; then
              echo "‚ùå Timeout waiting for applications to sync"
              exit 1
            fi

            sleep 10
          done

          echo ""
          echo "=== Final Application Status ==="
          kubectl get applications -n argocd

      - name: Verify Application Health
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Verifying Application Health ==="

          # Check if all apps are healthy
          UNHEALTHY=$(kubectl get applications -n argocd \
            -o jsonpath='{.items[?(@.status.health.status!="Healthy")].metadata.name}')

          if [ -z "$UNHEALTHY" ]; then
            echo "‚úÖ All applications are healthy!"
          else
            echo "‚ùå Unhealthy applications: $UNHEALTHY"
            echo ""
            echo "Checking details..."

            for app in $UNHEALTHY; do
              echo "=== $app ==="
              kubectl get application $app -n argocd -o yaml | grep -A 20 "status:"
            done

            exit 1
          fi

          echo ""
          echo "=== Pod Status in Dev Namespace ==="
          kubectl get pods -n dev -o wide

          echo ""
          echo "ArgoCD will now sync applications automatically."
          echo "Access ArgoCD UI to monitor deployment progress."

      - name: Verify Ingress Resources
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Checking Ingress Resources ==="

          # List all ingresses in dev namespace
          kubectl get ingress -n ${{ inputs.environment }}

          # Expected ingresses:
          # - frontend
          # - api-gateway

          INGRESS_COUNT=$(kubectl get ingress -n ${{ inputs.environment }} --no-headers | wc -l)

          if [ "$INGRESS_COUNT" -ge 2 ]; then
            echo "‚úÖ Found $INGRESS_COUNT ingress resources"
          else
            echo "‚ùå Expected at least 2 ingresses, found $INGRESS_COUNT"
            exit 1
          fi

      - name: Wait for ALB Provisioning
        run: |
          echo "=== Waiting for ALB to be provisioned ==="

          # Function to check if ingress has address
          check_ingress_ready() {
            local ingress_name=$1
            local address=$(kubectl get ingress $ingress_name -n ${{ inputs.environment }} \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)

            if [ ! -z "$address" ]; then
              echo "$ingress_name: $address"
              return 0
            else
              echo "$ingress_name: Pending..."
              return 1
            fi
          }

          # Wait up to 10 minutes for ALB provisioning
          for i in {1..60}; do
            echo ""
            echo "Check $i/60 (ALB provisioning can take 2-5 minutes)..."

            FRONTEND_READY=false
            API_READY=false

            if check_ingress_ready "frontend"; then
              FRONTEND_READY=true
            fi

            if check_ingress_ready "api-gateway"; then
              API_READY=true
            fi

            if [ "$FRONTEND_READY" = true ] && [ "$API_READY" = true ]; then
              echo ""
              echo "‚úÖ All ALBs provisioned successfully!"
              break
            fi

            if [ $i -eq 60 ]; then
              echo ""
              echo "‚ùå Timeout waiting for ALB provisioning"
              echo "Check ALB controller logs:"
              echo "kubectl logs -n kube-system deployment/aws-load-balancer-controller"
              exit 1
            fi

            sleep 10
          done

          echo ""
          echo "=== Final Ingress Status ==="
          kubectl get ingress -n ${{ inputs.environment }} -o wide

      - name: Verify ALB Target Health
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Verifying ALB Target Health ==="

          # Get ALB ARNs
          FRONTEND_ALB=$(kubectl get ingress frontend -n ${{ inputs.environment }} \
            -o jsonpath='{.metadata.annotations.alb\.ingress\.kubernetes\.io/load-balancer-arn}' 2>/dev/null)

          API_ALB=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} \
            -o jsonpath='{.metadata.annotations.alb\.ingress\.kubernetes\.io/load-balancer-arn}' 2>/dev/null)

          # Wait for target registration (ALB needs time to register targets)
          echo "Waiting 60 seconds for target registration..."
          sleep 60

          # Function to check target health
          check_target_health() {
            local alb_arn=$1
            local name=$2

            if [ -z "$alb_arn" ]; then
              echo "‚ö†Ô∏è $name: ALB ARN not found (may still be provisioning)"
              return 1
            fi

            # Get target groups for this ALB
            local tgs=$(aws elbv2 describe-target-groups \
              --load-balancer-arn "$alb_arn" \
              --region ${{ env.AWS_REGION }} \
              --query 'TargetGroups[*].TargetGroupArn' \
              --output text)

            echo "--- $name Target Groups ---"
            for tg in $tgs; do
              echo "Checking: $tg"
              aws elbv2 describe-target-health \
                --target-group-arn "$tg" \
                --region ${{ env.AWS_REGION }} \
                --query 'TargetHealthDescriptions[*].[Target.Id,TargetHealth.State,TargetHealth.Reason]' \
                --output table
            done
          }

          check_target_health "$FRONTEND_ALB" "Frontend"
          echo ""
          check_target_health "$API_ALB" "API Gateway"

      - name: Display Access URLs
        if: inputs.deployment_method == 'argocd'
        run: |
          echo ""
          echo "=============================================="
          echo "         APPLICATION ACCESS URLS              "
          echo "=============================================="
          echo ""

          # Get Frontend URL
          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ ! -z "$FRONTEND_URL" ]; then
            echo "Frontend URL: http://${FRONTEND_URL}"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üåê Access URLs" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Service | URL |" >> $GITHUB_STEP_SUMMARY
            echo "|---------|-----|" >> $GITHUB_STEP_SUMMARY
            echo "| Frontend | http://${FRONTEND_URL} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "Frontend: Ingress not ready yet"
          fi

          # Get API Gateway URL
          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ ! -z "$API_URL" ]; then
            echo "API Gateway URL: http://${API_URL}"
            echo "| API Gateway | http://${API_URL} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "API Gateway: Ingress not ready yet"
          fi

          echo ""
          echo "=============================================="
          echo ""

          # Save to output
          echo "frontend_url=http://${FRONTEND_URL}" >> $GITHUB_OUTPUT
          echo "api_url=http://${API_URL}" >> $GITHUB_OUTPUT

      - name: Test Application Endpoints
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "=== Testing Application Endpoints ==="

          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")

          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")

          # Wait for ALB to be fully ready
          echo "Waiting 30 seconds for ALB initialization..."
          sleep 30

          # Test Frontend
          echo ""
          echo "Testing Frontend..."
          if [ ! -z "$FRONTEND_URL" ] && curl -sf --max-time 10 --connect-timeout 5 "http://${FRONTEND_URL}" > /dev/null; then
            echo "‚úÖ Frontend is accessible"
          else
            echo "‚ö†Ô∏è Frontend not reachable yet (may need more time for ALB initialization)"
            [ ! -z "$FRONTEND_URL" ] && echo "Try manually: http://${FRONTEND_URL}"
          fi

          # Test API Gateway
          echo ""
          echo "Testing API Gateway health endpoint..."
          if [ ! -z "$API_URL" ] && curl -sf --max-time 10 --connect-timeout 5 "http://${API_URL}/health"; then
            echo ""
            echo "‚úÖ API Gateway is accessible"
          else
            echo "‚ö†Ô∏è API Gateway not reachable yet"
            [ ! -z "$API_URL" ] && echo "Try manually: http://${API_URL}/health"
          fi

          echo ""
          echo "### Endpoint Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "Frontend: http://${FRONTEND_URL}" >> $GITHUB_STEP_SUMMARY
          echo "API Gateway: http://${API_URL}/health" >> $GITHUB_STEP_SUMMARY
          echo ""
          echo "Note: If endpoints are not accessible immediately,"
          echo "wait 2-3 minutes for ALB health checks to pass."

      - name: Get Deployment Status
        run: |
          echo "### Deployment Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ inputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Method:** ${{ inputs.deployment_method }}" >> $GITHUB_STEP_SUMMARY
          echo "**Region:** ${{ env.AWS_REGION }}" >> $GITHUB_STEP_SUMMARY
          echo "**Cluster:** ${{ env.CLUSTER_NAME }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### Deployed Services" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get deployments -n ${{ inputs.environment }} >> $GITHUB_STEP_SUMMARY || echo "No deployments found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### Service Endpoints" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get services -n ${{ inputs.environment }} >> $GITHUB_STEP_SUMMARY || echo "No services found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "#### ALB Ingress URLs" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get ingress -n ${{ inputs.environment }} -o wide >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No ingress found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Pod Status" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get pods -n ${{ inputs.environment }} -o wide >> $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Get ArgoCD Access Info
        if: inputs.deployment_method == 'argocd'
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### ArgoCD Access" >> $GITHUB_STEP_SUMMARY

          # Get ArgoCD password
          ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d || echo "<not available>")

          # Get ArgoCD URL
          ARGOCD_URL=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "<pending>")

          echo "- **ArgoCD URL:** https://${ARGOCD_URL}" >> $GITHUB_STEP_SUMMARY
          echo "- **Username:** admin" >> $GITHUB_STEP_SUMMARY
          echo "- **Password:** See workflow output (not shown in summary for security)" >> $GITHUB_STEP_SUMMARY

          echo ""
          echo "=== ArgoCD Access Information ==="
          echo "URL: https://${ARGOCD_URL}"
          echo "Username: admin"
          echo "Password: ${ARGOCD_PASSWORD}"

          echo ""
          echo "=== ArgoCD Application Status ==="
          kubectl get applications -n argocd

  verify:
    name: Verify Deployment
    runs-on: ubuntu-latest
    needs: [deploy]
    if: success()
    continue-on-error: true

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Configure kubectl
        run: |
          echo "Configuring kubectl for cluster: ${{ env.CLUSTER_NAME }}"
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

          # Test kubectl access
          if ! kubectl cluster-info > /dev/null 2>&1; then
            echo "‚ùå kubectl authentication failed in verify job"
            echo "Current IAM identity:"
            aws sts get-caller-identity
            exit 1
          fi
          echo "‚úÖ kubectl configured successfully"

      - name: Wait for Pods Ready
        run: |
          echo "Waiting for pods to be ready..."

          # Wait up to 10 minutes for pods
          for i in {1..60}; do
            PENDING_PODS=$(kubectl get pods -n ${{ inputs.environment }} --no-headers 2>/dev/null | grep -v "Running\|Completed" | wc -l)

            if [ "$PENDING_PODS" -eq 0 ]; then
              echo "All pods are running!"
              break
            fi

            echo "Waiting for $PENDING_PODS pods to be ready... (attempt $i/60)"
            sleep 10
          done

          echo ""
          echo "=== Final Pod Status ==="
          kubectl get pods -n ${{ inputs.environment }} -o wide

          echo ""
          echo "=== Deployment Status ==="
          kubectl get deployments -n ${{ inputs.environment }}

      - name: Verify ALB and Display URLs
        run: |
          echo "### Final Access URLs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "=== Ingress Status ==="
          kubectl get ingress -n ${{ inputs.environment }} -o wide

          # Frontend
          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ ! -z "$FRONTEND_URL" ]; then
            echo ""
            echo "Frontend URL: http://${FRONTEND_URL}"
            echo "- **Frontend:** http://${FRONTEND_URL}" >> $GITHUB_STEP_SUMMARY
          fi

          # API Gateway
          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ ! -z "$API_URL" ]; then
            echo "API Gateway URL: http://${API_URL}"
            echo "- **API Gateway:** http://${API_URL}" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Run Health Checks
        run: |
          echo "### Health Check Results" >> $GITHUB_STEP_SUMMARY

          # Get Frontend URL and test
          FRONTEND_URL=$(kubectl get ingress frontend -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ ! -z "$FRONTEND_URL" ]; then
            echo "Testing Frontend at http://${FRONTEND_URL}..."
            if curl -sf --max-time 10 "http://${FRONTEND_URL}" > /dev/null; then
              echo "Frontend health check PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "Frontend health check FAILED or not reachable yet" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          # Get API Gateway URL and test
          API_URL=$(kubectl get ingress api-gateway -n ${{ inputs.environment }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ ! -z "$API_URL" ]; then
            echo "Testing API Gateway at http://${API_URL}/health..."
            if curl -sf --max-time 10 "http://${API_URL}/health"; then
              echo "API Gateway health check PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "API Gateway health check FAILED or not reachable yet" >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "#### Events (Last 20)" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          kubectl get events -n ${{ inputs.environment }} --sort-by='.lastTimestamp' | tail -20 >> $GITHUB_STEP_SUMMARY || echo "No events" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
